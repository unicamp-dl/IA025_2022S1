{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercício 01 - 2022S1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex01%20/%20Antonio_Angulo%20/%20Ex01_Antonio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoyGt5gXMgK"
      },
      "source": [
        "## Coloque seu nome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe"
      },
      "source": [
        "print('Meu nome é: Antonio Lincoln Angulo Salas')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1.1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j"
      },
      "source": [
        "def top_k(L, k):\n",
        "  i_count = {}\n",
        "  for i in L:\n",
        "    if i in i_count:\n",
        "      i_count[i] += 1\n",
        "    else:\n",
        "      i_count[i] = 1\n",
        "\n",
        "  resultado ={}\n",
        "  for j in range(k):\n",
        "    if len(str(max(i_count.values())))>0:\n",
        "      val = max(i_count.values())\n",
        "      for key, value in i_count.items():\n",
        "        if val == value:\n",
        "          resultado[key] = val\n",
        "          del i_count[key]\n",
        "          break\n",
        "    else:\n",
        "      continue\n",
        "  return resultado\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "outputId": "aa6ef0ee-9f93-47db-ed72-66519da7ea9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L, k=k)\n",
        "print(f'resultado: {resultado}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "outputId": "b69e4bbb-3a8f-4437-d441-0b5a814329a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000 loops, best of 5: 6.48 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 1.2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzv89trtTPc"
      },
      "source": [
        "def tokens_to_ids(text, vocabulary):\n",
        "  id = {}\n",
        "  text = text.lower()\n",
        "  for k,v in vocabulary.items():\n",
        "    text = text.replace(k,' ' + str(v))\n",
        "  id = text.split()\n",
        "\n",
        "  for g in range(len(id)):\n",
        "    if not id[g].isdigit():\n",
        "      id[g]= '-1'\n",
        "      \n",
        "  return id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a60627-adc8-4c30-f8f1-872f1e3f910f"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "print(tokens_to_ids(D, V))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '3', '2', '4', '-1', '5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fddc1e24-4b1d-456e-e86a-00673377c0f6"
      },
      "source": [
        "%%timeit\n",
        "resultado = tokens_to_ids(D, V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 1.16 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 1.3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsadE9SRG_9"
      },
      "source": [
        "def sample(path: str, k: int):\n",
        "  import random\n",
        "  rndSamples = sorted(random.sample(range(0,total_size-1),total_size - n_samples))\n",
        "  with open(filename, 'r') as file:\n",
        "      samples = file.read().splitlines()\n",
        "      for ele in sorted(rndSamples, reverse = True):\n",
        "          del samples[ele]\n",
        "  return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193284eb-00ab-4c13-d0c5-bbbaa7e65b18"
      },
      "source": [
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['line 12', 'line 20', 'line 21', 'line 29', 'line 37', 'line 48', 'line 59', 'line 83', 'line 85', 'line 99']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000\n",
        "n_samples = 10000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83aae83-ee49-4826-e505-5fa0fc227dff"
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 2.76 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 2.1\n",
        "\n",
        "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNXj45RJqUm"
      },
      "source": [
        "Resposta:\n",
        "- número de somas: $m \\times p$\n",
        "- número de multiplicações: $m \\times n \\times p$  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2.2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrXf18N5KrK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5eed4b-0ec1-4782-c797-822328111e2d"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "444d67ba-660f-4522-c287-edf16bc02e92"
      },
      "source": [
        "print(A.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 2.3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a1286a-a179-4d0f-cfbc-889eb3d9d36e"
      },
      "source": [
        "Amin = A.min()\n",
        "Amax = A.max()\n",
        "C = (A-Amin)/(Amax-Amin)\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.04347826 0.08695652 0.13043478 0.17391304 0.2173913 ]\n",
            " [0.26086957 0.30434783 0.34782609 0.39130435 0.43478261 0.47826087]\n",
            " [0.52173913 0.56521739 0.60869565 0.65217391 0.69565217 0.73913043]\n",
            " [0.7826087  0.82608696 0.86956522 0.91304348 0.95652174 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 2.4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a9c6be-debc-4a6c-ad72-935b5cf49b29"
      },
      "source": [
        "Amin = A.min(0)\n",
        "Amax = A.max(0)\n",
        "C = (A-Amin)/(Amax-Amin)\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.        ]\n",
            " [0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
            " [0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667]\n",
            " [1.         1.         1.         1.         1.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 2.5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e245652-56bf-4bc9-e446-f1e8c9d94312"
      },
      "source": [
        "Amin = A.min(1)\n",
        "Amax = A.max(1)\n",
        "C = (A.transpose()-Amin)/(Amax-Amin)\n",
        "print(C.transpose())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 2.6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(A):\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "    \n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "    Amax = A.max(1,keepdims=True) \n",
        "    exp_A = np.exp(A - Amax) \n",
        "    Asum = np.sum(exp_A,axis=1,keepdims=True)\n",
        "    s_max = exp_A / Asum \n",
        "    return s_max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EZ5ZD7HFao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "658f588b-7561-45ba-dc36-a508973779ca"
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\n",
        "              [-2,   0, 0.5]])\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 1.        ],\n",
              "       [0.04861082, 0.35918811, 0.59220107]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "O código a seguir verifica se sua implementação do softmax está correta. \n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70624b4-eb4c-46be-bce0-9b9cd2263860"
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d726906-8eec-4827-f05b-bc18b5ca5194"
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 276 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE6LaWi81zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06feb080-67aa-41b1-956d-57954f90ec4e"
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 2.7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "def one_hot(y, n_classes):\n",
        "    oneHotM = np.zeros((y.size, y.max()+1))\n",
        "    oneHotM[np.arange(y.size),y] = 1\n",
        "    return oneHotM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db3b607-485a-4fd6-dce0-03ec3b9d54de"
      },
      "source": [
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)\n",
        "print(y)\n",
        "print(one_hot(y, N_CLASSES))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 7 6 2 7 5 3 8 1 5]\n",
            "[[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c813785d-2fac-476d-e355-40277a080ab9"
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e524cd-f30c-4fcf-aa35-69f77edbd46d"
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 184 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMroZay2ubi"
      },
      "source": [
        "## Exercício 2.8\n",
        "\n",
        "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
        "```\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)  # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaedJ5Cf5Oy2"
      },
      "source": [
        "class Normalizer(object):\n",
        "  def __init__(self, b_arr):\n",
        "    b = np.array(b_arr)\n",
        "    self.bm = b.mean()\n",
        "    self.bstd = b.std()\n",
        "  def __call__(self, a_arr):\n",
        "    a = np.array(a_arr)\n",
        "    self.am = a.mean()\n",
        "    self.astd = a.std()\n",
        "    self.n = (a - self.am)/self.astd\n",
        "    return self.n * self.bstd + self.bm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlkNNU6h5RbR"
      },
      "source": [
        "Mostre que seu código está correto com o exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gad6zsbh5a0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b12425c-1f62-4eb8-f7c8-1517f3f26559"
      },
      "source": [
        "import numpy as np\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3187798  2.31425165 1.11696854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4BYg_6bs0t82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7668e0d4-ba50-4493-d76d-579ccb1eca97"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "**Tensor com atributo .requires_grad=True**\n",
        "\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3a3888-e110-4f6d-fb5e-c9e95171145f"
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a3ec75-1b15-4484-dbbe-2cbcdaaec03d"
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda62ecd-b421-4b50-e5a7-b49f4ee8fa7e"
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f8480b-cd4a-4b0e-fead-408ee22b421f"
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y; print('e =',e)\n",
        "e2 = e.pow(2) ; print('e2 =', e2)\n",
        "J = e2.sum()  ; print('J =', J)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec8d1405-f36f-4515-d6d3-662733e871a2"
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df190198-d59a-4524-f331-d0cb8531424d"
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 3.1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "62nZAfUoCZu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b73df86-dfb0-44fe-9d93-185a43328f8a"
      },
      "source": [
        "def J_func(w, x, y):\n",
        "    return (x * w - y).pow(2).sum()\n",
        "\n",
        "# Calcule o gradiente usando a regra diferenças finitas\n",
        "# Confira com o valor já calculado anteriormente\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "dw = 0.1\n",
        "grad = grad = (J_func(w + dw, x, y) - J_func(w - dw, x, y)) / (2 * dw)\n",
        "print('grad=', grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad= tensor(-28.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 3.2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNszCOED1Wtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "aa37d1ec-54cb-41ec-d771-43ccf766bfa7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(iteracoes):\n",
        "    J = J_func(w, x, y)\n",
        "    X.append(i)\n",
        "    Y.append(J)\n",
        "    grad = (J_func(w + dw, x, y) - J_func(w - dw, x, y)) / 2 / dw\n",
        "    w -= learning_rate * grad\n",
        "    print('i =', i, '\\tJ=', J, '\\tgrad =',grad, '\\tw =', w)\n",
        "\n",
        "# Plote o gráfico da loss J pela iteração i\n",
        "plt.plot(X, Y) \n",
        "plt.xlabel('Iteração')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0 \tJ= tensor(14.) \tgrad = tensor(-28.0000) \tw = tensor([1.2800])\n",
            "i = 1 \tJ= tensor(7.2576) \tgrad = tensor(-20.1600) \tw = tensor([1.4816])\n",
            "i = 2 \tJ= tensor(3.7623) \tgrad = tensor(-14.5152) \tw = tensor([1.6268])\n",
            "i = 3 \tJ= tensor(1.9504) \tgrad = tensor(-10.4509) \tw = tensor([1.7313])\n",
            "i = 4 \tJ= tensor(1.0111) \tgrad = tensor(-7.5247) \tw = tensor([1.8065])\n",
            "i = 5 \tJ= tensor(0.5241) \tgrad = tensor(-5.4178) \tw = tensor([1.8607])\n",
            "i = 6 \tJ= tensor(0.2717) \tgrad = tensor(-3.9008) \tw = tensor([1.8997])\n",
            "i = 7 \tJ= tensor(0.1409) \tgrad = tensor(-2.8086) \tw = tensor([1.9278])\n",
            "i = 8 \tJ= tensor(0.0730) \tgrad = tensor(-2.0222) \tw = tensor([1.9480])\n",
            "i = 9 \tJ= tensor(0.0379) \tgrad = tensor(-1.4560) \tw = tensor([1.9626])\n",
            "i = 10 \tJ= tensor(0.0196) \tgrad = tensor(-1.0483) \tw = tensor([1.9730])\n",
            "i = 11 \tJ= tensor(0.0102) \tgrad = tensor(-0.7548) \tw = tensor([1.9806])\n",
            "i = 12 \tJ= tensor(0.0053) \tgrad = tensor(-0.5434) \tw = tensor([1.9860])\n",
            "i = 13 \tJ= tensor(0.0027) \tgrad = tensor(-0.3913) \tw = tensor([1.9899])\n",
            "i = 14 \tJ= tensor(0.0014) \tgrad = tensor(-0.2817) \tw = tensor([1.9928])\n",
            "i = 15 \tJ= tensor(0.0007) \tgrad = tensor(-0.2028) \tw = tensor([1.9948])\n",
            "i = 16 \tJ= tensor(0.0004) \tgrad = tensor(-0.1460) \tw = tensor([1.9962])\n",
            "i = 17 \tJ= tensor(0.0002) \tgrad = tensor(-0.1052) \tw = tensor([1.9973])\n",
            "i = 18 \tJ= tensor(0.0001) \tgrad = tensor(-0.0757) \tw = tensor([1.9981])\n",
            "i = 19 \tJ= tensor(5.3059e-05) \tgrad = tensor(-0.0545) \tw = tensor([1.9986])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 3.3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP4d5vtHtqy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "8db8adc7-72ef-4400-a23b-86162d377b88"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(iteracoes):\n",
        "    J = J_func(w, x, y)\n",
        "    X.append(i)\n",
        "    Y.append(J.detach())\n",
        "    J.backward()\n",
        "    grad = w.grad\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * grad\n",
        "    print('i =', i, '\\tJ=', J, '\\tgrad =',grad, '\\tw =', w)\n",
        "    w.grad.zero_()\n",
        "\n",
        "# Plote aqui a loss pela iteração\n",
        "plt.plot(X, Y) \n",
        "plt.xlabel('Iteração')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0 \tJ= tensor(14., grad_fn=<SumBackward0>) \tgrad = tensor([-28.]) \tw = tensor([1.2800], requires_grad=True)\n",
            "i = 1 \tJ= tensor(7.2576, grad_fn=<SumBackward0>) \tgrad = tensor([-20.1600]) \tw = tensor([1.4816], requires_grad=True)\n",
            "i = 2 \tJ= tensor(3.7623, grad_fn=<SumBackward0>) \tgrad = tensor([-14.5152]) \tw = tensor([1.6268], requires_grad=True)\n",
            "i = 3 \tJ= tensor(1.9504, grad_fn=<SumBackward0>) \tgrad = tensor([-10.4509]) \tw = tensor([1.7313], requires_grad=True)\n",
            "i = 4 \tJ= tensor(1.0111, grad_fn=<SumBackward0>) \tgrad = tensor([-7.5247]) \tw = tensor([1.8065], requires_grad=True)\n",
            "i = 5 \tJ= tensor(0.5241, grad_fn=<SumBackward0>) \tgrad = tensor([-5.4178]) \tw = tensor([1.8607], requires_grad=True)\n",
            "i = 6 \tJ= tensor(0.2717, grad_fn=<SumBackward0>) \tgrad = tensor([-3.9008]) \tw = tensor([1.8997], requires_grad=True)\n",
            "i = 7 \tJ= tensor(0.1409, grad_fn=<SumBackward0>) \tgrad = tensor([-2.8086]) \tw = tensor([1.9278], requires_grad=True)\n",
            "i = 8 \tJ= tensor(0.0730, grad_fn=<SumBackward0>) \tgrad = tensor([-2.0222]) \tw = tensor([1.9480], requires_grad=True)\n",
            "i = 9 \tJ= tensor(0.0379, grad_fn=<SumBackward0>) \tgrad = tensor([-1.4560]) \tw = tensor([1.9626], requires_grad=True)\n",
            "i = 10 \tJ= tensor(0.0196, grad_fn=<SumBackward0>) \tgrad = tensor([-1.0483]) \tw = tensor([1.9730], requires_grad=True)\n",
            "i = 11 \tJ= tensor(0.0102, grad_fn=<SumBackward0>) \tgrad = tensor([-0.7548]) \tw = tensor([1.9806], requires_grad=True)\n",
            "i = 12 \tJ= tensor(0.0053, grad_fn=<SumBackward0>) \tgrad = tensor([-0.5434]) \tw = tensor([1.9860], requires_grad=True)\n",
            "i = 13 \tJ= tensor(0.0027, grad_fn=<SumBackward0>) \tgrad = tensor([-0.3913]) \tw = tensor([1.9899], requires_grad=True)\n",
            "i = 14 \tJ= tensor(0.0014, grad_fn=<SumBackward0>) \tgrad = tensor([-0.2817]) \tw = tensor([1.9928], requires_grad=True)\n",
            "i = 15 \tJ= tensor(0.0007, grad_fn=<SumBackward0>) \tgrad = tensor([-0.2028]) \tw = tensor([1.9948], requires_grad=True)\n",
            "i = 16 \tJ= tensor(0.0004, grad_fn=<SumBackward0>) \tgrad = tensor([-0.1460]) \tw = tensor([1.9962], requires_grad=True)\n",
            "i = 17 \tJ= tensor(0.0002, grad_fn=<SumBackward0>) \tgrad = tensor([-0.1052]) \tw = tensor([1.9973], requires_grad=True)\n",
            "i = 18 \tJ= tensor(0.0001, grad_fn=<SumBackward0>) \tgrad = tensor([-0.0757]) \tw = tensor([1.9981], requires_grad=True)\n",
            "i = 19 \tJ= tensor(5.3059e-05, grad_fn=<SumBackward0>) \tgrad = tensor([-0.0545]) \tw = tensor([1.9986], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 3.4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXQGEyvtiTAR"
      },
      "source": [
        "Resposta: a $\\Delta w$ pode ter valores grandes (maior a 1) ou pequenos (menor a 1), no caso de valores grandes não permitirá que a função de erro tenha muita incerteza e calculara os pesos erradamente, por outro lado se o valor for muito pequeno não aproveitará a precisão de o algoritmo, pode até ser contraproducente porque o custo computacional.Por isso é necessário escolher um valor que nos permita otimizar a precisão e o custo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 3.5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pna3bcicHj"
      },
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a)  Para calcular o custo computacional para fazer $(x_i w - y_i)^2$ com o método das diferenças finitas é necessário calcular a derivada parcial de $J$ em função da variável, com isso temos o custo para uma única atualização é $2 * O(N)$.\n",
        "\n",
        "b) No método de backpropagation só precisa calcular a derivada parcial de $J$ en relação de variável uma vez, neste caso o custo computacional é $O(N)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 3.6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTOphiVs6eN"
      },
      "source": [
        "Resposta: \n",
        "O custo é dado por: \n",
        "\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "\n",
        "$k_e$ é a classe do exemplo (amostra)\n",
        "\n",
        "$y_j = y_{ke}$ para classe do exemplo = 1\n",
        "\n",
        "Como $y$ possui apenas uma classe positiva em cada amostra, temos:\n",
        "$$ \\sum_{j=0}^{K-1} = 1 $$\n",
        "Depois:\n",
        "$$ L = -1 \\log \\frac{1}{K} = -(log1 - logK) = logK$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "Fim do notebook."
      ]
    }
  ]
}