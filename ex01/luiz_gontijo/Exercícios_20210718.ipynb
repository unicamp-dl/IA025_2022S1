{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercícios - 20210718",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex01/luiz_gontijo/Exerc%C3%ADcios_20210718.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff82b5a-a159-449f-838e-30c9cfa4afc4"
      },
      "source": [
        "print('Meu nome é: Luiz Fernando da Costa Gontijo RA203677')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é: Luiz Fernando da Costa Gontijo RA203677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sobre a execução dos exercícios propostos\n",
        "\n",
        "Algumas atividades contaram com a contribuição de códigos de outros membros da disciplina. dessa forma, gostaria de agradecer aos alunos Andersson Andreé e Beatriz Celante pelo apoio. Consegui agregar um grande conhecimento com as propostas dos colegas de turma e espero poder contribuir para que outros também tenham ganhos na disciplina. \n",
        "\n",
        "Por fim, algumas atividades necessitaram fontes externas. Algumas dessas fontes estão com seus links registrados no decorrer do notebook."
      ],
      "metadata": {
        "id": "0rQIgRUXiX1n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1.1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import collections"
      ],
      "metadata": {
        "id": "mw2a8Kuz7afy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j"
      },
      "source": [
        "def top_k(L, k):\n",
        "  # Escreva aqui o código\n",
        "  unique, counts = np.unique(L, return_counts=True)\n",
        "  count_array = np.asarray((unique, counts), dtype=object)\n",
        "  count_array = count_array.T\n",
        "  dict_count_array = dict(count_array)\n",
        "  dict_count_array_sorted = sorted(dict_count_array.items(), key=lambda x: x[1], reverse=True)\n",
        "  first_k = dict_count_array_sorted[:k]\n",
        "  return dict(first_k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756ec362-10c6-4c99-fe63-906fe33adbd0"
      },
      "source": [
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L, k=k)\n",
        "print(f'resultado: {resultado}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50c3a46-b9c5-4526-ab1d-a5a713aec8ca"
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)\n",
        "#print(f'resultado: {resultado}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 2.23 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 1.2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzv89trtTPc"
      },
      "source": [
        "# Agradecer pelo apoio na utilização da library re\n",
        "import re\n",
        "\n",
        "def tokens_to_ids(text, vocabulary):\n",
        "  # Escreva o código aqui.\n",
        "  tokens = re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
        "  list_ids = []\n",
        "  for token in tokens:\n",
        "    #print(token)\n",
        "    if token in vocabulary.keys():\n",
        "      #print(token)\n",
        "      list_ids.append(vocabulary[token])\n",
        "    else:\n",
        "      list_ids.append(vocabulary['unknown'])\n",
        "\n",
        "  return list_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a071e17-2824-4fb2-8088-0e959495c03f"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "print(tokens_to_ids(D, V))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 2, 4, -1, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba12f65-e09e-40b9-8cb8-3f92d0436ad0"
      },
      "source": [
        "%%timeit\n",
        "resultado = tokens_to_ids(D, V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 2.87 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 1.3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilização da biblioteca random: https://pynative.com/python-random-choice/"
      ],
      "metadata": {
        "id": "u1cTqEbAE9K2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsadE9SRG_9"
      },
      "source": [
        "import random\n",
        "\n",
        "def sample(path: str, k: int):\n",
        "  # Escreva o seu código aqui.\n",
        "  file = open(filename, 'r')\n",
        "  lines_read = file.read().split('\\n')\n",
        "  random_list = random.choices(lines_read, k=n_samples)\n",
        "  return random_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa2e9a8-5617-4a73-e16c-d5f315e06801"
      },
      "source": [
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['line 88', 'line 89', 'line 59', 'line 25', 'line 26', 'line 30', 'line 53', 'line 16', 'line 38', 'line 70']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000\n",
        "n_samples = 10000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62609196-8c52-4022-a25d-9c4ccf41a01e"
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 76.1 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 2.1\n",
        "\n",
        "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auxílio para a questão: https://mediatum.ub.tum.de/doc/625604/625604"
      ],
      "metadata": {
        "id": "Jion-7SeGMbg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNXj45RJqUm"
      },
      "source": [
        "Resposta:\n",
        "- número de somas: mp(n-1)\n",
        "- número de multiplicações: mpn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2.2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrXf18N5KrK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654fc1a4-0739-4739-9cb8-c0b4b1c80839"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4de6c62-cc54-4449-b1da-b23782f1abd4"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "A.mean(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.5,  8.5, 14.5, 20.5])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 2.3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0383e73-7025-4f01-8cb7-db54561864f9"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "C = (A - A.min())/(A.max() - A.min()) \n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.04347826 0.08695652 0.13043478 0.17391304 0.2173913 ]\n",
            " [0.26086957 0.30434783 0.34782609 0.39130435 0.43478261 0.47826087]\n",
            " [0.52173913 0.56521739 0.60869565 0.65217391 0.69565217 0.73913043]\n",
            " [0.7826087  0.82608696 0.86956522 0.91304348 0.95652174 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 2.4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcdd54ee-498e-45e3-b3a3-b566d138eb81"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "max_column = np.max(A, axis=0)\n",
        "min_column = np.min(A, axis=0)\n",
        "\n",
        "result_24 = (A-min_column)/(max_column-min_column)\n",
        "\n",
        "print(result_24)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.        ]\n",
            " [0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
            " [0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667]\n",
            " [1.         1.         1.         1.         1.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 2.5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c069ada-05e6-4d7e-b629-77a5dee4bcfa"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "max_row = np.max(A, axis=1)\n",
        "min_row = np.min(A, axis=1)\n",
        "\n",
        "C = (A.T-min_row)/(max_row-min_row)\n",
        "result_25 = C.T\n",
        "\n",
        "print(result_25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 2.6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Agradecer aluna Beatriz Celante Vicente\n",
        "\n",
        "def softmax(A):\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "    \n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "    # Escreva sua solução aqui.\n",
        "    max_row = np.max(A, axis=1)\n",
        "    min_row = np.min(A, axis=1)\n",
        "\n",
        "    C = (A.T-min_row)/(max_row-min_row)\n",
        "    A_norm = C.T\n",
        "\n",
        "    # depois testar com subtração de cada linha pelo valor máximo - ver se muda alguma coisa\n",
        "\n",
        "    # aplicar Exponencial para cada elemento da matriz\n",
        "    exp_matrix = np.exp(A_norm)\n",
        "\n",
        "    # aplicar equação da softmax para cada linha\n",
        "    div = np.sum(exp_matrix, axis=1, keepdims=True) \n",
        "\n",
        "    result_26 = exp_matrix/div\n",
        "\n",
        "    return result_26"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EZ5ZD7HFao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71b1a03-2634-4716-fd1b-82796c3c26f8"
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\n",
        "              [-2,   0, 0.5]])\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.21219195, 0.21187422, 0.57593383],\n",
              "       [0.16824189, 0.37442922, 0.45732888]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "O código a seguir verifica se sua implementação do softmax está correta. \n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "873386ca-a9af-4950-c54f-aa3381ebc6ff"
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa166ee2-f236-4b08-b507-3ff32353721c"
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 307 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE6LaWi81zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb2bdce-dceb-43ef-8efc-1f0728644b01"
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 2.7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def one_hot(y, n_classes):\n",
        "    # Escreva seu código aqui.\n",
        "    dim_matrix = (y.size, n_classes)\n",
        "    matrix_zeros = np.zeros(dim_matrix, dtype=int)\n",
        "    matrix_zeros[np.arange(y.size),y] = 1\n",
        "    return matrix_zeros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224ca214-7fac-4ebb-c6ec-a4eed266465e"
      },
      "source": [
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)\n",
        "print(y)\n",
        "print(one_hot(y, N_CLASSES))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 8 1 0 8 1 4 4 8 6]\n",
            "[[1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 1 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ac2498-fe52-4ed6-dcc0-d7568f7a7daa"
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be016cbf-ad6a-4480-dc62-ff099c0472b1"
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 189 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMroZay2ubi"
      },
      "source": [
        "## Exercício 2.8\n",
        "\n",
        "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
        "```\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)  # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaedJ5Cf5Oy2"
      },
      "source": [
        "# Escreva seu código aqui.\n",
        "import numpy as np\n",
        "\n",
        "class Normalizer:\n",
        "  def __init__(self, array_b):\n",
        "    self.std_b = np.std(array_b)\n",
        "    self.mean_b = np.mean(array_b)\n",
        "\n",
        "  def normalize(self, array_a):\n",
        "    mean_a = np.mean(array_a)\n",
        "    std_a = np.std(array_a)\n",
        "    vec_norm = self.mean_b+(array_a-mean_a)*(self.std_b/std_a)\n",
        "    return vec_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlkNNU6h5RbR"
      },
      "source": [
        "Mostre que seu código está correto com o exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gad6zsbh5a0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794a05f5-da36-415e-f277-e2aaf70a7cff"
      },
      "source": [
        "array_a = [-1, 1.5, 0]\n",
        "array_b = [1.4, 0.8, 0.3, 2.5]\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize.normalize(array_a)\n",
        "print(normalized_array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3187798  2.31425165 1.11696854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5fb8521f-3758-480a-ffab-aa4dbf524aa9"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "**Tensor com atributo .requires_grad=True**\n",
        "\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d40ac6a-7f95-4b47-fa0e-d004f09ff1c9"
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2effe3ae-acfd-4ebb-f440-d2126a00a01b"
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a58c70ac-3c61-45ed-94ef-f53e76a2ff01"
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71213b08-23a2-47dc-f2d2-7bac70f7a6d9"
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y; print('e =',e)\n",
        "e2 = e.pow(2) ; print('e2 =', e2)\n",
        "J = e2.sum()  ; print('J =', J)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc2fb02-614d-428e-ace9-8920b8151556"
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec92afc6-838b-426b-e103-8c708a3f7fb9"
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 3.1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "62nZAfUoCZu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fdd3fbe-7a99-4bdb-f4dd-78947b8cec04"
      },
      "source": [
        "def J_func(w, x, y):\n",
        "    # programe a função J_func, para facilitar\n",
        "    J = (x*w - y)**2\n",
        "    return J.sum()\n",
        "\n",
        "# Calcule o gradiente usando a regra diferenças finitas\n",
        "# Confira com o valor já calculado anteriormente\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "delta = 10**(-4)\n",
        "\n",
        "grad_J = (J_func(w + delta, x, y) - J_func(w - delta, x, y))/(2*delta)\n",
        "\n",
        "grad = grad_J\n",
        "print('grad=', grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad= tensor(-27.9999)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 3.2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNszCOED1Wtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0fd5992-a70a-47e9-df37-94b6035befa3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "J_append = [] # guardar resultados de J para plotar\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    J_append.append(J)\n",
        "    print('J=', J)\n",
        "    grad = (J_func(w + delta, x, y) - J_func(w - delta, x, y))/(2*delta)\n",
        "    print('grad =',grad)\n",
        "    w = w - learning_rate*(grad)\n",
        "    print('w =', w)\n",
        "\n",
        "# Plote o gráfico da loss J pela iteração i\n",
        "plt.plot(J_append,range(iteracoes))\n",
        "plt.xlabel('Interassões')\n",
        "plt.ylabel('LOSS')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14.)\n",
            "grad = tensor(-27.9999)\n",
            "w = tensor([1.2800])\n",
            "i = 1\n",
            "J= tensor(7.2576)\n",
            "grad = tensor(-20.1607)\n",
            "w = tensor([1.4816])\n",
            "i = 2\n",
            "J= tensor(3.7623)\n",
            "grad = tensor(-14.5137)\n",
            "w = tensor([1.6267])\n",
            "i = 3\n",
            "J= tensor(1.9505)\n",
            "grad = tensor(-10.4505)\n",
            "w = tensor([1.7312])\n",
            "i = 4\n",
            "J= tensor(1.0112)\n",
            "grad = tensor(-7.5281)\n",
            "w = tensor([1.8065])\n",
            "i = 5\n",
            "J= tensor(0.5240)\n",
            "grad = tensor(-5.4196)\n",
            "w = tensor([1.8607])\n",
            "i = 6\n",
            "J= tensor(0.2716)\n",
            "grad = tensor(-3.9014)\n",
            "w = tensor([1.8997])\n",
            "i = 7\n",
            "J= tensor(0.1407)\n",
            "grad = tensor(-2.8086)\n",
            "w = tensor([1.9278])\n",
            "i = 8\n",
            "J= tensor(0.0729)\n",
            "grad = tensor(-2.0218)\n",
            "w = tensor([1.9480])\n",
            "i = 9\n",
            "J= tensor(0.0378)\n",
            "grad = tensor(-1.4555)\n",
            "w = tensor([1.9626])\n",
            "i = 10\n",
            "J= tensor(0.0196)\n",
            "grad = tensor(-1.0472)\n",
            "w = tensor([1.9731])\n",
            "i = 11\n",
            "J= tensor(0.0102)\n",
            "grad = tensor(-0.7540)\n",
            "w = tensor([1.9806])\n",
            "i = 12\n",
            "J= tensor(0.0053)\n",
            "grad = tensor(-0.5432)\n",
            "w = tensor([1.9860])\n",
            "i = 13\n",
            "J= tensor(0.0027)\n",
            "grad = tensor(-0.3910)\n",
            "w = tensor([1.9900])\n",
            "i = 14\n",
            "J= tensor(0.0014)\n",
            "grad = tensor(-0.2814)\n",
            "w = tensor([1.9928])\n",
            "i = 15\n",
            "J= tensor(0.0007)\n",
            "grad = tensor(-0.2027)\n",
            "w = tensor([1.9948])\n",
            "i = 16\n",
            "J= tensor(0.0004)\n",
            "grad = tensor(-0.1458)\n",
            "w = tensor([1.9962])\n",
            "i = 17\n",
            "J= tensor(0.0002)\n",
            "grad = tensor(-0.1051)\n",
            "w = tensor([1.9973])\n",
            "i = 18\n",
            "J= tensor(0.0001)\n",
            "grad = tensor(-0.0756)\n",
            "w = tensor([1.9981])\n",
            "i = 19\n",
            "J= tensor(5.2911e-05)\n",
            "grad = tensor(-0.0544)\n",
            "w = tensor([1.9986])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 3.3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP4d5vtHtqy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "355416c7-7520-4ef1-fe06-d5d882410906"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "\n",
        "J_append = []\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    J_append.append(J.detach().numpy()) #tensor to numpy\n",
        "    print('J=', J)\n",
        "    w.retain_grad() # retain_grad\n",
        "    J.backward() #backward\n",
        "    grad = w.grad\n",
        "    print('grad =',grad)\n",
        "    w = w - learning_rate*grad\n",
        "    print('w =', w)\n",
        "\n",
        "# Plote aqui a loss pela iteração\n",
        "plt.plot(J_append,range(iteracoes))\n",
        "plt.xlabel('Interassões')\n",
        "plt.ylabel('LOSS')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14., grad_fn=<SumBackward0>)\n",
            "grad = tensor([-28.])\n",
            "w = tensor([1.2800], grad_fn=<SubBackward0>)\n",
            "i = 1\n",
            "J= tensor(7.2576, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-20.1600])\n",
            "w = tensor([1.4816], grad_fn=<SubBackward0>)\n",
            "i = 2\n",
            "J= tensor(3.7623, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-14.5152])\n",
            "w = tensor([1.6268], grad_fn=<SubBackward0>)\n",
            "i = 3\n",
            "J= tensor(1.9504, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-10.4509])\n",
            "w = tensor([1.7313], grad_fn=<SubBackward0>)\n",
            "i = 4\n",
            "J= tensor(1.0111, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-7.5247])\n",
            "w = tensor([1.8065], grad_fn=<SubBackward0>)\n",
            "i = 5\n",
            "J= tensor(0.5241, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-5.4178])\n",
            "w = tensor([1.8607], grad_fn=<SubBackward0>)\n",
            "i = 6\n",
            "J= tensor(0.2717, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-3.9008])\n",
            "w = tensor([1.8997], grad_fn=<SubBackward0>)\n",
            "i = 7\n",
            "J= tensor(0.1409, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.8086])\n",
            "w = tensor([1.9278], grad_fn=<SubBackward0>)\n",
            "i = 8\n",
            "J= tensor(0.0730, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.0222])\n",
            "w = tensor([1.9480], grad_fn=<SubBackward0>)\n",
            "i = 9\n",
            "J= tensor(0.0379, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.4560])\n",
            "w = tensor([1.9626], grad_fn=<SubBackward0>)\n",
            "i = 10\n",
            "J= tensor(0.0196, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.0483])\n",
            "w = tensor([1.9730], grad_fn=<SubBackward0>)\n",
            "i = 11\n",
            "J= tensor(0.0102, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.7548])\n",
            "w = tensor([1.9806], grad_fn=<SubBackward0>)\n",
            "i = 12\n",
            "J= tensor(0.0053, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.5434])\n",
            "w = tensor([1.9860], grad_fn=<SubBackward0>)\n",
            "i = 13\n",
            "J= tensor(0.0027, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.3913])\n",
            "w = tensor([1.9899], grad_fn=<SubBackward0>)\n",
            "i = 14\n",
            "J= tensor(0.0014, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2817])\n",
            "w = tensor([1.9928], grad_fn=<SubBackward0>)\n",
            "i = 15\n",
            "J= tensor(0.0007, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2028])\n",
            "w = tensor([1.9948], grad_fn=<SubBackward0>)\n",
            "i = 16\n",
            "J= tensor(0.0004, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1460])\n",
            "w = tensor([1.9962], grad_fn=<SubBackward0>)\n",
            "i = 17\n",
            "J= tensor(0.0002, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1052])\n",
            "w = tensor([1.9973], grad_fn=<SubBackward0>)\n",
            "i = 18\n",
            "J= tensor(0.0001, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0757])\n",
            "w = tensor([1.9981], grad_fn=<SubBackward0>)\n",
            "i = 19\n",
            "J= tensor(5.3059e-05, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0545])\n",
            "w = tensor([1.9986], grad_fn=<SubBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 3.4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXQGEyvtiTAR"
      },
      "source": [
        "Resposta: Uma das restrições de $Δw$ está na complexidade computacional ao calcular o gradiente de $J$ quando seu valor é muito pequeno: quanto menor o valor de $Δw$, mais iterações ocorrerão até a convegência do gradiente. Além disso, o valor de $Δw$ não pode ser igual a zero; caso isso ocorra o gradiente de $J$ não será alterado, assim como indicado pela regra das diferenças finitas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 3.5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pna3bcicHj"
      },
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a) Com uma matriz de $N$ parâmetros, devem ser executadas $N$ operações. Dessa forma, o custo computacional passaria a ser $O(N^2)$.\n",
        "\n",
        "b) No backpropagation é calculada a derivada da função de perca somente uma vez. Dessa forma, o custo computacional é igual a $O(N)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 3.6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTOphiVs6eN"
      },
      "source": [
        "Resposta: Dada a inicalização aleatória, a probabilidade $p_j$ de cada classe é igual a $1/K$. Além disso, como $y_j$ são vetores one-hot, a soma desses vetores é igual ao número total $K$ de classes. Dessa forma, tem-se as seguintes operações para obter o valor da entropia cruzada $L$:\n",
        "\n",
        "\\begin{equation}\n",
        "p_j = \\frac{1}{K}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "L = -\\sum_{0}^{K-1}y_j\\log p_j\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "L = -\\sum_{0}^{K-1}y_j\\log \\frac{1}{K}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "L = -K\\log \\frac{1}{K}\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "Fim do notebook."
      ]
    }
  ]
}