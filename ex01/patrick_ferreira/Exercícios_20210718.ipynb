{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Cópia de Exercícios - 20210718",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "pycharm-362cd54d",
   "language": "python",
   "display_name": "PyCharm (IA025 - Lotufo)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "117px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTVOQpMfhgLM"
   },
   "source": [
    "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMoyGt5gXMgK"
   },
   "source": [
    "## Coloque seu nome"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iBHbXcibXPRe",
    "outputId": "d3330308-5cd3-4ba2-befa-953ab9f808b5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print('Meu nome é: Patrick de Carvalho Tavares Rezende Ferreira')"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é: Patrick de Carvalho Tavares Rezende Ferreira\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9S5acRbm1Zr"
   },
   "source": [
    "# Parte 1:\n",
    "\n",
    "##Exercícios de Processamento de Dados\n",
    "\n",
    "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxS5h1V8nDn6"
   },
   "source": [
    "##Exercício 1.1\n",
    "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
    "\n",
    "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gT08b5Z_nC-j"
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def top_k(L, k):\n",
    "    return dict(Counter(L).most_common(k))"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLD_e3C9p4xO"
   },
   "source": [
    "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iMW9NiBgnkvA",
    "outputId": "0128f4b2-3425-4b8a-86c1-d4299d79e7be",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
    "k = 3\n",
    "resultado = top_k(L=L, k=k)\n",
    "print(f'resultado: {resultado}')"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBeqZScQqJ0a"
   },
   "source": [
    "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O_lhcm4ko8bY"
   },
   "source": [
    "import random\n",
    "\n",
    "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
    "k = 10000"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L9U-Bgs2o-f_",
    "outputId": "35d71ed4-7e30-4e33-a334-74b65c0a4c35",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "%%timeit\n",
    "resultado = top_k(L=L, k=k)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 ms ± 2.85 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJHDaOz_tK38"
   },
   "source": [
    "## Exercício 1.2\n",
    "\n",
    "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
    "\n",
    "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
    "\n",
    "O código deve ser insensível a maiúsculas (case-insensitive).\n",
    "\n",
    "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rVzv89trtTPc"
   },
   "source": [
    "def tokens_to_ids(text, vocabulary):\n",
    "    return [vocabulary.get(word, -1) for word in text.lower().split()]"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCGZeiqkY-sm"
   },
   "source": [
    "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iApR1h7gY98E",
    "outputId": "0aa6b9a1-344b-4826-8aa8-37d76b4253d6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
    "D = 'Eu gosto de comer pizza.'\n",
    "\n",
    "print(tokens_to_ids(D, V))"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2, 4, -1]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWtTMxlXZN25"
   },
   "source": [
    "Mostre que sua implementação é eficiente com um exemplo grande:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pxT_g-ZxZUsX"
   },
   "source": [
    "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
    "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kp1nataGZU-V",
    "outputId": "b1fa8fcf-71cd-4dd0-90f0-4dce09e29b73",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "%%timeit\n",
    "resultado = tokens_to_ids(D, V)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "913 ms ± 7.74 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRfaKfXwRXn_"
   },
   "source": [
    "## Exercício 1.3\n",
    "\n",
    "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
    "\n",
    "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
    "\n",
    "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
    "\n",
    "Nota 2: Assuma que o arquivo não cabe em memória.\n",
    "\n",
    "Nota 3: Utilize apenas bibliotecas nativas do python."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2PsadE9SRG_9"
   },
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def sample(path: str, k: int):\n",
    "    with open(path, 'r') as f:\n",
    "\n",
    "        # Get file LENGTH without loading into memory\n",
    "        # iterate until file is over\n",
    "        length = 0\n",
    "        while f.readline():\n",
    "            length = length + 1\n",
    "\n",
    "        # random sampling\n",
    "        random_indexes = random.sample(range(length), k=k)\n",
    "\n",
    "    retorno = []\n",
    "    with open(path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i in random_indexes:\n",
    "                retorno.append(line[:-1])\n",
    "\n",
    "    return retorno\n"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycEnlFWxSt0i"
   },
   "source": [
    "Mostre que sua implementação está correta com um exemplo pequeno:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vyLJ1e2ZSzC9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "outputId": "541af2ad-fc0a-45cb-e5ec-6b9ac8757ec1"
   },
   "source": [
    "filename = 'small.txt'\n",
    "total_size = 100\n",
    "n_samples = 10\n",
    "\n",
    "with open(filename, 'w') as fout:\n",
    "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
    "\n",
    "samples = sample(path=filename, k=n_samples)\n",
    "print(samples)\n",
    "print(len(samples) == n_samples)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['line 1', 'line 10', 'line 35', 'line 41', 'line 42', 'line 47', 'line 60', 'line 61', 'line 83', 'line 84']\n",
      "True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2r4FMiMj12Xg"
   },
   "source": [
    "Mostre que sua implementação é eficiente com um exemplo grande:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PUwnNMGg18Ty"
   },
   "source": [
    "filename = 'large.txt'\n",
    "total_size = 1_000_000\n",
    "n_samples = 10000\n",
    "\n",
    "with open(filename, 'w') as fout:\n",
    "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iA9sAZmo0UDN"
   },
   "source": [
    "%%timeit\n",
    "samples = sample(path=filename, k=n_samples)\n",
    "assert len(samples) == n_samples"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 37s ± 16.2 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udS0Ns4etoJs"
   },
   "source": [
    "# Parte 2:\n",
    "\n",
    "##Exercícios de Numpy\n",
    "\n",
    "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcMz3Vzjt144"
   },
   "source": [
    "##Exercício 2.1\n",
    "\n",
    "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gNXj45RJqUm"
   },
   "source": [
    "Resposta:\n",
    "- número de somas: m * (n - 1) * p\n",
    "- número de multiplicações: m * n * p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iI7udBFeDlP"
   },
   "source": [
    "## Exercício 2.2\n",
    "\n",
    "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
    "mas sim, utiliza-se operações matriciais.\n",
    "\n",
    "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
    "\n",
    "Utilize apenas a biblioteca numpy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cjrXf18N5KrK"
   },
   "source": [
    "import numpy as np"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-fqxgNBW27Z0"
   },
   "source": [
    "A = np.arange(24).reshape(4, 6)\n",
    "print(A)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5]\n",
      " [ 6  7  8  9 10 11]\n",
      " [12 13 14 15 16 17]\n",
      " [18 19 20 21 22 23]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J1EmKFrT5g7B"
   },
   "source": [
    "np.mean(A, axis=1)"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 2.5,  8.5, 14.5, 20.5])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtgSAAKjUfcO"
   },
   "source": [
    "## Exercício 2.3\n",
    "\n",
    "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
    "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
    "\n",
    "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T00:00:34.072719Z",
     "start_time": "2019-12-11T00:00:34.036017Z"
    },
    "id": "_pDhb2-0eDlS"
   },
   "source": [
    "C = (A - A.min()) / (A.max() - A.min())\n",
    "C"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.04347826, 0.08695652, 0.13043478, 0.17391304,\n        0.2173913 ],\n       [0.26086957, 0.30434783, 0.34782609, 0.39130435, 0.43478261,\n        0.47826087],\n       [0.52173913, 0.56521739, 0.60869565, 0.65217391, 0.69565217,\n        0.73913043],\n       [0.7826087 , 0.82608696, 0.86956522, 0.91304348, 0.95652174,\n        1.        ]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GF_P_GARU62m"
   },
   "source": [
    "## Exercício 2.4\n",
    "\n",
    "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6NgVzFOYeDla"
   },
   "source": [
    "C = (A - A.min(axis=0)) / (A.max(axis=0) - A.min(axis=0))\n",
    "C"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        ],\n       [0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n        0.33333333],\n       [0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n        0.66666667],\n       [1.        , 1.        , 1.        , 1.        , 1.        ,\n        1.        ]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbXIXsDIUmtp"
   },
   "source": [
    "## Exercício 2.5\n",
    "\n",
    "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T17:56:40.413601Z",
     "start_time": "2019-12-10T17:56:40.405056Z"
    },
    "id": "i-5Hv8-heDlW"
   },
   "source": [
    "C = (A - A.min(axis=1).reshape(-1, 1)) / (A.max(axis=1) - A.min(axis=1)).reshape(-1, 1)\n",
    "C"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0. , 0.2, 0.4, 0.6, 0.8, 1. ],\n       [0. , 0.2, 0.4, 0.6, 0.8, 1. ],\n       [0. , 0.2, 0.4, 0.6, 0.8, 1. ],\n       [0. , 0.2, 0.4, 0.6, 0.8, 1. ]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKnLAyL7zgpa"
   },
   "source": [
    "## Exercício 2.6\n",
    "\n",
    "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
    "\n",
    "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lA5W9vxNEmOj"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(A):\n",
    "    '''\n",
    "    Aplica a função de softmax à matriz `A`.\n",
    "\n",
    "    Entrada:\n",
    "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
    "      independentemente e N é o tamanho de cada exemplo.\n",
    "\n",
    "    Saída:\n",
    "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
    "    '''\n",
    "    # evita estouro da exponenciação\n",
    "    A = (A - A.min(axis=1).reshape(-1, 1)) / (A.max(axis=1) - A.min(axis=1)).reshape(-1, 1)\n",
    "    # Expoente multiplicado por 10 para dispercar melhor os valores\n",
    "    A = np.exp(10 * A)\n",
    "    return A / A.sum(axis=1).reshape(-1, 1)"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpxlbh4ND54q"
   },
   "source": [
    "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L6EZ5ZD7HFao"
   },
   "source": [
    "A = np.array([[0.5, -1, 1000],\n",
    "              [-2, 0, 0.5]])\n",
    "softmax(A)"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[4.60811555e-05, 4.53957767e-05, 9.99908523e-01],\n       [3.99865265e-05, 1.19198156e-01, 8.80761858e-01]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j2uXmKH8HF4"
   },
   "source": [
    "O código a seguir verifica se sua implementação do softmax está correta.\n",
    "- A soma de cada linha de A deve ser 1;\n",
    "- Os valores devem estar entre 0 e 1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r-sN4STk7qyN"
   },
   "source": [
    "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5_ZRWRfCZtI"
   },
   "source": [
    "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bhUeyrGaJ3J2"
   },
   "source": [
    "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jaa-C8XkKJin"
   },
   "source": [
    "%%timeit\n",
    "softmax(A)"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 ms ± 3.25 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4XE6LaWi81zZ"
   },
   "source": [
    "SM = softmax(A)\n",
    "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Flr1lI5o-HpG"
   },
   "source": [
    "## Exercício 2.7\n",
    "\n",
    "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
    "\n",
    "| Decimal | Binary | One-hot\n",
    "| ------- | ------ | -------\n",
    "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
    "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
    "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
    "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
    "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
    "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
    "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
    "| 7 | 111    | 0 0 0 0 0 0 0 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CqXP_5ABbfo"
   },
   "source": [
    "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "la-02w7qCH7L"
   },
   "source": [
    "def one_hot(y, n_classes):\n",
    "    # converte para valores binarios\n",
    "    # Em binario, cada potencia ocupa 1 unico bit\n",
    "    y = 2 ** y\n",
    "    # retorna uma matriz em que cada elemento eh a representacao\n",
    "    # em binario do numero de entrada. Como sao potencias de 2,\n",
    "    # fica tudo em one-hot.\n",
    "    # O shift ali embaixo esta apenas aplicando mascara binaria\n",
    "    return ((y.reshape(-1, 1) & (1 << np.arange(n_classes))) > 0) // 1"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zf5zyZO5Aiz_"
   },
   "source": [
    "N_CLASSES = 9\n",
    "N_SAMPLES = 10\n",
    "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)\n",
    "print(y)\n",
    "print(one_hot(y, N_CLASSES))"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 2 8 8 0 8 0 7 6 8]\n",
      "[[0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nwuKnQUCzve"
   },
   "source": [
    "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uwuFy5rWC2tA"
   },
   "source": [
    "N_SAMPLES = 100_000\n",
    "N_CLASSES = 1_000\n",
    "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7azMtF7wDJ2_"
   },
   "source": [
    "%%timeit\n",
    "one_hot(y, N_CLASSES)"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.46 s ± 20.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqMroZay2ubi"
   },
   "source": [
    "## Exercício 2.8\n",
    "\n",
    "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
    "```\n",
    "array_a = np.array([-1, 1.5, 0])\n",
    "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
    "normalize = Normalizer(array_b)\n",
    "normalized_array = normalize(array_a)\n",
    "print(normalized_array)  # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qaedJ5Cf5Oy2"
   },
   "source": [
    "class Normalizer:\n",
    "    def __init__(self, b):\n",
    "        b = np.array(b)\n",
    "        self.b = b\n",
    "        self.std = b.std()\n",
    "        self.mean = b.mean()\n",
    "\n",
    "    def __call__(self, a):\n",
    "        a = np.array(a)\n",
    "        a_norm = (a - a.mean()) / a.std()\n",
    "        return a_norm * self.std + self.mean\n"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlkNNU6h5RbR"
   },
   "source": [
    "Mostre que seu código está correto com o exemplo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gad6zsbh5a0D"
   },
   "source": [
    "array_a = [-1, 1.5, 0]\n",
    "array_b = [1.4, 0.8, 0.3, 2.5]\n",
    "normalize = Normalizer(array_b)\n",
    "normalized_array = normalize(array_a)\n",
    "print(normalized_array)"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3187798  2.31425165 1.11696854]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrGVQFUYI_LP"
   },
   "source": [
    "# Parte 3:\n",
    "\n",
    "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
    "\n",
    "Nesta parte pode-se usar quaisquer bibliotecas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIlQdKAuCZtR"
   },
   "source": [
    "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF_-dJ2nCZtT"
   },
   "source": [
    "## Grafo computacional\n",
    "\n",
    "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
    "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
    "que pode ser reescrita como:\n",
    "$$ \\hat{y_i} = x_i w $$\n",
    "$$ e_i = \\hat{y_i} - y_i $$\n",
    "$$ e2_i = e_i^2 $$\n",
    "$$ J = \\sum_i e2_i $$\n",
    "\n",
    "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
    "regra da cadeia podemos escrever:\n",
    "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jboejVQMCZtU"
   },
   "source": [
    "```\n",
    "    y_pred = x * w\n",
    "    e = y_pred - y\n",
    "    e2 = e**2\n",
    "    J = e2.sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7JmU6qhc2Y2"
   },
   "source": [
    "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeeEBKl4CZtV"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yZun7wrCZtX"
   },
   "source": [
    "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/autograd.html"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T00:23:00.431853Z",
     "start_time": "2019-12-11T00:23:00.414813Z"
    },
    "id": "HlT2d-4fCZtZ"
   },
   "source": [
    "import torch"
   ],
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-11T00:23:00.863228Z",
     "start_time": "2019-12-11T00:23:00.844457Z"
    },
    "id": "xX0QwUduCZtf"
   },
   "source": [
    "torch.__version__"
   ],
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "'1.8.1'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsqzALS4CZtl"
   },
   "source": [
    "**Tensor com atributo .requires_grad=True**\n",
    "\n",
    "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T03:07:22.117010Z",
     "start_time": "2019-09-29T03:07:22.041861Z"
    },
    "id": "foaAb94aCZtm"
   },
   "source": [
    "y = torch.arange(0, 8, 2).float()\n",
    "y"
   ],
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 2., 4., 6.])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T03:07:28.610934Z",
     "start_time": "2019-09-29T03:07:28.598223Z"
    },
    "id": "no6SdSyICZtr"
   },
   "source": [
    "x = torch.arange(0, 4).float()\n",
    "x"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 1., 2., 3.])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T03:07:31.523762Z",
     "start_time": "2019-09-29T03:07:31.497683Z"
    },
    "id": "eL_i1mwGCZtw"
   },
   "source": [
    "w = torch.ones(1, requires_grad=True)\n",
    "w"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1.], requires_grad=True)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjEl-0l7CZt0"
   },
   "source": [
    "## Cálculo automático do gradiente da função perda J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pUh-SCnCZt1"
   },
   "source": [
    "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
    "\n",
    "Queremos calcular a derivada de $J$ em relação a $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMwwVtJ1CZt2"
   },
   "source": [
    "## Forward pass\n",
    "\n",
    "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T03:07:36.290122Z",
     "start_time": "2019-09-29T03:07:36.273229Z"
    },
    "id": "zp2aK4YhCZt3"
   },
   "source": [
    "# predict (forward)\n",
    "y_pred = x * w;\n",
    "print('y_pred =', y_pred)\n",
    "\n",
    "# cálculo da perda J: loss\n",
    "e = y_pred - y;\n",
    "print('e =', e)\n",
    "e2 = e.pow(2);\n",
    "print('e2 =', e2)\n",
    "J = e2.sum();\n",
    "print('J =', J)"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
      "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
      "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
      "J = tensor(14., grad_fn=<SumBackward0>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XC96wB7PCZt8"
   },
   "source": [
    "## Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-04T15:55:45.308858",
     "start_time": "2017-10-04T15:55:45.304654"
    },
    "id": "kKbf4D0CCZt-"
   },
   "source": [
    "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
    "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
    "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
    "\n",
    "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f-CjLPu6clVo"
   },
   "source": [
    "e2.retain_grad()\n",
    "e.retain_grad()\n",
    "y_pred.retain_grad()"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtsZS2Bicof-"
   },
   "source": [
    "E agora calculamos os gradientes com o `backward()`.\n",
    "\n",
    "w.grad é o gradiente de J em relação a w."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T03:07:40.267334Z",
     "start_time": "2019-09-29T03:07:40.247422Z"
    },
    "id": "Z1lnkb0GCZt_"
   },
   "source": [
    "if w.grad: w.grad.zero_()\n",
    "J.backward()\n",
    "print(w.grad)"
   ],
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-28.])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1xYDPR_uOcZ"
   },
   "source": [
    "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Enuk2tf0sDyO"
   },
   "source": [
    "print(e2.grad)\n",
    "print(e.grad)\n",
    "print(y_pred.grad)"
   ],
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n",
      "tensor([ 0., -2., -4., -6.])\n",
      "tensor([ 0., -2., -4., -6.])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsOThnt8fDJV"
   },
   "source": [
    "##Exercício 3.1\n",
    "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "hidden": true,
    "id": "62nZAfUoCZu5"
   },
   "source": [
    "def J_func(w, x, y):\n",
    "    return ((y - w * x) ** 2).sum()\n",
    "\n",
    "\n",
    "# Calcule o gradiente usando a regra diferenças finitas\n",
    "# Confira com o valor já calculado anteriormente\n",
    "x = torch.arange(0, 4).float()\n",
    "y = torch.arange(0, 8, 2).float()\n",
    "w = torch.ones(1)\n",
    "grad = (J_func(w + (10 ** -3), x, y) - J_func(w - (10 ** -3), x, y)) / (2 * (10 ** -3))\n",
    "print('grad=', grad)"
   ],
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad= tensor(-28.0008)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_Sx1QXZxJ3u"
   },
   "source": [
    "##Exercício 3.2\n",
    "\n",
    "Minimizando $J$ pelo gradiente descendente\n",
    "\n",
    "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
    "\n",
    "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
    "\n",
    "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PNszCOED1Wtu"
   },
   "source": [
    "learning_rate = 0.01\n",
    "iteracoes = 20\n",
    "\n",
    "x = torch.arange(0, 4).float()\n",
    "y = torch.arange(0, 8, 2).float()\n",
    "w = torch.ones(1)\n",
    "\n",
    "losses_list = []\n",
    "for i in range(iteracoes):\n",
    "    print('i =', i)\n",
    "    J = J_func(w, x, y)\n",
    "    print('J=', J)\n",
    "    grad = (J_func(w + (10 ** -3), x, y) - J_func(w - (10 ** -3), x, y)) / (2 * (10 ** -3))\n",
    "    print('grad =', grad)\n",
    "    w = w - grad * learning_rate\n",
    "    print('w =', w)\n",
    "    losses_list.append(J.detach().numpy())\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.close()\n",
    "plt.plot(losses_list)\n",
    "plt.xticks(range(iteracoes))\n",
    "plt.legend(['Loss', ], loc='upper right')\n",
    "plt.title(\"Gráfico de Loss por iteração\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iteração\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Plote o gráfico da loss J pela iteração i"
   ],
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "J= tensor(14.)\n",
      "grad = tensor(-28.0008)\n",
      "w = tensor([1.2800])\n",
      "i = 1\n",
      "J= tensor(7.2574)\n",
      "grad = tensor(-20.1611)\n",
      "w = tensor([1.4816])\n",
      "i = 2\n",
      "J= tensor(3.7621)\n",
      "grad = tensor(-14.5156)\n",
      "w = tensor([1.6268])\n",
      "i = 3\n",
      "J= tensor(1.9501)\n",
      "grad = tensor(-10.4510)\n",
      "w = tensor([1.7313])\n",
      "i = 4\n",
      "J= tensor(1.0109)\n",
      "grad = tensor(-7.5245)\n",
      "w = tensor([1.8065])\n",
      "i = 5\n",
      "J= tensor(0.5240)\n",
      "grad = tensor(-5.4175)\n",
      "w = tensor([1.8607])\n",
      "i = 6\n",
      "J= tensor(0.2716)\n",
      "grad = tensor(-3.9003)\n",
      "w = tensor([1.8997])\n",
      "i = 7\n",
      "J= tensor(0.1408)\n",
      "grad = tensor(-2.8083)\n",
      "w = tensor([1.9278])\n",
      "i = 8\n",
      "J= tensor(0.0730)\n",
      "grad = tensor(-2.0218)\n",
      "w = tensor([1.9480])\n",
      "i = 9\n",
      "J= tensor(0.0378)\n",
      "grad = tensor(-1.4557)\n",
      "w = tensor([1.9626])\n",
      "i = 10\n",
      "J= tensor(0.0196)\n",
      "grad = tensor(-1.0482)\n",
      "w = tensor([1.9731])\n",
      "i = 11\n",
      "J= tensor(0.0102)\n",
      "grad = tensor(-0.7546)\n",
      "w = tensor([1.9806])\n",
      "i = 12\n",
      "J= tensor(0.0053)\n",
      "grad = tensor(-0.5433)\n",
      "w = tensor([1.9860])\n",
      "i = 13\n",
      "J= tensor(0.0027)\n",
      "grad = tensor(-0.3912)\n",
      "w = tensor([1.9899])\n",
      "i = 14\n",
      "J= tensor(0.0014)\n",
      "grad = tensor(-0.2816)\n",
      "w = tensor([1.9928])\n",
      "i = 15\n",
      "J= tensor(0.0007)\n",
      "grad = tensor(-0.2028)\n",
      "w = tensor([1.9948])\n",
      "i = 16\n",
      "J= tensor(0.0004)\n",
      "grad = tensor(-0.1460)\n",
      "w = tensor([1.9962])\n",
      "i = 17\n",
      "J= tensor(0.0002)\n",
      "grad = tensor(-0.1051)\n",
      "w = tensor([1.9973])\n",
      "i = 18\n",
      "J= tensor(0.0001)\n",
      "grad = tensor(-0.0757)\n",
      "w = tensor([1.9981])\n",
      "i = 19\n",
      "J= tensor(5.3023e-05)\n",
      "grad = tensor(-0.0545)\n",
      "w = tensor([1.9986])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtUklEQVR4nO3deXxddZ3/8dcnW9MmadM1XWkKNC2llEIqIEVKwKUIsrjM4IDCqFPxp4KgMyw6CIPjoOMKIuPCoiIURRCsiGwpBSxbS4Eu0ALd6Q5pk6Zpts/vj3NSbtMkvfcmNyfJeT8fj/PIveeez/l+7pLPOfd7zv0ec3dERCQ+sqJOQEREupcKv4hIzKjwi4jEjAq/iEjMqPCLiMSMCr+ISMyo8EtazOxvZvalVvPONbP1ZlZjZseY2TIzOyXDedxhZt/JZBu9Tfj6Hxp1HgBm9mszW25m48zs8ajzkYAKf0yZ2Xlm9pyZ7TazreHt/2dmlkTsZ4Ed7n5Lq4d+AHzF3Qvd/SV3P9Ld52ci/65gZm5mh0edR1cLX/+3oEdsGIcB5wP3AH+IMA9JkBN1AtL9zOzrwH8AXwb+DtQA04FvALcCe9uIyXb3pvBuIfDFNlY9HliWgZSlDWaW4+6NPbkNdz8nvHli12QkXcLdNcVoAgYBu4FPHGS5O4BbgIfC5T8InAG8BOwC1gPXhsv2I9h4eLjsm+H8NcAHw9vZwNXAm0A1sAgYFz52IvACsDP8e2IHeR0DLA7XcQ8wF/hOwuNnAkuAKuAfwLQO1uXA4e28Rr8FtgFrgW8BWeFjhwNPhrluB+4J5xvwY2Br+NgrwNR22p0P/A/wfLjsA8CQhMfPItiAVoXLHpHw2BrginD9e4Gc9p4XMAdoAOrD9+cv4eOjgT+Fz281cElC7LXAvcCd4fv8BeA4YGGYzybgZ0BeQsyRwKPAO8AW4Opw/sHikn7fNXVxHYg6AU3d/IbDbKCxrYLRark7wn/ImQRdgvnAqcC08P60sMidkxCzXyFl/8L/78CrwKSwSB4NDAWGAO8CnyH4Bvrp8P7QNnLKCwvxZUAu8MmwsH0nfPzYMKfjCTY0F4Y59GvnObZX+H8bFuMioBRYCXw+fOxu4JsJr8lJ4fyPEGzMisPndwQwqp125wMbgalAQViE7wwfKyPYeH4ofI7/AbzRUjDD57MEGAf0P9jzCt/HxA1jVpjnNeHreSjwFvCR8PFrw9f0nHDZ/kA5cEL4/pQCK4CvhcsXERT1r4evRxFwfPhYR3FJv++aMlAHok5AUze/4XABsLnVvH8Q7JXtAU4O590B/PYg6/oJ8OOE+x0V/teBs9tYx2eA51vNWwhc1MayJwNvA9Yq95bCfwtwfauY14FZ7eR/QOEn2GDsBaYkzPsiMD+8/Vvgl8DYVnGnEmwgTiD8dtDB6zYfuCHh/hSCvfJs4D+BPyQ8lkWwkTgl4TX93EHW31HhPx5Y12r5q4Dbw9vXAgsOsv6vAfeHtz8NvJTkZy8xLun3XVPXTzq4Gz87gGFmtu/4jruf6O7F4WOJn4n1iYFmdmx4Ns8aM1sLXERw8C4Z4wi6eVobTbAXn2gtMKadZTd6WCUSlm0xHvi6mVW1TGG7o5PMEYLn0/LNoq18/oNgj/758KylzwG4+xMEXRk3A1vM7JdmNrCDdhJf27UEe/fDaPV6uHtzuOyYdmJTNR4Y3eo1uhooaW/9ZlZmZvPMbLOZ7QK+y3vve3vv68HiUnnfpYup8MfPQoI92rOTWLb10K33APMI9ibHA78hKILJWA8c1sb8twmKUaJDCPZyW9sEjGl15tEhrdr4b3cvTpgGuPvdSeYIQb99Q6uc9uXj7pvd/d/cfTTBN4Gft5wZ5O43uns5QZ93GUH3VnvGtVp/Q9j2fq9H+FzHsf/rkcqQuq2XXQ+sbvUaFbn7RzuIuQV4DZjo7gMJNhSWsL623teDxaXyvksXU+GPGXevAq4jKFifNLNCM8sys+kE/c0dKQb2uHujmR1H8DU/Wb8GrjeziRaYZmZDCQ4el5nZv5hZjpn9M0HXx7w21rGQ4PjEJeGyHyc4gNjiV8DFZnZ82EaBmZ1hZkUd5JVnZvktUzjvD8B/m1mRmY0HLic42ImZfcrMxobLvUtQJJvM7H1hu7kEffR1QBPtu8DMppjZAOC/gHs9OGvqD8AZZnZauK6vE2yo/9HBujqyhaAfv8XzwC4zu8LM+ptZtplNNbP3dbCOIoIDvTVmNhlI/P3GPGCkmX3NzPqFr9nxScSl8r5LV4u6r0lTNBPBudXPA7UEZ3c8R3AWSMtBxDtI6BsO532S4Ot4NcE/6M8ID0qGj3fUx59NcHbM6jD+BcJ+cuAkggOOO8O/J3WQ9wyCM4tazuq5h/37sGeH664i+IbwR6ConXV5G9MXgMEEhX4bwR7tNbx3Vs/3CfZKawi6OOaE808jONOmhmDP/fdAYTvtzue9s3p2AX8BhiU8fi6wPHw9ngSObOs17eA1Suzjn8h7Zzn9OZw3muAg9WaCjdezCe/TtYnvaTjvZII99xrgKYIN1dMJj08FHg/b3QVcmWRc0u+7pq6dLHwDRKSbmNl8guL666hz6Upm9gHgw+7+n1HnIh1TV4+IdJqZFQLrgIqoc5GDU+EXka5wHUH3lProewF19YiIxIz2+EVEYqZXDNI2bNgwLy0tTSt29+7dFBQc7CxFxSte8Yrv+vioc1i0aNF2dx9+wANRn1aUzFReXu7pqqysTDtW8YpXvOI7K8ocgBe9jZqqrh4RkZhR4RcRiRkVfhGRmOkVB3dFRNLV0NDAhg0bqKurSzl20KBBrFixolPtd3YdycTn5+czduxYcnNzk1qnCr+I9GkbNmygqKiI0tJSkrik9H6qq6spKupojL/Mr+Ng8e7Ojh072LBhAxMmTEhqnRnr6jGz28KLeC9t47FvhBe6TnYsdxGRtNTV1TF06NCUi35vYWYMHTo0pW80mezjv4NgpMT9mNk4gsvKrctg2yIi+/TVot8i1eeXscLv7gsILr7c2o8JrmKU8bEinnhtC/Peqs90MyIivUpGx+oxs1JgnrtPDe+fBZzm7pea2Rpghrtvbyd2DsH48JSUlJTPnTs35fbvfm0vT6xr4BcfKiArzS1+TU0NhYWFacUqXvGKjz5+zJgxHH744WnFNzU1kZ2dnXb7LesYO3YsmzZtymgOb7zxBjt37txvXkVFxSJ3n3HAwm39qqurJqAUWBreHkBwsY9B4f01JFx8oqMp3V/u3vP8Oh9/xTxfva0mrXj36H/5p3jFK75z8cuXL087fteuXZ1qv2UdBQUFGc+hredJD/jl7mHABODlcG9/LLDYzEZmqsGJJcGewsot1ZlqQkQkLUuWLOGEE05g2rRpnHvuubz77rsA3HjjjUyZMoVp06Zx3nnnAfDkk08yffp0pk+fzjHHHEN1dedqWredzunurwIjWu4frKunK0wsCU6BWrW1hg8fmalWRKS3uO4vy1j+9q6kl0+mm2XK6IF8+2OpF5jPfvaz3HTTTcyaNYtrrrmG6667jp/85CfccMMNrF69mn79+lFVVQXAD37wA26++WZmzpxJTU0N+fn5Ha/8IDJ5OufdBBfHnmRmG8zs85lqqz2F/XIYmm+8vll7/CLSc+zcuZOqqipmzZoFwIUXXsiCBQsAmDZtGueffz533nknOTnBvvnMmTO5/PLLufHGG6mqqto3P10Z2+N3908f5PHSTLWdaExhlrp6RAQg5T3zrvgBV6r++te/smDBAh588EGuv/56nn32Wa688krOOOMMHnroIU444QQee+wxJk+enHYbfX6sntGFWby1bTeNTc1RpyIiAgTDMAwePJinnnoKgN/97nfMmjWL5uZm1q9fT0VFBd///vepqqqipqaGN998k6OOOoorrriCGTNm8Nprr3Wq/T4/ZMOYQqO+qZm179Ry2PD0TwsTEUlXbW0tY8eO3Xf/8ssv5ze/+Q0XX3wxtbW1HHroodx+++00NTVxwQUXsHPnTtydyy67jOLiYq6++moqKyvJzs5mypQpnH766Z3Kp+8X/qLgS82qLdUq/CISiebmtnscnn322QPmPf300/vdr66u5qabburSfPp+V09B8BRXbqmJOBMRkZ6hzxf+/Bxj7OD+OsArIhLq84UfoKykiFXa4xeJLc/g0DQ9QarPLzaF/63tNTTozB6R2MnPz2fHjh19tvh7OB5/Kj/q6vMHdwHKSgppaHLW7tjN4SO695xcEYnW2LFj2bBhA9u2bUs5tq6urtO/ku3sOpKJb7kCV7JiUviDYr9yS40Kv0jM5ObmJn1lqtbmz5/PMccc06n2O7uOrsihtVh09Rw2vBAzDdYmIgIxKfz987I5ZMgAHeAVESEmhR9g4ogi7fGLiBCjwl9WUsjq7bupb9SZPSISbzEq/EU0Njurt++OOhURkUjFpvDralwiIoHYFP7DhheSZcFgbSIicRabwp+fm834oQUarE1EYi82hR9g4ohCVm7VHr+IxFusCn9ZSRFrd9Syt7Ep6lRERCKTyYut32ZmW81sacK8/zWz18zsFTO738yKM9V+W8pGFtHU7Ly1TWf2iEh8ZXKP/w5gdqt5jwJT3X0asBK4KoPtH6BMZ/aIiGSu8Lv7AuCdVvMecffG8O6zQPLDyXWBCcMKyM4yDd0gIrFmmRyj2sxKgXnuPrWNx/4C3OPud7YTOweYA1BSUlI+d+7ctHKoqamhsPC9a+1e9VQtowqyuOTY5IZJbR3f2fYVr3jFxyc+6hwqKioWufuMAx5w94xNQCmwtI353wTuJ9zwHGwqLy/3dFVWVu53/+Lfvein/G9lm8smE9/Z9hWveMXHJz7qHIAXvY2a2u1n9ZjZhcCZwPlhYt1qYkkRa3fspq5BZ/aISDx1a+E3s9nAFcBZ7l7bnW23KCsppNnhzW3q5xeReMrk6Zx3AwuBSWa2wcw+D/wMKAIeNbMlZvZ/mWq/Pe9djUtn9ohIPGXs0ovu/uk2Zt+aqfaSVTq0gJws09ANIhJbsfrlLkBeThYThhVosDYRia3YFX4Iunu0xy8icRXLwj+xpJD179ayp15n9ohI/MSy8JeVFOEOb2zVXr+IxE9sCz/ozB4RiadYFv7SoQPIy87S2PwiEkuxLPw52VkcOrxAg7WJSCzFsvBDMHSDunpEJI5iW/jLRhSy4d097N7bePCFRUT6kNgW/onhAV6d2SMicRPbwq+rcYlIXMW28I8fWkBeThartMcvIjET28KfnWUcNryQ1zdrj19E4iW2hR+C7h4N1iYicRPzwl/E2zvrqK5riDoVEZFuE+vCP3FEcIBX/fwiEiexLvwtY/aou0dE4iTWhX/ckAHk52ZpbH4RiZVYF/7sLOPwEYU6l19EYiXWhR+gbESRBmsTkVjJWOE3s9vMbKuZLU2YN8TMHjWzVeHfwZlqP1kTS4rYvKuOnXt0Zo+IxEMm9/jvAGa3mncl8Li7TwQeD+9HqmXohjc0Nr+IxETGCr+7LwDeaTX7bOA34e3fAOdkqv1kvXc1LnX3iEg8mLtnbuVmpcA8d58a3q9y9+KEx9919za7e8xsDjAHoKSkpHzu3Llp5VBTU0NhYWG7jze7c/Fjtcwam8P5R/RLOb6z7Ste8Yrvu/FR51BRUbHI3Wcc8IC7Z2wCSoGlCferWj3+bjLrKS8v93RVVlYedJmP3fSUn/+rZ9OO72z7ile84vtmfNQ5AC96GzW1u8/q2WJmowDCv1u7uf02TRyhq3GJSHx0d+F/ELgwvH0h8EA3t9+mspJCtlbvpaq2PupUREQyLpOnc94NLAQmmdkGM/s8cAPwITNbBXwovB85HeAVkTjJydSK3f3T7Tx0WqbaTNfEhKtxHTdhSMTZiIhkVux/uQswprg/BXnZGqxNRGJBhR8wMyaWFKmrR0RiQYU/VFZSyCr9eldEYkCFP1RWUsT2mnre2a0ze0Skb1PhD03cd2aP9vpFpG9T4Q+1DNamA7wi0tep8IdGDsynqF+ODvCKSJ+nwh8KzuzR1bhEpO9T4U9QVlLEqq3a4xeRvk2FP8HEkiLe2V3P9pq9UaciIpIxKvwJyhKGbhAR6atU+BPsG6xtswq/iPRdKvwJRhT1Y2B+DivVzy8ifZgKfwIzY9LIIp3LLyJ9mgp/Ky2DtXkGr0UsIhIlFf5WykYUsnNPA9uqdWaPiPRNKvyt6GpcItLXqfC3osHaRKSvU+FvZVhhHoMH5GpsfhHpsyIp/GZ2mZktM7OlZna3meVHkUdbdDUuEenrur3wm9kY4BJghrtPBbKB87o7j46UhYO16cweEemLourqyQH6m1kOMAB4O6I82lRWUkR1XSNbdunMHhHpeyyKvVozuxT4b2AP8Ii7n9/GMnOAOQAlJSXlc+fOTautmpoaCgsLU4pZsaOJ771Qxzdm9KM0vy7l+M62r3jFK75vxEedQ0VFxSJ3n3HAA+7erRMwGHgCGA7kAn8GLugopry83NNVWVmZcsz26joff8U8/9WCN9OK72z7ile84vtGfNQ5AC96GzU1iq6eDwKr3X2buzcA9wEnRpBHu4YW9mNoQR6rdIBXRPqgKAr/OuAEMxtgZgacBqyIII8OTSwp5HWdyy8ifVC3F353fw64F1gMvBrm8MvuzuNgJpUU8cZWjdkjIn1PThSNuvu3gW9H0XayJpYUUbO3kXfqcqNORUSkSyW1x29mBWaWFd4uM7OzzKxPV8SWMXs21jRHnImISNdKtqtnAZAf/vjqceBfgTsylVRP0HIZxo016uoRkb4l2cJv7l4LfBy4yd3PBaZkLq3oFQ/IY3hRP+3xi0ifk3ThN7P3A+cDfw3nRXJ8oDuVlRSq8ItIn5Ns4f8acBVwv7svM7NDgcqMZdVDTB45kA3Vzeypb4o6FRGRLpNU4Xf3J939LHf/XniQd7u7X5Lh3CJ36uQRNDTDkyu3Rp2KiEiXSfasnrvMbKCZFQDLgdfN7N8zm1r0jp8whMJc+NvSzVGnIiLSZZLt6pni7ruAc4CHgEOAz2QqqZ4iJzuLY0bk8MSKrextVHePiPQNyRb+3PC8/XOAB8IxdmJxnuOMkdlU723kmTe2R52KiEiXSLbw/wJYAxQAC8xsPLArU0n1JFOGZlPUL4e/varuHhHpG5I9uHuju49x94+Go32uBSoynFuPkJtlnHbECB5dsYWGJp3aKSK9X7IHdweZ2Y/M7MVw+iHB3n8szJ46iqraBp57652oUxER6bRku3puA6qBfwqnXcDtmUqqp5lVNpz+udn8bemmqFMREem0ZAv/Ye7+bXd/K5yuAw7NZGI9Sf+8bComD+fvy7bQ1ByLY9oi0oclW/j3mNlJLXfMbCbB9XJjY/bUUWyv2cuite9GnYqISKckO97OxcBvzWxQeP9d4MLMpNQznTp5BHk5Wfxt6SaOmzAk6nRERNKW7Fk9L7v70cA0YJq7HwOcmtHMepjCfjmcPHE4Dy/dTLO6e0SkF0vp0ovuviv8BS/A5RnIp0c7fepINu2s4+UNVVGnIiKSts5cc9e6LIte4oNHlJCTZTyssXtEpBfrTOFPu7/DzIrN7F4ze83MVoRj/fd4gwbkcuLhw/jb0s26CLuI9FodFn4zqzazXW1M1cDoTrT7U+Bhd58MHA2s6MS6utXpU0ey7p1alm+KxYgVItIHdVj43b3I3Qe2MRW5e1pX4DKzgcDJwK1hG/XuXpXOuqLw4SklZBnq7hGRXsu6u8vCzKYDvyQY1/9oYBFwqbvvbrXcHGAOQElJSfncuXPTaq+mpobCwsK0820r/obn97Brr/PdDwyIpH3FK17xvSM+6hwqKioWufuMAx5w926dgBlAI3B8eP+nwPUdxZSXl3u6Kisr045tL/6OZ1b7+Cvm+aotuyJpX/GKV3zviI86B+BFb6Omdubgbro2ABvc/bnw/r3AsRHkkbaPHDkSQEM1i0iv1O2F3903A+vNbFI46zSCbp9eY+SgfI49pFiXZBSRXimKPX6ArwK/N7NXgOnAdyPKI22nTx3F8k27WLejNupURERSEknhd/cl7j7D3ae5+znu3utGPps9Nezu0VDNItLLRLXH3+uNGzKAo8YMUnePiPQ6KvydMHvqSJasr+LtqliNUC0ivZwKfyecHnb36MdcItKbqPB3wqHDC5lUUqTCLyK9igp/J82eOpIX1r7D1uq6qFMREUmKCn8nnX7USNzhkWVbok5FRCQpKvydNKmkiAnDCtTdIyK9hgp/J5kZs6eOZOFbO3h3d33U6YiIHJQKfxc4fepImpqdR1eou0dEej4V/i5w1JhBjCnur+4eEekVVPi7gJlx+tSRPL1qO9V1DVGnIyLSIRX+LnL6USOpb2rmide2Rp2KiEiHVPi7yDHjBlMysJ/G6BeRHk+Fv4tkZRkfOXIk81dupba+Mep0RETapcLfhWZPHUldQzPzX98WdSoiIu1S4e9Cx5UOYUhBnoZqFpEeTYW/C+VkZ/HhKSU8sWILdQ1NUacjItImFf4uNnvqSHbXN/H0qu1RpyIi0iYV/i524mHDKMrPUXePiPRYkRV+M8s2s5fMbF5UOWRCXk4WHzqihMdWbKGhqTnqdEREDhDlHv+lwIoI28+Y2VNHsnNPAwvf3BF1KiIiB4ik8JvZWOAM4NdRtJ9pJ5cNpyAvW909ItIjmbt3f6Nm9wL/AxQB33D3M9tYZg4wB6CkpKR87ty5abVVU1NDYWFh2rmmG//zJXW89k4T33mfM7Co+9tXvOIVH3181DlUVFQscvcZBzzg7t06AWcCPw9vnwLMO1hMeXm5p6uysjLt2M7Ez3v5bR9/xTy/5U+PRdK+4hWv+Ojjo84BeNHbqKlRdPXMBM4yszXAXOBUM7szgjwy6pRJw+mXk8WLmzV8g4j0LN1e+N39Kncf6+6lwHnAE+5+QXfnkWkF/XKYVTacF7Y0sadeP+YSkZ5D5/Fn0OdPmsDOvc6NT6yKOhURkX0iLfzuPt/bOLDbVxx/6FA+MCaHXy14i9c274o6HRERQHv8GffPk/IY2D+Xq+97lebm7j+DSkSkNRX+DCvMM751xhEsXlfFXc+vizodEREV/u5w7jFjmHn4UL738Gts3VUXdToiEnMq/N3AzPjOOUext7GZ6+YtjzodEYk5Ff5uMmFYAV+tOJy/vrKJSl2QXUQipMLfjebMOpTDRxTyrT8v1XV5RSQyKvzdqF9ONt899yg2Vu3hp4/r3H4RiYYKfzc7bsIQznvfOH791GqWv61z+0Wk+6nwR+DK0ydT3D+Xq+9/lSad2y8i3UyFPwLFA/L4zzOnsGR9FXc9tzbqdEQkZlT4I3L29NF8YOIwvv/w62zRuf0i0o1U+CMSnNs/lfqmZq77y7Ko0xGRGFHhj9D4oQVcctpEHnp1M4+v2BJ1OiISEyr8Efu3DxxKWUkh1zywjN17dW6/iGSeCn/E8nKy9p3b/5PHVkadjojEgAp/DzCjdAifPu4QbntmDUs37ow6HRHp41T4e4grZ09m8IA8vqlz+0Ukw1T4e4hBA3K55mNTeHnDTn63cE3U6YhIH6bC34N8bNooTi4bzg8eWcmmnXuiTkdE+igV/h7EzPjO2VNpaGrmugc1br+IZEa3F34zG2dmlWa2wsyWmdml3Z1DT3bI0AFc+sGJPLxsM48u17n9ItL1otjjbwS+7u5HACcAXzazKRHk0WP92wcOZVJJEdc8sJQ9jTrQKyJdq9sLv7tvcvfF4e1qYAUwprvz6Mlys7P47sePYtPOOu5bVR91OiLSx5h7dHuUZlYKLACmuvuuVo/NAeYAlJSUlM+dOzetNmpqaigsLEw7xyjjf7t8L0+sa+SjE3L5ZFkuWWbd2r7iFa/4zsVHnUNFRcUid59xwAPuHskEFAKLgI8fbNny8nJPV2VlZdqxUcfXNzb5v/7sYR9/xTz/3O3P+6499d3avuIVr/jOxUedA/Cit1FTIzmrx8xygT8Bv3f3+6LIoTfIzc7is0f24/qzj2T+ym184pZ/sG5HbdRpiUgvF8VZPQbcCqxw9x91d/u90WfeX8pvP3ccW3bt5eybn2bhmzuiTklEerEo9vhnAp8BTjWzJeH00Qjy6FVmHj6MP395JkMK8vjMrc9x13Prok5JRHqpnO5u0N2fBlI/SilMGFbA/V+eyVfveomr73+VlVuq+dYZR5CTrd/hiUjyVDF6mYH5udx20fv4wkkTuOMfa7jo9hfYWdsQdVoi0ouo8PdC2VnGt86cwvc/MY3nVu/gnJ8/w5vbaqJOS0R6CRX+Xuyf3jeOu/7tBHbuaeCcm59hwcptUackIr2ACn8v977SITzw5ZmMKe7PRbc/z21Pr275nYSISJtU+PuAcUMG8KcvnchpR5TwX/OWc9V9r1Lf2Bx1WiLSQ6nw9xEF/XL4xQXlfLniMOa+sJ4Lbn2Od3ZrnB8ROZAKfx+SlWX8+0cm89PzprNkfRVn/exp1ldrz19E9qfC3wedPX0Mf/ji+6lvbObaf+zhsnuW8PL6qqjTEpEeQoW/j5o+rpi/fPUkTj0kh0eXb+Hsm5/h3J8/wwNLNqr/XyTmVPj7sJKB+Zx/RD8WXnUq135sClW1DVw6dwknfe8JfvrYKrZV7406RRGJQLcP2SDdryg/l4tmTuCz7y/lyVXbuOOZNfz4sZXcXPkGZ04bxUUzS5k2tjjqNEWkm6jwx0hWllExaQQVk0bw5rYafrdwLX98cT33vbSRYw8p5qKZEzh96khyNfaPSJ+mwh9Thw0v5NqzjuTrHy7j3kUb+M0/1nDJ3S9RMrAf5x8/nk8fdwjDi/pFnaaIZIAKf8wV5efyrzMncOH7S3ly5TZu/8cafvToSn72xBucefQojshtYmZTs74FiPQhKvwChN1Ak0dQMXkEb2yt4bcL1/CnRRu4r76JHy7+O9PGFHPM+GKOPWQwxx4yWN8GRHoxFX45wOEjCvmvs6fyjY9M4pb7n6SucBSL11Vx29Or+UXTWwCMG9J/30bg2EMGM3lUkb4ViPQSKvzSroH5uRw/KodTTjkSgLqGJpa9vZPFa6tYvO5dnn1rBw8seRuA/Nwspo1t+UZQzLHjBzOsUN8KRHoiFX5JWn5uNuXjh1A+fggA7s7bO+tYvPZdFq97l8Xrqrj16bf4v6ZgdNBDhgxgZN5eFu5ZwehB/Rld3J9Rg/IZU9yf4gG5BJdfFpHupsIvaTMzxhT3Z0xxfz529Ggg+FawdOPOYEOwtopFb21hydNrqG/a/9fC+blZjC7uH24Q8hk1KFjPqOL8ffP752VH8bRE+rxICr+ZzQZ+CmQDv3b3G6LIQ7pefm42M0qHMKM0+FYwf/58Tj55Ftt372VTVR1vV+3h7Z3B300797Cxqo75r29jW81eWl9GYPCAXAqyGhn9+kIG5udQlJ9LUX4OA8O/Rfm5DOy///yW5fJzs/SNQqQd3V74zSwbuBn4ELABeMHMHnT35d2di3SPrCxjRFE+I4ryOXpccZvL1Dc2s2VXHRvDDcLb4UZi+eqNGLCxqo7qumqq6xqprmug+SDXmsnNNoryc8lubmDwS0+Sl5NFv5xs+uVkhVM2/XITbudkhfcTlsnN5q2NDexcspHsLCMny8jJyiI7O7id3XI/K+F+djCv5f6OPcHzMoMss3AKvi1lZwW3s8zafFwkU6LY4z8OeMPd3wIws7nA2YAKf4zl5WQxbsgAxg0ZsN/8+fN3cMop799vnruzu76J6roGqusa2bUn/FvXwK5ww9Ay/811Gxk8tJC9jc3sbWxib0Mz1XWNwe3GZvY2NFPf1MzehuB+Y1tblFeXdO7JPfl4WmHZWQbuZD36EEawcTDjvdsEGwgDSNiAJM5vaKgn7+nHAPY91nI7CLNW9/ff6NTV1dH/uSf23U/cHhn7b5z2fyywZ88eBrw4v8Pn2dEmrra2lgGLOo7vSG1tLQWdjV/8ZNrxALW7axnQiXV8qrSJUzqVwYGsuy/TZ2afBGa7+xfC+58Bjnf3r7Rabg4wB6CkpKR87ty5abVXU1NDYWFh2vkqPl7xTc1OYzM0NENDs1NVXUv/AQNoaoYmd5odmpz9/zY7Te3M31O3l7x+/Wh2cMA9mJohnOf7zfNwueZw3t76enJz82j5Lw22S074J1hn+Jgn3g9v1zc0kJubu+8+CcuTEEerx1rmNTQ2kJuTe0Cct15LG+sI4hvJzWl///Jg5aehsZGcDuIPprGT8Z1tvytyOG1kI5NHpvc/UFFRscjdZxzwgLt36wR8iqBfv+X+Z4CbOoopLy/3dFVWVqYdq3jFK17xnRVlDsCL3kZNjeIXNxuAcQn3xwJvR5CHiEgsRVH4XwAmmtkEM8sDzgMejCAPEZFY6vaDu+7eaGZfAf5OcDrnbe6+rLvzEBGJq0jO43f3h4CHomhbRCTuNKqWiEjMqPCLiMSMCr+ISMyo8IuIxEy3/3I3HWa2DVibZvgwYHsnmle84hWv+M6IMofx7j78gLlt/aqrL02088s1xSte8YrPdHxPyaH1pK4eEZGYUeEXEYmZOBT+Xype8YpXfETxPSWH/fSKg7siItJ14rDHLyIiCVT4RURipk8XfjObbWavm9kbZnZlirG3mdlWM1uaZtvjzKzSzFaY2TIzuzTF+Hwze97MXg7jr0sjh2wze8nM5qUaG8avMbNXzWyJmb2YRnyxmd1rZq+Fr8P7Dx61L3ZS2G7LtMvMvpZi+5eFr91SM7vbzPJTjL80jF2WTNttfWbMbIiZPWpmq8K/g1OM/1TYfrOZHXglpYPH/2/4+r9iZvebWXGK8deHsUvM7BEzG51KfMJj3zAzN7NhKbZ/rZltTPgcfDTV9s3sq2EdWGZm30+x/XsS2l5jZktSjJ9uZs+2/A+Z2XEpxh9tZgvD/8O/mNnA9uJT0tXnh/aUiWDI5zeBQ4E84GVgSgrxJwPHAkvTbH8UcGx4uwhYmWL7BhSGt3OB54ATUszhcuAuYF6az2ENMKwT78FvgC+Et/OA4k68l5sJfoySbMwYYDXQP7z/B+CiFOKnAkuBAQSj2D4GTEz1MwN8H7gyvH0l8L0U448AJgHzgRlptP9hICe8/b002h+YcPsS4P9SiQ/njyMYhn1tR5+ndtq/FvhGku9ZW/EV4XvXL7w/ItX8Ex7/IXBNiu0/Apwe3v4oMD/F+BeAWeHtzwHXJ/sZ7mjqy3v8+y7q7u71QMtF3ZPi7guAd9Jt3N03ufvi8HY1sIKgGCUb7+5eE97NDaekj8Sb2VjgDODXSSfdhcI9k5OBWwHcvd7dq9Jc3WnAm+6e6q+3c4D+ZpZDUMBTudLbEcCz7l7r7o3Ak8C5HQW085k5m2ADSPj3nFTi3X2Fu7+eTMLtxD8S5g/wLMEV71KJ35Vwt4AOPoMd/M/8GPiPjmIPEp+UduK/BNzg7nvDZbam076ZGfBPwN0pxjvQspc+iA4+g+3ETwIWhLcfBT7RXnwq+nLhHwOsT7i/gRQKb1cys1LgGIK99lTissOvlluBR909lfifEPyzNafSZisOPGJmi8xsToqxhwLbgNvD7qZfm1lBmnmcRwf/cG1x943AD4B1wCZgp7s/ksIqlgInm9lQMxtAsLc27iAxbSlx901hTpuAEWmso6t8DvhbqkFm9t9mth44H7gmxdizgI3u/nKq7Sb4StjddFtHXWXtKAM+YGbPmdmTZva+NHP4ALDF3VelGPc14H/D1+8HwFUpxi8Fzgpvf4r0PoMH6MuF39qY1+3nrppZIfAn4Gut9p4Oyt2b3H06wV7acWY2Nck2zwS2uvuiVPNtZaa7HwucDnzZzE5OITaH4GvrLe5+DLCboKsjJRZcnvMs4I8pxg0m2NueAIwGCszsgmTj3X0FQdfIo8DDBF2FjR0G9WBm9k2C/H+faqy7f9Pdx4WxX0mhzQHAN0lxY9HKLcBhwHSCDfgPU4zPAQYDJwD/Dvwh3HtP1adJcecj9CXgsvD1u4zwG3AKPkfwv7eIoMu4Po0cDtCXC3/kF3U3s1yCov97d78v3fWEXSTzgdlJhswEzjKzNQRdXKea2Z1ptPt2+HcrcD9B91myNgAbEr6l3EuwIUjV6cBid9+SYtwHgdXuvs3dG4D7gBNTWYG73+rux7r7yQRfwVPd2wPYYmajAMK/7XY1ZIqZXQicCZzvYWdxmu4ita6Gwwg2vC+Hn8WxwGIzG5nsCtx9S7gD1Az8itQ+gxB8Du8Lu06fJ/gG3O4B5raEXYUfB+5JsW2ACwk+exDsvKSUv7u/5u4fdvdygg3Pm2nkcIC+XPgjvah7uFdxK7DC3X+URvzwljMwzKw/QSF7LZlYd7/K3ce6eynB837C3ZPe2w3bLDCzopbbBAcJkz7Dyd03A+vNbFI46zRgeSo5hNLd01oHnGBmA8L34jSC4yxJM7MR4d9DCP7x08njQYJ/fsK/D6SxjrSZ2WzgCuAsd69NI35iwt2zSPIzCODur7r7CHcvDT+LGwhOeNicQvujEu6eSwqfwdCfgVPDdZURnGSQ6kiXHwRec/cNKcZBsLM5K7x9KinuPCR8BrOAbwH/l0YOB+qKI8Q9dSLol11JsJX8ZoqxdxN8tWwg+MB+PsX4kwi6ll4BloTTR1OInwa8FMYvpYOzCQ6ynlNI46wegj76l8NpWaqvX7iO6cCL4XP4MzA4xfgBwA5gUJrP/TqCQrUU+B3hmR0pxD9FsLF6GTgtnc8MMBR4nOAf/nFgSIrx54a39wJbgL+nGP8GwbGuls9gR2fltBX/p/D1ewX4CzAm3f8ZDnKWWDvt/w54NWz/QWBUivF5wJ3hc1gMnJpq/sAdwMVpvv8nAYvCz9BzQHmK8ZcS1LCVwA2Eoy10dtKQDSIiMdOXu3pERKQNKvwiIjGjwi8iEjMq/CIiMaPCLyISMyr8EjtmVhP+LTWzf+mG9vLM7CEze9zMfprp9kQORqdzSuyYWY27F5rZKQQjP56ZQmy2uzdlLDmRbqA9fomzGwgG8Fpiwdj92RaMX/9COCjYFwHM7BQLrq1wF8GPiTCzP4eD1y1LHMDOgmtALLbgOgoPhfM+Fg4S9pKZPWZmJeH8IeF6XgnHbJ/W/S+BxJH2+CV22tvjDwv4CHf/jpn1A54hGBFxPPBXYKq7rw6XHeLu74TDabxA8LP8LIJfKp/s7msTlhkMVLm7m9kXgCPc/etmdhOw3d2vM7NTgR95MCifSEblRJ2ASA/yYWCamX0yvD8ImEgwIuLzLUU/dImZtYzPPy5cbjjwlIfXDXD3lrHVxwL3hOPO5BFcIAaCn/N/Ilz2iXAI6EHuvjMzT08koK4ekfcY8FV3nx5OE/y9Mfx371so+KbwQeD97n40wZhK+bQ9FDjATcDP3P0o4IvhsrSzvL6CS8ap8EucVROMcd7i78CXwuG0MbOydi4eMwh4191rzWwywVjvAAsJjhmMD+OHJCy/Mbx9YcJ6FhBc3KRlY7LdU7xmg0g61NUjcfYK0GhmLxOMwPhToJRgzHgjuILYOW3EPQxcbGavAK8TXNIQd99mZhcDfw6H032JYBz8a4E/mtnGcNkJ4XquJbhC2StALftvFEQyRgd3RTLAzH4I/Jf666UnUlePSBczs7uBjwG5Ueci0hbt8YuIxIz2+EVEYkaFX0QkZlT4RURiRoVfRCRmVPhFRGLm/wMYOv+VTc7p+QAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBXxBmWGK3IU"
   },
   "source": [
    "##Exercício 3.3\n",
    "\n",
    "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lMP4d5vtHtqy"
   },
   "source": [
    "learning_rate = 0.01\n",
    "iteracoes = 20\n",
    "\n",
    "x = torch.arange(0, 4).float()\n",
    "y = torch.arange(0, 8, 2).float()\n",
    "w = torch.ones(1, requires_grad=True)\n",
    "\n",
    "losses_list = []\n",
    "for i in range(iteracoes):\n",
    "    print('i =', i)\n",
    "    J = J_func(w, x, y)\n",
    "    w.retain_grad()\n",
    "    J.backward()\n",
    "    print('J=', J)\n",
    "    grad = w.grad\n",
    "    print('grad =', grad)\n",
    "    w = w - grad * learning_rate\n",
    "    print('w =', w)\n",
    "    losses_list.append(J.detach().numpy())\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.close()\n",
    "plt.plot(losses_list)\n",
    "plt.xticks(range(iteracoes))\n",
    "plt.legend(['Loss', ], loc='upper right')\n",
    "plt.title(\"Gráfico de Loss por iteração\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iteração\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Plote aqui a loss pela iteração"
   ],
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "J= tensor(14., grad_fn=<SumBackward0>)\n",
      "grad = tensor([-28.])\n",
      "w = tensor([1.2800], grad_fn=<SubBackward0>)\n",
      "i = 1\n",
      "J= tensor(7.2576, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-20.1600])\n",
      "w = tensor([1.4816], grad_fn=<SubBackward0>)\n",
      "i = 2\n",
      "J= tensor(3.7623, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-14.5152])\n",
      "w = tensor([1.6268], grad_fn=<SubBackward0>)\n",
      "i = 3\n",
      "J= tensor(1.9504, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-10.4509])\n",
      "w = tensor([1.7313], grad_fn=<SubBackward0>)\n",
      "i = 4\n",
      "J= tensor(1.0111, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-7.5247])\n",
      "w = tensor([1.8065], grad_fn=<SubBackward0>)\n",
      "i = 5\n",
      "J= tensor(0.5241, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-5.4178])\n",
      "w = tensor([1.8607], grad_fn=<SubBackward0>)\n",
      "i = 6\n",
      "J= tensor(0.2717, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-3.9008])\n",
      "w = tensor([1.8997], grad_fn=<SubBackward0>)\n",
      "i = 7\n",
      "J= tensor(0.1409, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-2.8086])\n",
      "w = tensor([1.9278], grad_fn=<SubBackward0>)\n",
      "i = 8\n",
      "J= tensor(0.0730, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-2.0222])\n",
      "w = tensor([1.9480], grad_fn=<SubBackward0>)\n",
      "i = 9\n",
      "J= tensor(0.0379, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-1.4560])\n",
      "w = tensor([1.9626], grad_fn=<SubBackward0>)\n",
      "i = 10\n",
      "J= tensor(0.0196, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-1.0483])\n",
      "w = tensor([1.9730], grad_fn=<SubBackward0>)\n",
      "i = 11\n",
      "J= tensor(0.0102, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-0.7548])\n",
      "w = tensor([1.9806], grad_fn=<SubBackward0>)\n",
      "i = 12\n",
      "J= tensor(0.0053, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-0.5434])\n",
      "w = tensor([1.9860], grad_fn=<SubBackward0>)\n",
      "i = 13\n",
      "J= tensor(0.0027, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-0.3913])\n",
      "w = tensor([1.9899], grad_fn=<SubBackward0>)\n",
      "i = 14\n",
      "J= tensor(0.0014, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-0.2817])\n",
      "w = tensor([1.9928], grad_fn=<SubBackward0>)\n",
      "i = 15\n",
      "J= tensor(0.0007, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-0.2028])\n",
      "w = tensor([1.9948], grad_fn=<SubBackward0>)\n",
      "i = 16\n",
      "J= tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-0.1460])\n",
      "w = tensor([1.9962], grad_fn=<SubBackward0>)\n",
      "i = 17\n",
      "J= tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-0.1052])\n",
      "w = tensor([1.9973], grad_fn=<SubBackward0>)\n",
      "i = 18\n",
      "J= tensor(0.0001, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-0.0757])\n",
      "w = tensor([1.9981], grad_fn=<SubBackward0>)\n",
      "i = 19\n",
      "J= tensor(5.3059e-05, grad_fn=<SumBackward0>)\n",
      "grad = tensor([-0.0545])\n",
      "w = tensor([1.9986], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GulfYtzBMx2e"
   },
   "source": [
    "##Exercício 3.4\n",
    "\n",
    "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXQGEyvtiTAR"
   },
   "source": [
    "Resposta:   Valores muito grandes de learning rate (LR) poderiam fazer o gradiente oscilar em torno do valor ideal e nunca se aproximar da solução.\n",
    "            Valores muito pequenos de LR poderiam resultar em erros no cálculo devido a instabilidades numéricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsrSF8GEiXk4"
   },
   "source": [
    "##Exercício 3.5\n",
    "\n",
    "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
    "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
    ">\n",
    "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4Pna3bcicHj"
   },
   "source": [
    "Resposta (justifique):\n",
    "\n",
    "a) Para obter o valor do erro, precisamos calcular a função de erro, com O(N), e repetí-la para cada um dos N parâmetros que terão seus gradientes avaliados, gerando custo final de O(N^2).\n",
    "\n",
    "b) Para calcular o erro (loss), temos O(N). Para atualizar cada um dos N parâmetros, também temos O(N).\n",
    "Logo, o custo final também é O(N^2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35I5w8EZdjIo"
   },
   "source": [
    "##Exercício 3.6\n",
    "\n",
    "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
    "\n",
    "A equação da entropia cruzada é:\n",
    "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
    "Onde:\n",
    "\n",
    "- K é o número de classes;\n",
    "\n",
    "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
    "\n",
    "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
    "\n",
    "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
    "\n",
    "- K = número de classes\n",
    "\n",
    "- B = batch size\n",
    "\n",
    "- D = dimensão de qualquer vetor do modelo\n",
    "\n",
    "- LR = learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swTOphiVs6eN"
   },
   "source": [
    "Resposta: A probabilidade de cada classe ser predita em um modelo inicializado aleatoriamente é de 1/K, ou seja, p_i = 1/K.\n",
    "y_j vale 1 para apenas uma classe em cada iteração, logo: L = -log(1/K) = log(K)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UNdHqgSB6S9"
   },
   "source": [
    "Fim do notebook."
   ]
  }
 ]
}