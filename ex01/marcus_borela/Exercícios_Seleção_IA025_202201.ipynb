{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercícios - Seleção - IA025 - 202201",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex01/marcus_borela/Exerc%C3%ADcios_Sele%C3%A7%C3%A3o_IA025_202201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoyGt5gXMgK"
      },
      "source": [
        "## Coloque seu nome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe647368-7837-48a3-8280-207e80db21f7"
      },
      "source": [
        "print('Meu nome: Marcus Vinícius Borela de Castro')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome: Marcus Vinícius Borela de Castro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1.1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "S8qnIRkFUib_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j"
      },
      "source": [
        "def top_k(L, k):\n",
        "    # Escreva aqui o código\n",
        "    return dict(Counter(L).most_common(k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a402ca-715f-4330-e226-214b3df71e96"
      },
      "source": [
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L, k=k)\n",
        "print(f'resultado: {resultado}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6fd2bb-dafb-4996-ebb5-e0980fb65af8"
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 427 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 1.2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "BuFzyS_bVQ02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzv89trtTPc"
      },
      "source": [
        "def tokens_to_ids(text, vocabulary):\n",
        "    return [vocabulary[palavra] if palavra in vocabulary else vocabulary['unknown'] for palavra in re.findall(r\"\\w+|[^\\s\\w]\",text.lower(), re.IGNORECASE) ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d481fa-54f4-4d41-ff6e-f49e76c157a8"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "print(tokens_to_ids(D, V))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 2, 4, -1, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aafae849-2609-4242-c927-3e9f275873db"
      },
      "source": [
        "%%timeit\n",
        "resultado = tokens_to_ids(D, V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 1.98 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 1.3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time, strftime"
      ],
      "metadata": {
        "id": "GA29UIsUWIoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "print (io.DEFAULT_BUFFER_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0glCW7XBWJpG",
        "outputId": "1e36eabc-165b-483d-c1e7-d83b67ab880a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_blocks(files, size=65536):\n",
        "  while True:\n",
        "      b = files.read(size)\n",
        "      if not b: break\n",
        "      yield b\n",
        "\n",
        "def retorna_num_linhas_arquivo(path:str) -> int:\n",
        "  num_linhas = 0\n",
        "  with open(path, \"r\",encoding=\"utf-8\",errors='ignore') as f:\n",
        "      num_linhas += sum(bl.count(\"\\n\") for bl in get_blocks(f))\n",
        "  return num_linhas\n",
        "\n",
        " \n",
        "def retorna_linhas_indicadas_de_arquivo(path:str, lista_indice_dist_uniforme:list, total_selecao, ordenada_por_posicao_arquivo:bool=False)-> list:\n",
        "  lista_linhas_posicao_conteudo = []\n",
        "  lista_conteudo = []\n",
        "  cnt_linha_selecionada = 0\n",
        "  lista_indice_ordenada = sorted(lista_indice_dist_uniforme)\n",
        "  proxima_linha_selecionada = lista_indice_ordenada[cnt_linha_selecionada]\n",
        "  # print(f\"lista_indice_ordenada {lista_indice_ordenada}\")\n",
        "  \n",
        "  for cnt_linha_lida, linha in enumerate(open(path, buffering=1)): # line buffering   \n",
        "      # print(f\"cnt_linha_lida: {cnt_linha_lida}\") \n",
        "      while cnt_linha_lida == proxima_linha_selecionada: \n",
        "        lista_linhas_posicao_conteudo.append((lista_indice_dist_uniforme.index(proxima_linha_selecionada),linha))\n",
        "        lista_conteudo.append(linha)\n",
        "        cnt_linha_selecionada += 1\n",
        "        if cnt_linha_selecionada == total_selecao:\n",
        "          break\n",
        "        else:\n",
        "          proxima_linha_selecionada = lista_indice_ordenada[cnt_linha_selecionada]\n",
        "          # print(f\"proxima_linha_selecionada: {proxima_linha_selecionada} cnt_linha_selecionada: {cnt_linha_selecionada}\")\n",
        "\n",
        "  # print(f\"len lista_linhas_posicao_conteudo {len(lista_linhas_posicao_conteudo)}\")\n",
        "  \n",
        "  if not ordenada_por_posicao_arquivo:\n",
        "    return [item[1] for item in sorted(lista_linhas_posicao_conteudo, key=lambda x: x[0])]\n",
        "  else:\n",
        "    return lista_conteudo\n",
        "\n",
        "\n",
        "def sample(path: str, k: int, ordenada_por_posicao_arquivo:bool=False):    \n",
        "  num_linhas = retorna_num_linhas_arquivo(path)\n",
        "  # print(f'num_linhas: {num_linhas} k: {k}')\n",
        "  lista_indice_dist_uniforme = list(np.random.choice(num_linhas, k, replace=True))\n",
        "  # print(f\"len(lista_indice_dist_uniforme) {len(lista_indice_dist_uniforme)}\")\n",
        "  # print(lista_indice_dist_uniforme)\n",
        "  return retorna_linhas_indicadas_de_arquivo(path, lista_indice_dist_uniforme, k, ordenada_por_posicao_arquivo)"
      ],
      "metadata": {
        "id": "FDryTx60WV2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f8b5a31-e44f-497a-b25f-b859eed57bc4"
      },
      "source": [
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['line 8\\n', 'line 38\\n', 'line 81\\n', 'line 98\\n', 'line 61\\n', 'line 56\\n', 'line 83\\n', 'line 7\\n', 'line 54\\n', 'line 31\\n']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000\n",
        "n_samples = 10000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d20a6c-c4f3-4760-c893-051ffca158b5"
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 1.11 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 2.1\n",
        "\n",
        "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resposta:\n",
        "\n",
        "- número de somas: m*p*(n - 1) = m*p*n - m*p\n",
        "  \n",
        "   Explicação: nova matriz vai ter mxp e para cada célula houve (n - 1) adições\n",
        "\n",
        "- número de multiplicações: m*p*n = m*p*n  \n",
        "  \n",
        "   Explicação: nova matriz vai ter mxp e para cada célula houve n multiplicações\n",
        "\n",
        "Flops total: multiplicação + adição = \n",
        "     mpn + (mpn - mp) = 2mpn - mp = mp(2n - 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "jwU6rCUSXPBG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2.2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6534635-949e-4609-ccf3-6d824064f6e2"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa7a5afe-71d7-4f04-9915-d766bf3fe1c6"
      },
      "source": [
        "np.mean(A, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.5,  8.5, 14.5, 20.5])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 2.3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2dd4810-8262-4154-e25c-311be5a4d86e"
      },
      "source": [
        "C = (A - A.min()) / A.max() - A.min()\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.04347826 0.08695652 0.13043478 0.17391304 0.2173913 ]\n",
            " [0.26086957 0.30434783 0.34782609 0.39130435 0.43478261 0.47826087]\n",
            " [0.52173913 0.56521739 0.60869565 0.65217391 0.69565217 0.73913043]\n",
            " [0.7826087  0.82608696 0.86956522 0.91304348 0.95652174 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 2.4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31380c2e-ca7e-4496-81eb-b44a66462311"
      },
      "source": [
        "(A - A.min(axis=0)) / (A.max(axis=0) - A.min(axis=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n",
              "        0.33333333],\n",
              "       [0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n",
              "        0.66666667],\n",
              "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "        1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 2.5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1012c3d9-30ca-42af-a444-516ecceba2a9"
      },
      "source": [
        "A.min(axis=1,  keepdims = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0],\n",
              "       [ 6],\n",
              "       [12],\n",
              "       [18]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(A - A.min(axis=1,  keepdims = True)) / (A.max(axis=1, keepdims = True) - A.min(axis=1, keepdims = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_vEXIBzYnDM",
        "outputId": "791124a2-5309-4474-9814-f85ad3072088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0. , 0.2, 0.4, 0.6, 0.8, 1. ],\n",
              "       [0. , 0.2, 0.4, 0.6, 0.8, 1. ],\n",
              "       [0. , 0.2, 0.4, 0.6, 0.8, 1. ],\n",
              "       [0. , 0.2, 0.4, 0.6, 0.8, 1. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 2.6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(A):\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "    \n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "    MAX_por_linha = np.transpose(np.full((A.shape[1], A.shape[0]), A.max(axis=1)))    \n",
        "    # print(\"MAX_por_linha\", MAX_por_linha)\n",
        "    # print(\"A - MAX_por_linha\", A - MAX_por_linha)\n",
        "    matriz_sem_valor_alto = np.exp(A - MAX_por_linha)\n",
        "    # print(\"matriz_sem_valor_alto\", matriz_sem_valor_alto)\n",
        "    SOMA_por_linha = np.transpose(np.full((A.shape[1], A.shape[0]), matriz_sem_valor_alto.sum(axis=1)))\n",
        "    # print(\"SOMA_por_linha\", SOMA_por_linha)\n",
        "    return matriz_sem_valor_alto / SOMA_por_linha\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EZ5ZD7HFao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "377a1eb3-0837-4964-f7d6-244a4722b5c4"
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\n",
        "              [-2,   0, 0.5]])\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 1.        ],\n",
              "       [0.04861082, 0.35918811, 0.59220107]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "O código a seguir verifica se sua implementação do softmax está correta. \n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721acbfb-108b-46fa-95f8-265b67a25794"
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ffb6606-ef29-43ce-c740-d7d650ceb64b"
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 424 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE6LaWi81zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd753426-11f1-4b59-9759-6dfba4329f63"
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 2.7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "def one_hot(y:[int], n_classes:int)-> list:\n",
        "  one_hot_encode = np.zeros((len(y), n_classes))\n",
        "  for idx, val in enumerate(y):\n",
        "    one_hot_encode[idx, val] = 1\n",
        "  return one_hot_encode  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6570332a-16b5-4f97-bfc2-ab582ce3196e"
      },
      "source": [
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)\n",
        "print(y)\n",
        "print(one_hot(y, N_CLASSES))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8 8 3 6 6 2 3 2 3 2]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6093c61e-6b63-44de-bd29-5c2411b966ae"
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7dd8fd-b40d-4d21-d8a9-9b172eb7f29f"
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 118 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMroZay2ubi"
      },
      "source": [
        "## Exercício 2.8\n",
        "\n",
        "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
        "```\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)  # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaedJ5Cf5Oy2"
      },
      "source": [
        "class Normalizer:\n",
        "    def __init__(self, base):\n",
        "        b = np.array(base)\n",
        "        self.m = b.mean()\n",
        "        self.s = b.std()\n",
        "    def __call__(self, array):\n",
        "        b = np.array(array)\n",
        "        m = b.mean()\n",
        "        s = b.std()\n",
        "        return (b - m) * self.s / s + self.m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlkNNU6h5RbR"
      },
      "source": [
        "Mostre que seu código está correto com o exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gad6zsbh5a0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee98a4ba-01ca-4b58-b85b-cb4f731510f0"
      },
      "source": [
        "array_a = [-1, 1.5, 0]\n",
        "array_b = [1.4, 0.8, 0.3, 2.5]\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3187798  2.31425165 1.11696854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c920f5a5-b238-46d6-8e14-f0bee68ec964"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.10.0+cu111'"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "**Tensor com atributo .requires_grad=True**\n",
        "\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9260d90a-f892-470a-f4d8-8a72e0959d1a"
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b0aaad-d002-4140-8930-6e34def1ee7a"
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3490f4d3-9f04-4269-bd09-ab5d612733fc"
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d84a516-8cb0-48ba-8a30-acc511308f21"
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y; print('e =',e)\n",
        "e2 = e.pow(2) ; print('e2 =', e2)\n",
        "J = e2.sum()  ; print('J =', J)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29436010-7e08-4c0a-b506-40dc10d9809e"
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eda689c-ca33-42fc-fd76-d15ef4c6c2b8"
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 3.1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def J_func(w, x, y):\n",
        "    return (np.multiply(w, x) - y).pow(2).sum()\n",
        "\n",
        "# Calcule o gradiente usando a regra diferenças finitas\n",
        "# Confira com o valor já calculado anteriormente\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "dw = 0.1\n",
        "print(dw)\n",
        "print(f'J_func(w) {J_func(w,x,y)}; f_func(w+dw) {J_func(w+dw, x, y)}; f_func(w-dw) {J_func(w-dw, x, y)}')\n",
        "grad = (J_func(w+dw, x, y) - J_func(w-dw, x, y)) / (2 * dw)\n",
        "print('grad=', grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMWjq2BMbR4N",
        "outputId": "2a2d10b9-8eb9-4c2d-f290-47dbf3077bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1\n",
            "J_func(w) 14.0; f_func(w+dw) 11.339998245239258; f_func(w-dw) 16.94000244140625\n",
            "grad= tensor(-28.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 3.2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rGPsGkJobbYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "perdas = []\n",
        "for i in range(iteracoes):\n",
        "    print()\n",
        "    J = J_func(w, x, y)\n",
        "    grad = (J_func(w+dw, x, y) - J_func(w-dw, x, y)) / (2 * dw)\n",
        "    w = w - learning_rate * grad\n",
        "    print(f'i = {i} J= {J} grad = {grad} w ={w}')\n",
        "    perdas.append(J)\n",
        "\n",
        "# Plote o gráfico da loss J pela iteração i\n",
        "plt.plot(range(20), perdas) \n",
        "plt.xlabel('I (iterações)')\n",
        "plt.xticks(range(20))\n",
        "plt.ylabel('y (loss)')\n",
        "plt.xlim(0,20)\n",
        "plt.title('Loss por iteração')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vAa2O_cabhFB",
        "outputId": "07c5ecdc-3768-401e-c41a-8eb497c8d3a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "i = 0 J= 14.0 grad = -28.00002098083496 w =tensor([1.2800])\n",
            "\n",
            "i = 1 J= 7.257596015930176 grad = -20.16000747680664 w =tensor([1.4816])\n",
            "\n",
            "i = 2 J= 3.762335777282715 grad = -14.5152006149292 w =tensor([1.6268])\n",
            "\n",
            "i = 3 J= 1.950394630432129 grad = -10.450942039489746 w =tensor([1.7313])\n",
            "\n",
            "i = 4 J= 1.0110836029052734 grad = -7.524674892425537 w =tensor([1.8065])\n",
            "\n",
            "i = 5 J= 0.5241460204124451 grad = -5.41776704788208 w =tensor([1.8607])\n",
            "\n",
            "i = 6 J= 0.2717174291610718 grad = -3.900789976119995 w =tensor([1.8997])\n",
            "\n",
            "i = 7 J= 0.14085820317268372 grad = -2.808570146560669 w =tensor([1.9278])\n",
            "\n",
            "i = 8 J= 0.07302085310220718 grad = -2.0221707820892334 w =tensor([1.9480])\n",
            "\n",
            "i = 9 J= 0.03785398602485657 grad = -1.455962896347046 w =tensor([1.9626])\n",
            "\n",
            "i = 10 J= 0.019623516127467155 grad = -1.0482927560806274 w =tensor([1.9730])\n",
            "\n",
            "i = 11 J= 0.010172825306653976 grad = -0.7547721266746521 w =tensor([1.9806])\n",
            "\n",
            "i = 12 J= 0.005273588001728058 grad = -0.5434329509735107 w =tensor([1.9860])\n",
            "\n",
            "i = 13 J= 0.002733835019171238 grad = -0.39127466082572937 w =tensor([1.9899])\n",
            "\n",
            "i = 14 J= 0.0014172473456710577 grad = -0.281717985868454 w =tensor([1.9928])\n",
            "\n",
            "i = 15 J= 0.0007347092032432556 grad = -0.20283769071102142 w =tensor([1.9948])\n",
            "\n",
            "i = 16 J= 0.00038088360452093184 grad = -0.1460452377796173 w =tensor([1.9962])\n",
            "\n",
            "i = 17 J= 0.00019744501332752407 grad = -0.10515324771404266 w =tensor([1.9973])\n",
            "\n",
            "i = 18 J= 0.00010235930676572025 grad = -0.07570989429950714 w =tensor([1.9981])\n",
            "\n",
            "i = 19 J= 5.3059407946420833e-05 grad = -0.05450993776321411 w =tensor([1.9986])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 3.3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "\n",
        "\n",
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "e = y_pred - y\n",
        "e2 = e.pow(2) \n",
        "J = e2.sum()  \n",
        "\n",
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()\n",
        "\n",
        "if w.grad: w.grad.zero_()\n",
        "perdas = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbmESzp1b8dg",
        "outputId": "a7403b8d-37a1-4a3d-d652-6579c6ce243b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "J.backward(retain_graph=True)"
      ],
      "metadata": {
        "id": "u_l547qPcBJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yhq4Zr1cCsh",
        "outputId": "c67e5b08-e2a0-4920-a3e4-a5f9254eeb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for i in range(iteracoes):\n",
        "  loss = J_func(w.detach().numpy() , x, y)\n",
        "  perdas.append(loss)\n",
        "  y_pred = x * w; \n",
        "  e = y_pred - y\n",
        "  e2 = e.pow(2) \n",
        "  J = e2.sum()  \n",
        "  J.backward()\n",
        "  grad = w.grad\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "  # w = w - learning_rate * grad\n",
        "  print(f'w ={w}')\n",
        "  print(f'i = {i} J= {J} grad = {grad} w ={w}')\n",
        "  w.grad.zero_()\n",
        "\n",
        "\n",
        "# Plote o gráfico da loss J pela iteração i\n",
        "plt.plot(range(20), perdas) \n",
        "plt.xlabel('x - iterações')\n",
        "plt.xticks(range(20))\n",
        "plt.ylabel('y - loss')\n",
        "plt.xlim(0,20)\n",
        "plt.title('Loss por iteração')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rhs4xWlmcTtw",
        "outputId": "3cb095ee-cddb-4e51-e291-79b22db53aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w =tensor([1.5600], requires_grad=True)\n",
            "i = 0 J= 14.0 grad = tensor([-56.]) w =tensor([1.5600], requires_grad=True)\n",
            "w =tensor([1.6832], requires_grad=True)\n",
            "i = 1 J= 2.7104005813598633 grad = tensor([-12.3200]) w =tensor([1.6832], requires_grad=True)\n",
            "w =tensor([1.7719], requires_grad=True)\n",
            "i = 2 J= 1.4050711393356323 grad = tensor([-8.8704]) w =tensor([1.7719], requires_grad=True)\n",
            "w =tensor([1.8358], requires_grad=True)\n",
            "i = 3 J= 0.7283890247344971 grad = tensor([-6.3867]) w =tensor([1.8358], requires_grad=True)\n",
            "w =tensor([1.8818], requires_grad=True)\n",
            "i = 4 J= 0.37759679555892944 grad = tensor([-4.5984]) w =tensor([1.8818], requires_grad=True)\n",
            "w =tensor([1.9149], requires_grad=True)\n",
            "i = 5 J= 0.1957462579011917 grad = tensor([-3.3109]) w =tensor([1.9149], requires_grad=True)\n",
            "w =tensor([1.9387], requires_grad=True)\n",
            "i = 6 J= 0.10147492587566376 grad = tensor([-2.3838]) w =tensor([1.9387], requires_grad=True)\n",
            "w =tensor([1.9559], requires_grad=True)\n",
            "i = 7 J= 0.052604615688323975 grad = tensor([-1.7164]) w =tensor([1.9559], requires_grad=True)\n",
            "w =tensor([1.9682], requires_grad=True)\n",
            "i = 8 J= 0.027270279824733734 grad = tensor([-1.2358]) w =tensor([1.9682], requires_grad=True)\n",
            "w =tensor([1.9771], requires_grad=True)\n",
            "i = 9 J= 0.014136932790279388 grad = tensor([-0.8898]) w =tensor([1.9771], requires_grad=True)\n",
            "w =tensor([1.9835], requires_grad=True)\n",
            "i = 10 J= 0.007328573148697615 grad = tensor([-0.6406]) w =tensor([1.9835], requires_grad=True)\n",
            "w =tensor([1.9881], requires_grad=True)\n",
            "i = 11 J= 0.003799171419814229 grad = tensor([-0.4613]) w =tensor([1.9881], requires_grad=True)\n",
            "w =tensor([1.9915], requires_grad=True)\n",
            "i = 12 J= 0.0019694678485393524 grad = tensor([-0.3321]) w =tensor([1.9915], requires_grad=True)\n",
            "w =tensor([1.9939], requires_grad=True)\n",
            "i = 13 J= 0.0010209650499746203 grad = tensor([-0.2391]) w =tensor([1.9939], requires_grad=True)\n",
            "w =tensor([1.9956], requires_grad=True)\n",
            "i = 14 J= 0.0005292787682265043 grad = tensor([-0.1722]) w =tensor([1.9956], requires_grad=True)\n",
            "w =tensor([1.9968], requires_grad=True)\n",
            "i = 15 J= 0.0002743711811490357 grad = tensor([-0.1240]) w =tensor([1.9968], requires_grad=True)\n",
            "w =tensor([1.9977], requires_grad=True)\n",
            "i = 16 J= 0.00014223942707758397 grad = tensor([-0.0892]) w =tensor([1.9977], requires_grad=True)\n",
            "w =tensor([1.9983], requires_grad=True)\n",
            "i = 17 J= 7.373015250777826e-05 grad = tensor([-0.0643]) w =tensor([1.9983], requires_grad=True)\n",
            "w =tensor([1.9988], requires_grad=True)\n",
            "i = 18 J= 3.822529106400907e-05 grad = tensor([-0.0463]) w =tensor([1.9988], requires_grad=True)\n",
            "w =tensor([1.9991], requires_grad=True)\n",
            "i = 19 J= 1.981569585041143e-05 grad = tensor([-0.0333]) w =tensor([1.9991], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 3.4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resposta:\n",
        "Precisamos fazer o dw tender a zero para que teoricamente tenhamos a própria fórmula da derivada:\n",
        "\n",
        "<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\lim_{dw\\rightarrow&space;0}&space;\\frac{y_{i&plus;1}&space;-&space;y_i}{dw}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\lim_{dw\\rightarrow&space;0}&space;\\frac{y_{i&plus;1}&space;-&space;y_i}{dw}\" title=\"\\lim_{dw\\rightarrow 0} \\frac{y_{i+1} - y_i}{dw}\" /></a>\n",
        "\n",
        "Mas, se o valor de dw for muito pequeno, o erro ao calcular a aproximação numérica cresce (devido aos erros de arredondamento para a representação numérica computacional). Ver só uma ilustração no gráfico abaixo:\n",
        "\n",
        "![grafico.png](data:image/png;base64,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)\n",
        "\n",
        "Gráfico 1 – Diferença entre derivada real e estimada a partir de dw (h) (fonte: figura 8.1 em https://www.ufrgs.br/reamat/CalculoNumerico/livro-oct/dn-diferencas_finitas.html )\n",
        " \n",
        "Isso nos motiva a procurar o valor ótimo de dw que minimiza o erro. \n",
        "Um procedimento simples para se resolver esse problema seria atribuir a dw valores gradativamente menores. A partir de um determinado ponto as soluções obtidas serão muito parecidas, sendo que uma maior redução em dw a partir deste ponto não irá reduzir o erro global de forma signicativa e irá aumentar o erro de arredondamento.\n",
        "\n",
        "Obs.: se for o caso, a depender da função alvo, talvez seja necessária uma avaliação mais aprofundada para o valor de dw e para o cálculo da derivada aproximada em duas situações:\n",
        "\n",
        ". nos pontos de platô do gráfico (derivada = 0): a diferença entre y(x+dw) e y(x-dw) tende a ser zero;\n",
        "\n",
        ". nos pontos de descontinuidades (não é o caso da loss da questão anterior): o cálculo pode possuir comportamento oscilatório nas vizinhanças dessas descontinuidades.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uWs74DiGdKMn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 3.5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a) $O(N^2)$ - quadrática: pois o cálculo de f que é O(N) precisa ser executado N vezes, para cada dimensão em w:\n",
        " f(w1+epsilon,w2,w3,...), f(w1,w2+epsilon,w3,...), etc\n",
        "\n",
        "b) $O(N)$ - linear: pois cada peso w precisa ser \"visitado\" pelo menos uma vez. Fonte: https://towardsdatascience.com/back-propagation-demystified-in-7-minutes-4294d71a04d7 :: Complexity of Back Propagation.\n",
        "\n"
      ],
      "metadata": {
        "id": "-_ljI0-Kdug_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 3.6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resposta:\n",
        "\n",
        "Sendo aleatória a inicialização, há uma tendência de que haja uma distribuição equilibrada entre as classes, o que levaria a uma entropia de uma distribuição \"meio a meio\", dada pela fórmula:\n",
        "\n",
        "$$L = \\log K $$\n",
        "\n",
        "Fonte: https://medium.com/ensina-ai/uma-explica%C3%A7%C3%A3o-visual-para-fun%C3%A7%C3%A3o-de-custo-binary-cross-entropy-ou-log-loss-eaee662c396c :: fórmula para Entropia de uma distribuição meio a meio\n",
        "\n",
        "\n",
        "\n",
        "Considerando que será 1/k em cada classe (só para o começo do algoritmo): \n",
        "\n",
        "- log 1/k = log k\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BwVo7kqad-Xe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "Fim do notebook."
      ]
    }
  ]
}