{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercícios - 20210718 - Pedro Gengo",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex01/Pedro_Gengo/Exerc%C3%ADcios_20210718_Pedro_Gengo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoyGt5gXMgK"
      },
      "source": [
        "## Coloque seu nome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8d2fbe-972f-423d-a4d5-9018001500a3"
      },
      "source": [
        "print('Meu nome é: Pedro Gabriel Gengo Lourenco')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é: Pedro Gabriel Gengo Lourenco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1.1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j"
      },
      "source": [
        "from collections import Counter\n",
        "def top_k(L, k):\n",
        "    c = Counter(L).most_common(k)\n",
        "    return dict(c)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13cef19-3af3-4e1f-ecca-095c61ed515c"
      },
      "source": [
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L, k=k)\n",
        "print(f'resultado: {resultado}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0517ceee-14fe-43d0-8345-232d897f4e33"
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 573 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 1.2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzv89trtTPc"
      },
      "source": [
        "from re import findall\n",
        "def tokens_to_ids(text, vocabulary):\n",
        "    tokens = findall(r'\\w+|\\S', text)\n",
        "    unknown = vocabulary['unknown']\n",
        "    tokenized = [vocabulary.get(token.lower(), unknown) for token in tokens]\n",
        "    return tokenized"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ebabd0-7016-4648-c2d9-aee698314252"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "print(tokens_to_ids(D, V))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 2, 4, -1, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7463e42-ab95-40cb-da28-f79e114f7851"
      },
      "source": [
        "%%timeit\n",
        "resultado = tokens_to_ids(D, V)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 2.77 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 1.3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsadE9SRG_9"
      },
      "source": [
        "def sample(path: str, k: int):\n",
        "  # Escreva o seu código aqui.\n",
        "  from random import randint\n",
        "\n",
        "  samples = []\n",
        "\n",
        "  for index, line in enumerate(open(path, 'r')):\n",
        "    if index < k:\n",
        "      samples.append(line[:-1])\n",
        "    else:\n",
        "      j = randint(0, index)\n",
        "      if j < k:\n",
        "        samples[j] = line[:-1]\n",
        "  return samples"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01a4448-4ba9-40be-8efd-187fbb235893"
      },
      "source": [
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['line 65', 'line 14', 'line 15', 'line 49', 'line 21', 'line 73', 'line 11', 'line 58', 'line 31', 'line 33']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000\n",
        "n_samples = 10000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "603e6c8e-c03a-42bd-e16e-4f408aa0ec9a"
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 1.53 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 2.1\n",
        "\n",
        "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNXj45RJqUm"
      },
      "source": [
        "Resposta:\n",
        "- número de somas: (n-1) * m * p\n",
        "- número de multiplicações: n * m * p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2.2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrXf18N5KrK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082ea21a-b884-498d-87a4-253d5364020f"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c18d77-a170-4f8b-d90f-f5408d5d80d5"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "print(A.mean(axis=1))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.5  8.5 14.5 20.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 2.3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf3a891-b4b6-422c-b8e5-d6afddce0679"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "matrix_min = A.min()\n",
        "matrix_max = A.max()\n",
        "C = (A - matrix_min) / (matrix_max - matrix_min)\n",
        "print(C)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.04347826 0.08695652 0.13043478 0.17391304 0.2173913 ]\n",
            " [0.26086957 0.30434783 0.34782609 0.39130435 0.43478261 0.47826087]\n",
            " [0.52173913 0.56521739 0.60869565 0.65217391 0.69565217 0.73913043]\n",
            " [0.7826087  0.82608696 0.86956522 0.91304348 0.95652174 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 2.4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08475100-3834-4fee-974e-8346b1328ea2"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "column_min = A.min(axis=0)\n",
        "column_max = A.max(axis=0)\n",
        "C_column = (A - column_min) / (column_max - column_min)\n",
        "print(C_column)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.        ]\n",
            " [0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
            " [0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667]\n",
            " [1.         1.         1.         1.         1.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 2.5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8269aaac-cbb1-4f75-b191-46dc2e10774f"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "row_min = A.min(axis=1).reshape(-1, 1)\n",
        "row_max = A.max(axis=1).reshape(-1, 1)\n",
        "C_row = (A - row_min) / (row_max - row_min)\n",
        "print(C_row)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 2.6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(A):\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "    \n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "    # Escreva sua solução aqui.\n",
        "    max_array = A.max(axis=1).reshape(-1,1)\n",
        "    exp_A = np.exp(A-max_array)\n",
        "    return exp_A / exp_A.sum(axis=1).reshape(-1, 1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K76i3PcVJnk3",
        "outputId": "d7b716f9-c20e-42bf-acef-ddd17d290bc2"
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\n",
        "              [-2,   0, 0.5]])\n",
        "softmax(A)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 1.        ],\n",
              "       [0.04861082, 0.35918811, 0.59220107]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "O código a seguir verifica se sua implementação do softmax está correta. \n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcfb955d-eaa5-48f6-f974-a79e14b31335"
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c85ac10-cd58-47f2-a4bb-83b79dbee8f8"
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 316 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE6LaWi81zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1821d130-2287-4503-a181-c42ee5120d3a"
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 2.7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "import numpy as np\n",
        "def one_hot(y, n_classes):\n",
        "    onehot = np.zeros((len(y), n_classes))\n",
        "    onehot[np.arange(len(y)).reshape(-1, 1), y.reshape(-1, 1)] = 1\n",
        "    return onehot"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d603181-161e-46cb-b1b0-08c4091bf8eb"
      },
      "source": [
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)\n",
        "print(y)\n",
        "print(one_hot(y, N_CLASSES))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 0 1 0 6 7 3 6 6 4]\n",
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA",
        "outputId": "4fbf7c2b-ce23-4fdc-f0b4-8ba2f73d7481",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8aa7c1-ee14-4977-cd31-a8bec894fe1a"
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 182 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMroZay2ubi"
      },
      "source": [
        "## Exercício 2.8\n",
        "\n",
        "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
        "```\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)  # Deve imprimir # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaedJ5Cf5Oy2"
      },
      "source": [
        "# Escreva seu código aqui.\n",
        "class Normalizer():\n",
        "\n",
        "  def __init__(self, target_array):\n",
        "    self.target_mean = target_array.mean()\n",
        "    self.target_std = target_array.std()\n",
        "  \n",
        "  def __call__(self, array):\n",
        "    normal = (array - array.mean()) / array.std()\n",
        "    return (normal * self.target_std) + self.target_mean"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlkNNU6h5RbR"
      },
      "source": [
        "Mostre que seu código está correto com o exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gad6zsbh5a0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59fa86ec-3a2e-48bc-a0e7-ce85a61a9eb1"
      },
      "source": [
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3187798  2.31425165 1.11696854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bfe25742-cf21-4c12-c951-0f5f672488a0"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "**Tensor com atributo .requires_grad=True**\n",
        "\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a43454fa-5761-4588-e2fc-a997c7e62386"
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686ddf3b-4513-43b9-d164-5f91f625505f"
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26c06db-d1f7-45dd-8d8b-5638f941d98c"
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee35d0f3-971e-45d0-9b5d-751be83d89a4"
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y; print('e =',e)\n",
        "e2 = e.pow(2) ; print('e2 =', e2)\n",
        "J = e2.sum()  ; print('J =', J)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8500ba-8e1d-4870-eb60-2c18fe0ea530"
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6752bb0e-f18d-4f95-f3a9-510dbbf64757"
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 2.1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "62nZAfUoCZu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619886c7-ede0-4f9d-8395-138dabce44d2"
      },
      "source": [
        "def J_func(w, x, y):\n",
        "    # programe a função J_func, para facilitar\n",
        "    return ((y - w * x) ** 2).sum()\n",
        "\n",
        "def J_diff(w, x, y):\n",
        "    delta_w = 1e-3\n",
        "    J_sum = J_func(w + delta_w, x, y)\n",
        "    J_sub = J_func(w - delta_w, x, y)\n",
        "    return (J_sum - J_sub) / (2 * delta_w)\n",
        "\n",
        "# Calcule o gradiente usando a regra diferenças finitas\n",
        "# Confira com o valor já calculado anteriormente\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "grad = J_diff(w, x, y)\n",
        "print('grad=', grad)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad= tensor(-28.0008)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 2.2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNszCOED1Wtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58200612-5e1f-42e8-a266-8d8089572124"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "loss = []\n",
        "\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    loss.append(J.item())\n",
        "    print('J=', J)\n",
        "    grad = J_diff(w, x, y)\n",
        "    print('grad =',grad)\n",
        "    w -=  learning_rate * grad\n",
        "    print('w =', w)\n",
        "\n",
        "# Plote o gráfico da loss J pela iteração i\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss)\n",
        "plt.title('Loss over iterations')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss');"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14.)\n",
            "grad = tensor(-28.0008)\n",
            "w = tensor([1.2800])\n",
            "i = 1\n",
            "J= tensor(7.2574)\n",
            "grad = tensor(-20.1611)\n",
            "w = tensor([1.4816])\n",
            "i = 2\n",
            "J= tensor(3.7621)\n",
            "grad = tensor(-14.5156)\n",
            "w = tensor([1.6268])\n",
            "i = 3\n",
            "J= tensor(1.9501)\n",
            "grad = tensor(-10.4510)\n",
            "w = tensor([1.7313])\n",
            "i = 4\n",
            "J= tensor(1.0109)\n",
            "grad = tensor(-7.5245)\n",
            "w = tensor([1.8065])\n",
            "i = 5\n",
            "J= tensor(0.5240)\n",
            "grad = tensor(-5.4175)\n",
            "w = tensor([1.8607])\n",
            "i = 6\n",
            "J= tensor(0.2716)\n",
            "grad = tensor(-3.9003)\n",
            "w = tensor([1.8997])\n",
            "i = 7\n",
            "J= tensor(0.1408)\n",
            "grad = tensor(-2.8083)\n",
            "w = tensor([1.9278])\n",
            "i = 8\n",
            "J= tensor(0.0730)\n",
            "grad = tensor(-2.0218)\n",
            "w = tensor([1.9480])\n",
            "i = 9\n",
            "J= tensor(0.0378)\n",
            "grad = tensor(-1.4557)\n",
            "w = tensor([1.9626])\n",
            "i = 10\n",
            "J= tensor(0.0196)\n",
            "grad = tensor(-1.0482)\n",
            "w = tensor([1.9731])\n",
            "i = 11\n",
            "J= tensor(0.0102)\n",
            "grad = tensor(-0.7546)\n",
            "w = tensor([1.9806])\n",
            "i = 12\n",
            "J= tensor(0.0053)\n",
            "grad = tensor(-0.5433)\n",
            "w = tensor([1.9860])\n",
            "i = 13\n",
            "J= tensor(0.0027)\n",
            "grad = tensor(-0.3912)\n",
            "w = tensor([1.9899])\n",
            "i = 14\n",
            "J= tensor(0.0014)\n",
            "grad = tensor(-0.2816)\n",
            "w = tensor([1.9928])\n",
            "i = 15\n",
            "J= tensor(0.0007)\n",
            "grad = tensor(-0.2028)\n",
            "w = tensor([1.9948])\n",
            "i = 16\n",
            "J= tensor(0.0004)\n",
            "grad = tensor(-0.1460)\n",
            "w = tensor([1.9962])\n",
            "i = 17\n",
            "J= tensor(0.0002)\n",
            "grad = tensor(-0.1051)\n",
            "w = tensor([1.9973])\n",
            "i = 18\n",
            "J= tensor(0.0001)\n",
            "grad = tensor(-0.0757)\n",
            "w = tensor([1.9981])\n",
            "i = 19\n",
            "J= tensor(5.3023e-05)\n",
            "grad = tensor(-0.0545)\n",
            "w = tensor([1.9986])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 2.3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP4d5vtHtqy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee438878-47e1-4bcb-9deb-e485f72b15c1"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "\n",
        "loss = []\n",
        "\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    loss.append(J.item())\n",
        "    print('J=', J)\n",
        "    if w.grad: w.grad.zero_()\n",
        "    J.backward()\n",
        "    grad = w.grad\n",
        "    print('grad =',grad)\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * grad\n",
        "    print('w =', w)\n",
        "\n",
        "# Plote aqui a loss pela iteração\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss)\n",
        "plt.title('Loss over iterations')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss');"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14., grad_fn=<SumBackward0>)\n",
            "grad = tensor([-28.])\n",
            "w = tensor([1.2800], requires_grad=True)\n",
            "i = 1\n",
            "J= tensor(7.2576, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-20.1600])\n",
            "w = tensor([1.4816], requires_grad=True)\n",
            "i = 2\n",
            "J= tensor(3.7623, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-14.5152])\n",
            "w = tensor([1.6268], requires_grad=True)\n",
            "i = 3\n",
            "J= tensor(1.9504, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-10.4509])\n",
            "w = tensor([1.7313], requires_grad=True)\n",
            "i = 4\n",
            "J= tensor(1.0111, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-7.5247])\n",
            "w = tensor([1.8065], requires_grad=True)\n",
            "i = 5\n",
            "J= tensor(0.5241, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-5.4178])\n",
            "w = tensor([1.8607], requires_grad=True)\n",
            "i = 6\n",
            "J= tensor(0.2717, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-3.9008])\n",
            "w = tensor([1.8997], requires_grad=True)\n",
            "i = 7\n",
            "J= tensor(0.1409, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.8086])\n",
            "w = tensor([1.9278], requires_grad=True)\n",
            "i = 8\n",
            "J= tensor(0.0730, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.0222])\n",
            "w = tensor([1.9480], requires_grad=True)\n",
            "i = 9\n",
            "J= tensor(0.0379, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.4560])\n",
            "w = tensor([1.9626], requires_grad=True)\n",
            "i = 10\n",
            "J= tensor(0.0196, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.0483])\n",
            "w = tensor([1.9730], requires_grad=True)\n",
            "i = 11\n",
            "J= tensor(0.0102, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.7548])\n",
            "w = tensor([1.9806], requires_grad=True)\n",
            "i = 12\n",
            "J= tensor(0.0053, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.5434])\n",
            "w = tensor([1.9860], requires_grad=True)\n",
            "i = 13\n",
            "J= tensor(0.0027, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.3913])\n",
            "w = tensor([1.9899], requires_grad=True)\n",
            "i = 14\n",
            "J= tensor(0.0014, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2817])\n",
            "w = tensor([1.9928], requires_grad=True)\n",
            "i = 15\n",
            "J= tensor(0.0007, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2028])\n",
            "w = tensor([1.9948], requires_grad=True)\n",
            "i = 16\n",
            "J= tensor(0.0004, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1460])\n",
            "w = tensor([1.9962], requires_grad=True)\n",
            "i = 17\n",
            "J= tensor(0.0002, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1052])\n",
            "w = tensor([1.9973], requires_grad=True)\n",
            "i = 18\n",
            "J= tensor(0.0001, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0757])\n",
            "w = tensor([1.9981], requires_grad=True)\n",
            "i = 19\n",
            "J= tensor(5.3059e-05, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0545])\n",
            "w = tensor([1.9986], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxddX3/8df7zkwmy0w2MpksJCRAJgEDAoZFEItCMUEE7KM/xa1Q/UntT6u2thbca9Va7Ubr1lQQtIi2IMrPsroAIhgMkSUhkARIyJ4hIclkmWSWT/84Z8JlmEmGzNx7Zu55Px+P+5hzz/ne8/3ck5vP+d7v+d7vUURgZmb5Ucg6ADMzKy8nfjOznHHiNzPLGSd+M7OcceI3M8sZJ34zs5xx4jcbIJJ2STo6w/rPlvRUVvXb0OHEbyUhabWk87KOo5wioi4ingGQdJ2kL5SyPkkh6dii+n8VEbNLWadVBid+s1dIUnUl1GH55cRvZSWpVtK/SNqQPv5FUm26bYKkn0raLmmbpF9JKqTb/lrSekktkp6SdG4v+x8j6buSmiWtkfQpSYW03u2S5haVbZC0V9LE9PmFkh5Jyz0g6cSisqvTGB4DdveUmLta4JKuAN4FfDzt/vn/6fYpkm5OY3tW0oeLXvs5STdJ+k9JO4HLJZ0m6cE0no2SviZpWFr+vvSlj6Z1vF3SOZLWFe3zOEn3pK9fJumiom3XSfq6pP9Jj+kiScek2yTpnyVtkbRT0uPFx80qQET44ceAP4DVwHk9rP888BtgItAAPAD8bbrt74BvATXp42xAwGxgLTAlLTcDOKaXer8L/ASoT8utAN6XbrsW+GJR2Q8Cd6TLJwNbgNOBKuCy9D3UFr2fR4BpwIhe6g7g2HT5OuALRdsKwMPAZ4BhwNHAM8Cb0u2fA9qAS9KyI4DXAGcA1el7WQ58tKf60ufnAOvS5RpgFfCJtL43Ai3A7KL4tgKnpfu/AfhBuu1Naaxj0+N/HDA568+UHwP3cIvfyu1dwOcjYktENAN/A7wn3dYGTAaOioi2SPqsA+gAaoHjJdVExOqIeLr7jiVVAZcCV0VES0SsBv6xaP/fT7d3eWe6DuAK4N8jYlFEdETE9cA+ksTb5V8jYm1E7D2M930q0BARn4+I/ZFcC/iPbvE8GBE/jojOiNgbEQ9HxG8ioj19L/8O/F4f6zsDqAO+nNb3C+CnwDuKytwSEQ9FRDtJ4j8pXd9GcuKcAygilkfExsN4zzZIOfFbuU0B1hQ9X5OuA/gqSSv1LknPSLoSICJWAR8laRVvkfQDSVN4uQkkLd3u+5+aLv8SGCnpdEkzSBLdLem2o4CPpd0i2yVtJ2ndF9ez9pW/3QOOAqZ02/8ngMbe9i+pKe362pR2/3wpfY99MQVYGxGdReuKjwXApqLlPSQnCtKTxNeAr5Mc74WSRvexXhsCnPit3DaQJMEu09N1pK30j0XE0cBFwF909eVHxPcj4nXpawP4+x72/TxJa7X7/ten++gA/ouk1fsO4KcR0ZKWW0vSDTS26DEyIm4s2tcrmcq2e9m1wLPd9l8fERcc5DXfBJ4EZkXEaJIThfpY/wZgWtc1ktSBY3HI4CP+NSJeAxwPNAF/1cd6bQhw4rdSqpE0vOhRDdwIfCq9sDqBpM/7P+HAxdVjJQnYQdLF0ylptqQ3pheBW4G9QGf3yooS+xcl1Us6CviLrv2nvg+8naTL6ftF6/8D+ED6bUCSRkl6s6T6w3zvm0n68bs8BLSkF4hHSKqSNFfSqQfZRz2wE9glaQ7wp4eoo9giklb8xyXVSDoHeAvwg0MFLunU9DjUALtJjvnLjrcNXU78Vkq3kSTprsfngC8Ai4HHgMeBJek6gFnAz4BdwIPANyLilyT9+18madFvIrkwfFUvdf4ZSbJ6BrifJLlf27UxIhal26cAtxetXwy8n6SL4wWSLqfLD/udwzUk1yS2S/pxelK6kKR76dn0vXwbGHOQffwlyXWIFpIT0w+7bf8ccH1ax9uKN0TEfpJEvyCt6xvAH0XEk32IfXRa3wsk3UNbSbrhrEIouXZmZmZ54Ra/mVnOOPGbmeWME7+ZWc448ZuZ5cyQmAhqwoQJMWPGjKzDMDMbUh5++OHnI6Kh+/ohkfhnzJjB4sWLsw7DzGxIkbSmp/Xu6jEzyxknfjOznHHiNzPLGSd+M7OcceI3M8uZkiV+Sdemt25b2sO2j6W3qevr3OJmZjZAStnivw6Y332lpGnA+cBzJazbzMx6UbLEHxH3Adt62PTPwMd5ZTe1OCy/eHIz37hnVamrMTMbUsraxy/pYmB9RDzah7JXSFosaXFzc/Nh1ffAqq1c/bOVdHR66mkzsy5lS/ySRpLcOu4zfSkfEQsjYl5EzGtoeNkvjvukqbGefe2drN2257Beb2ZWicrZ4j8GmAk8Kmk1cCSwRNKkUlU4q7EOgBWbWw5R0swsP8qW+CPi8YiYGBEzImIGsA44JSI2larOWY3J7VJXbtlVqirMzIacUg7nvJHkvqmzJa2T9L5S1dWbutpqpo4dwVOb3OI3M+tSstk5I+Idh9g+o1R1F5vVWOeuHjOzIhX/y92mxnqead5Ne0dn1qGYmQ0KFZ/4Z02sY39HJ2s8ssfMDMhB4m/qusDr7h4zMyAHif/YiV1DOj2yx8wMcpD4R9VWc+S4Eb7Aa2aWqvjED0l3z0q3+M3MgBwl/mee30WbR/aYmeUl8dfR1hGs2bo761DMzDKXk8SfjOzxBV4zs5wk/mMa6pA8WZuZGeQk8Y8YVsX08SN9gdfMjJwkfoBZE+vd4jczI0eJv6mxjmef383+do/sMbN8y1Hir6e9M3j2eY/sMbN8y03i9924zMwSuUn8xzTUUZAnazMzy03iH15TxVFHjPJYfjPLvdwkfkjm5l+xxS1+M8u3XCX+psZ61mzdw772jqxDMTPLTClvtn6tpC2Slhat+6qkJyU9JukWSWNLVX9PmibV09EZPNPskT1mll+lbPFfB8zvtu5uYG5EnAisAK4qYf0v0+SRPWZmpUv8EXEfsK3bursioj19+hvgyFLV35OZE0ZRVZCnbjCzXMuyj/+9wO29bZR0haTFkhY3NzcPSIW11VXMOGKkW/xmlmuZJH5JnwTagRt6KxMRCyNiXkTMa2hoGLC6mxrrWbnFLX4zy6+yJ35JlwMXAu+KiCh3/bMa61mzdTetbR7ZY2b5VNbEL2k+8HHgoojYU866uzQ11tEZ8HSzW/1mlk+lHM55I/AgMFvSOknvA74G1AN3S3pE0rdKVX9vXrwbl/v5zSyfqku144h4Rw+rrylVfX0144hRVBfkqRvMLLdy9ctdgGHVBWZOGOXJ2swst3KX+CHp7nGL38zyKpeJf1ZjHWtf2MPe/R7ZY2b5k8vEP7uxnghY5fH8ZpZDuUz8szyyx8xyLJeJf8YRIxlWVfDc/GaWS7lM/NVVBY5uGOXJ2swsl3KZ+CHp7nFXj5nlUW4Tf9PEOta9sJfd+9oPXdjMrILkNvF3XeD1yB4zy5vcJn7fjcvM8iq3if+oI0YxrLrgufnNLHdym/irCuKYhjqe2uQWv5nlS24TPyTdPZ6szczyJueJv54NO1ppaW3LOhQzs7LJdeKfNTG5wOt+fjPLk1wn/tmTkiGd7u4xszzJdeKfNm4kw2sKnpvfzHIl14m/UBDHTqzzWH4zy5VcJ36Apon1nqzNzHKlZIlf0rWStkhaWrRuvKS7Ja1M/44rVf19Nauxnk07W9mx1yN7zCwfStnivw6Y323dlcDPI2IW8PP0eaa6pm5Y5bn5zSwnSpb4I+I+YFu31RcD16fL1wOXlKr+vmo6cDcud/eYWT6Uu4+/MSI2psubgMbeCkq6QtJiSYubm5tLFtDUsSMYUVPlC7xmlhuZXdyNiADiINsXRsS8iJjX0NBQsjgKBTGrsc4XeM0sN8qd+DdLmgyQ/t1S5vp7NGui78ZlZvlR7sR/K3BZunwZ8JMy19+jpsY6trTsY/ue/VmHYmZWcqUcznkj8CAwW9I6Se8Dvgz8vqSVwHnp88z5Aq+Z5Ul1qXYcEe/oZdO5parzcDVN6kr8LZw2c3zG0ZiZlVbuf7kLMGXMcOpqqz1Zm5nlghM/IHXN2eOuHjOrfE78qabGOlb617tmlgNO/Kmmxnqe37Wfbbs9ssfMKpsTf2pW44sXeM3MKpkTf6prsjZf4DWzSufEn5o0ejj1tdW+wGtmFc+JPyUlc/a4q8fMKp0Tf5GmxnpWbnGL38wqmxN/kVmN9WzbvZ/nd+3LOhQzs5Jx4i/SdYHX3T1mVsmc+IscmKxtkxO/mVUuJ/4iE+trGTOihhXu5zezCubEX0RSMnWDu3rMrII58Xczq7GeFZt3kdwZ0sys8jjxd9M0sY4de9tobvHIHjOrTE783fhuXGZW6Zz4u/FkbWZW6Zz4u5lQN4xxI2s8N7+ZVaxMEr+kP5e0TNJSSTdKGp5FHD1J5uypd1ePmVWssid+SVOBDwPzImIuUAVcWu44DqYpnazNI3vMrBJl1dVTDYyQVA2MBDZkFEePmhrraWltZ/NOj+wxs8pT9sQfEeuBfwCeAzYCOyLiru7lJF0habGkxc3NzWWNcdZEX+A1s8qVRVfPOOBiYCYwBRgl6d3dy0XEwoiYFxHzGhoayhqjJ2szs0qWRVfPecCzEdEcEW3Aj4AzM4ijV0fU1TKhbhgrfYHXzCpQFon/OeAMSSMlCTgXWJ5BHAc1a2I9T7nFb2YVKIs+/kXATcAS4PE0hoXljuNQmhrrWLXFc/aYWeWpzqLSiPgs8Nks6u6rWY317NrXzoYdrUwdOyLrcMzMBox/uduLJk/dYGYVyom/F10jezw3v5lVmj4lfkkfkTRaiWskLZF0fqmDy9LYkcNoqK/11A1mVnH62uJ/b0TsBM4HxgHvAb5csqgGCd+Ny8wqUV8Tv9K/FwDfi4hlResq1pxJo3lyUwt793dkHYqZ2YDpa+J/WNJdJIn/Tkn1QGfpwhoc3jhnIvvaO7l3xZasQzEzGzB9TfzvA64ETo2IPUAN8Mcli2qQOH3meMaNrOH2pZuyDsXMbMD0NfG/FngqIran8+p8CthRurAGh+qqAr9/fCO/WL6Ffe3u7jGzytDXxP9NYI+kVwMfA54GvluyqAaRBXMn07KvnV+vej7rUMzMBkRfE397JHMXXAx8LSK+DtSXLqzB48xjj6C+tprbH3d3j5lVhr5O2dAi6SqSYZxnSyqQ9PNXvNrqKs49biJ3L99MW0cnNVX+zZuZDW19zWJvB/aRjOffBBwJfLVkUQ0y8+dOZvueNhY9sy3rUMzM+q1PiT9N9jcAYyRdCLRGRC76+AF+r6mBETVV3L50Y9ahmJn1W1+nbHgb8BDwf4C3AYsk/WEpAxtMRgyr4g1zGrhz2WY6Oj1Ns5kNbX3t6vkkyRj+yyLij4DTgE+XLqzBZ/7cyTy/ax8Pr3kh61DMzPqlr4m/EBHFP1/d+gpeWxHeOGciw6oL7u4xsyGvr8n7Dkl3Srpc0uXA/wC3lS6swaeutprXz2rgjqWb6HR3j5kNYX29uPtXJLdHPDF9LIyIvy5lYIPRgrmT2LijlUfXbc86FDOzw9bnWy9GxM3AzSWMZdA777hGqgvijqWbOHn6uKzDMTM7LAdt8UtqkbSzh0eLpJ2HW6mksZJukvSkpOWSXnu4+yqnMSNrOPPYCdy+dJNvwm5mQ9ZBE39E1EfE6B4e9RExuh/1Xg3cERFzgFcDy/uxr7JaMHcSz23bwxMbD/u8Z2aWqbKPzJE0Bng9cA1AROyPiCHTaX7+8Y0UBHd4qmYzG6KyGJI5E2gGviPpd5K+LWlU90KSrpC0WNLi5ubm8kfZiyPqajlt5njP0W9mQ1YWib8aOAX4ZkScDOwmucnLS0TEwoiYFxHzGhoayh3jQS2YO5lVW3axaovvx2tmQ08WiX8dsC4iFqXPbyI5EQwZb3rVJABP1WxmQ1LZE3864dtaSbPTVecCT5Q7jv6YNGY4p0wf6+4eMxuSspp24c+AGyQ9BpwEfCmjOA7bBSdM5omNO1mzdXfWoZiZvSKZJP6IeCTtvz8xIi6JiCE389mB7h63+s1siMnVRGsDadr4kZwwdYwTv5kNOU78/TB/7iQeXbudDdv3Zh2KmVmfOfH3w4K5SXePf8xlZkOJE38/HN1Qx+zGeid+MxtSnPj7af7cSfx2zTa2tLRmHYqZWZ848ffTghMmEQF3LducdShmZn3ixN9PsxvrmTlhlLt7zGzIcOLvJ0nMnzuJB5/Zygu792cdjpnZITnxD4AL5k6mozO4e7m7e8xs8HPiHwBzp47myHEj3N1jZkOCE/8AkMT8V03iVyub2dnalnU4ZmYH5cQ/QBacMIm2juAXy7dkHYqZ2UE58Q+Qk6eNo3F0Lbcv3Zh1KGZmB+XEP0AKBfGmV03i3hXN7NnfnnU4Zma9cuIfQPPnTqK1rZN7nho89wg2M+vOiX8AnTZjPONHDfNUzWY2qDnxD6DqqgLnH9/IL5ZvprWtI+twzMx65MQ/wObPncTu/R3cv/L5rEMxM+uRE/8AO/OYCYweXu3uHjMbtDJL/JKqJP1O0k+ziqEUhlUXOO/4Rn62fDNtHZ1Zh2Nm9jJZtvg/AizPsP6SWTB3Mjv2tvHg01uzDsXM7GUySfySjgTeDHw7i/pL7exZExg1rMo/5jKzQSmrFv+/AB8Heu0LkXSFpMWSFjc3D61x8cNrqnjDnInctWwzHZ2RdThmZi9R9sQv6UJgS0Q8fLByEbEwIuZFxLyGhoYyRTdwFsydzNbd+3no2W1Zh2Jm9hJZtPjPAi6StBr4AfBGSf+ZQRwldc7sBmqrC9zh7h4zG2TKnvgj4qqIODIiZgCXAr+IiHeXO45SG1Vbze81NXDb0k3s3e8fc5nZ4OFx/CX0vtfNpLllH1f/fGXWoZiZHZBp4o+IeyLiwixjKKXTjz6Ct807km//6hme3LQz63DMzAC3+EvuqgXHMXpEDVf96HE6PcLHzAYBJ/4SGzdqGJ++8Dh+99x2bnjouazDMTNz4i+HS06aylnHHsFXbn+SLTtbsw7HzHLOib8MJPGFS05gX0cnf/PTJ7IOx8xyzom/TGZOGMWH33gs//PYRn75pG/IbmbZceIvoytefwzHTqzjUz9e6vvymllmnPjLaFh1gS+99QTWb9/L1T/z2H4zy4YTf5mdNnM8l546jW/f/yxPbPDYfjMrPyf+DFy5YA7jRtZw1S2Pe/ZOMys7J/4MjB05jE9feDyPrt3ODYvWZB2OmeWME39GLnr1FM6eNYGv3PEUmz2238zKyIk/I8nY/rm0dXTyuVuXZR2OmeWIE3+GjjpiFB8+dxa3L93Ez57YnHU4ZpYTTvwZe//ZR9PUWMdnb13G7n0e229mpefEn7Hisf3/fPeKrMMxsxxw4h8E5s0YzztPn861v36Wpet3ZB2OmVU4J/5B4q/fNIfxo2r5hMf2m1mJOfEPEmNG1vCZtxzPY+t28L0HV2cdjplVMCf+QeQtJ07m9U0NfPXOp9i4Y2/W4ZhZhXLiH0Qk8cVL5tIR4bH9ZlYyZU/8kqZJ+qWkJyQtk/SRcscwmE0bP5KPnNvEncs2c9eyTVmHY2YVKIsWfzvwsYg4HjgD+KCk4zOIY9D6v2fPZM6kej576zJ2eWy/mQ2wsif+iNgYEUvS5RZgOTC13HEMZjVVBb741hPYtLOVf7jzqazDMbMKk2kfv6QZwMnAoh62XSFpsaTFzc3N5Q4tc685ahzvOeMorntgNX9323IP8TSzAVOdVcWS6oCbgY9GxMvuSBIRC4GFAPPmzctl1vv0hccTAf9+3zOs3LKLqy89ifrhNVmHZWZDXCYtfkk1JEn/hoj4URYxDAU1VQX+9pK5/O3Fr+LeFc38wTce4Lmte7IOy8yGuCxG9Qi4BlgeEf9U7vqHove8dgbfe+9pbGnZx0Vfv58Hn96adUhmNoRl0eI/C3gP8EZJj6SPCzKIY0g589gJ/OSDZ3HEqGG855pFfH/Rc1mHZGZDVNn7+CPifkDlrrcSzJgwils+eBYfvvF3fOKWx1mxuYVPvfk4qqv8Ozwz6ztnjCFm9PAarrnsVN5/9kyue2A1l3/nt+zY05Z1WGY2hDjxD0FVBfHJNx/PV/7wRBY9u5VLvvFrnm7elXVYZjZEOPEPYW+bN43vv/8Mdu5t45Kv/5p7V+Tv9w5m9so58Q9xp84Yz08+dBZTx47gj7/zENfe/ywRufzZg5n1kRN/BThy3Ehu/tMzOe+4Rj7/0ye46kePs7+9M+uwzGyQcuKvEKNqq/nWu1/Dh95wLD/47Vrefc0itu3en3VYZjYIOfFXkEJB/OWbZnP1pSfx6NrtXPS1+3ly08tmwzCznHPir0AXnzSV//qT17K/vZO3/Nv9/PkPH+HRtduzDsvMBgkNhQuB8+bNi8WLF2cdxpCzZWcr37jnaW56eB279rVz8vSxXH7mDBbMncywap/zzSqdpIcjYt7L1jvxV76W1jZufngd1z+4hmef383E+lredfpRvPP06TTU12YdnpmViBO/0dkZ3Luymet+vZp7VzQzrKrAhSdO5vKzZnDikWOzDs/MBlhviT+z+fit/AoF8YbZE3nD7Ik83byL7z24hv9evJYf/W49p0wfy+VnzWTB3EnUeO4fs4rmFn/OtbS2cdPD67j+gdWs3rqHxtFJN9A7TnM3kNlQ564eO6jOzuDeFc1854HV3NfVDfTqybzr9OmceORYfwswG4Lc1WMHVSiIN8yZyBvmTGTVll1898HV3PzwOn60ZD3DawqcOHUsJx81llOmj+OU6eP8bcBsCHOL33q1s7WNe59qZslzL7Dkue08sWEHbR3J52Xa+BEHTgKnTB/HnMn1/lZgNsi4q8f6rbWtg2UbdrBkzfb0ZPACm3fuA0i+FRzZ9Y1gLKccNY4Jdf5WYJYlJ34bcBHBhh2tLFnzQo/fCqaPH8kJR47hyHEjmDJmBFPGjmDymOFMHTuCsSNrSG6/bGal4j5+G3CSmDp2BFPHjuAtr54CJN8Klq7fkZwI1mxn6fod3L1sM/s7Xjpb6PCaAlPGdp0QhjN5TLKfyWOHH1g/YlhVFm/LrOJlkvglzQeuBqqAb0fEl7OIwwbe8Joq5s0Yz7wZ4w+s6+wMtu7ez8Yde9mwfS/rt7eycfteNuzYy4btrdzzVDPNu/bR/cvnuJE1NI4ezugRNYweXk398Brqh1czOv1bP7yG0SNeur6r3PCagr9RmPWi7IlfUhXwdeD3gXXAbyXdGhFPlDsWK49CQTTU19JQX9vrL4T3t3eyeWcrG4pOCBu272Xzzn20tLaxfnsrLa0ttLS209LaRucheihrqkT98BrqaqsZXlNgWHWB2uoqaqsL6aOK2pqi5epC+ryoTE0Vw6oKVFeJqoKoLojqQoGqqmS5qut5oeh5VbKu63nXQ4KClD6Sb0tVhWS5oJ63m5VKFi3+04BVEfEMgKQfABcDTvw5Nqy6wLTxI5k2fuQhy0YEu/d30NLaRktrOzv3pn9b29iZnhi61u/a187+9k72tXeyr72DfW2dtLS2J8vtnexr62R/Ryf72pLn7Yc6o5RRVUEIkEAkJ4eXLJOcIARQdAIpXq+ujQf2w4HlZIu6PX/5Saf46UuWOUi5l6w/+EnskKe4fp4D+3sKzfok/KW3nsBpM8cfuuArkEXinwqsLXq+Dji9eyFJVwBXAEyfPr08kdmQIIm62mrqaquZPGZg993e0XUiePFk0dEZtHcG7R2RLnceWPfi307aOl76vKt8Z0BnBBEvLnd0BpEu97S9s/h1QAQEyWsi0r/d1kPXforKpu8r2R5Fy0V/i9a/tPyL23jx5d0X0/LR47ZDjR051Gm2v4NP+n0aHwTtgFG1A3+ta9Be3I2IhcBCSEb1ZByO5UR1VYHqqgIjh2UdiVnpZPGLm/XAtKLnR6brzMysDLJI/L8FZkmaKWkYcClwawZxmJnlUtm7eiKiXdKHgDtJhnNeGxHLyh2HmVleZdLHHxG3AbdlUbeZWd55Vi0zs5xx4jczyxknfjOznHHiNzPLmSExLbOkZmDNYb58AvD8AIYz0Bxf/zi+/nF8/TeYYzwqIhq6rxwSib8/JC3uaT7qwcLx9Y/j6x/H139DIcbu3NVjZpYzTvxmZjmTh8S/MOsADsHx9Y/j6x/H139DIcaXqPg+fjMze6k8tPjNzKyIE7+ZWc5UTOKXNF/SU5JWSbqyh+21kn6Ybl8kaUYZY5sm6ZeSnpC0TNJHeihzjqQdkh5JH58pV3xp/aslPZ7WvbiH7ZL0r+nxe0zSKWWMbXbRcXlE0k5JH+1WpqzHT9K1krZIWlq0brykuyWtTP+O6+W1l6VlVkq6rIzxfVXSk+m/3y2SerwB8qE+CyWM73OS1hf9G17Qy2sP+n+9hPH9sCi21ZIe6eW1JT9+/RbpLd+G8oNkeuengaOBYcCjwPHdyvw/4Fvp8qXAD8sY32TglHS5HljRQ3znAD/N8BiuBiYcZPsFwO0ktzA9A1iU4b/1JpIfpmR2/IDXA6cAS4vWfQW4Ml2+Evj7Hl43Hngm/TsuXR5XpvjOB6rT5b/vKb6+fBZKGN/ngL/sw7//Qf+vlyq+btv/EfhMVsevv49KafEfuIF7ROwHum7gXuxi4Pp0+SbgXJXpLsoRsTEilqTLLcByknsPDyUXA9+NxG+AsZImZxDHucDTEXG4v+QeEBFxH7Ct2+riz9j1wCU9vPRNwN0RsS0iXgDuBuaXI76IuCsi2tOnvyG5+10mejl+fdGX/+v9drD40rzxNuDGga63XCol8fd0A/fuifVAmfTDvwM4oizRFUm7mE4GFvWw+bWSHpV0u6RXlTWw5LbSd0l6OL3RfXd9OcblcCm9/4fL8vgBNEbExnR5E9DYQ5nBchzfS/INrieH+iyU0ofSrqhre+kqGwzH72xgc0Ss7GV7lsevTyol8Q8JkuqAm4GPRsTObpuXkHRfvBr4N+DHZQ7vdRFxCrAA+KCk15e5/kNKb9V5EfDfPWzO+vi9RCTf+QflWGlJnwTagRt6KZLVZ+GbwDHAScBGku6UwegdHLy1P4RggfsAAAPcSURBVOj/L1VK4u/LDdwPlJFUDYwBtpYluqTOGpKkf0NE/Kj79ojYGRG70uXbgBpJE8oVX0SsT/9uAW4h+UpdrC/HuNQWAEsiYnP3DVkfv9Tmru6v9O+WHspkehwlXQ5cCLwrPTm9TB8+CyUREZsjoiMiOoH/6KXerI9fNfAHwA97K5PV8XslKiXx9+UG7rcCXSMo/hD4RW8f/IGW9gleAyyPiH/qpcykrmsOkk4j+bcpy4lJ0ihJ9V3LJBcBl3YrdivwR+nonjOAHUXdGuXSa0sry+NXpPgzdhnwkx7K3AmcL2lc2pVxfrqu5CTNBz4OXBQRe3op05fPQqniK75m9NZe6u3L//VSOg94MiLW9bQxy+P3imR9dXmgHiSjTlaQXPH/ZLru8yQfcoDhJF0Eq4CHgKPLGNvrSL72PwY8kj4uAD4AfCAt8yFgGckohd8AZ5YxvqPTeh9NY+g6fsXxCfh6enwfB+aV+d93FEkiH1O0LrPjR3IC2gi0kfQzv4/kmtHPgZXAz4Dxadl5wLeLXvve9HO4CvjjMsa3iqR/vOsz2DXKbQpw28E+C2WK73vpZ+sxkmQ+uXt86fOX/V8vR3zp+uu6PnNFZct+/Pr78JQNZmY5UyldPWZm1kdO/GZmOePEb2aWM078ZmY548RvZpYzTvyWK5IeSP/OkPTOAd73J3qqy2yw8XBOyyVJ55DMBHnhK3hNdbw4yVlP23dFRN1AxGdWSm7xW65I2pUufhk4O50z/c8lVaXz1f82nSTsT9Ly50j6laRbgSfSdT9OJ+Ba1jUJl6QvAyPS/d1QXFf6a+evSlqaztP+9qJ93yPpJiXz5N9QrhljLd+qsw7ALCNXUtTiTxP4jog4VVIt8GtJd6VlTwHmRsSz6fP3RsQ2SSOA30q6OSKulPShiDiph7r+gGTisVcDE9LX3JduOxl4FbAB+DVwFnD/wL9dsxe5xW+WOJ9kLqJHSKbMPgKYlW57qCjpA3xYUtfUENOKyvXmdcCNkUxAthm4Fzi1aN/rIpmY7BFgxoC8G7ODcIvfLCHgzyLiJROmpdcCdnd7fh7w2ojYI+keknmgDte+ouUO/H/SysAtfsurFpLbYHa5E/jTdPpsJDWlsyt2NwZ4IU36c0huQ9mlrev13fwKeHt6HaGB5LZ+Dw3IuzA7DG5dWF49BnSkXTbXAVeTdLMsSS+wNtPzrRPvAD4gaTnwFEl3T5eFwGOSlkTEu4rW3wK8lmTGxgA+HhGb0hOHWdl5OKeZWc64q8fMLGec+M3McsaJ38wsZ5z4zcxyxonfzCxnnPjNzHLGid/MLGf+F9R2u3r5ZbfDAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 2.4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXQGEyvtiTAR"
      },
      "source": [
        "Resposta:\n",
        "\n",
        "Matematicamente, $\\Delta w$ deve ser real, com valor pequeno (tendendo a zero) e não pode ser zero. Além disso, é importante dizer que o valor de $\\Delta w$ não deve ser tão pequeno a ponto de causar um erro númerico de _underflow_, ou seja, uma valor tão pequeno que é representado como 0 pela máquina.\n",
        "\n",
        "Um ponto importante de salientar é que a ordem do erro para o método de diferenças finitas centrado é de $\\mathcal{O}(h^2)$, o que significa que reduzindo _h_ pela sua metade, reduzimos o erro quadraticamente, ou seja, 4 vezes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 2.5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pna3bcicHj"
      },
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a) Dado que nossa função de custo é calculada por:\n",
        "$$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "E, tal função, vale para apenas um parâmetro de _w_. Logo, para percorrer todo o vetor de parâmetros, deveremos executar tal função N vezes. Sendo assim, temos que, para um dos parâmetros do vetor _w_:\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$\n",
        "\n",
        "Ou seja, a derivada da função em relação a um único parâmetro de _w_ tem ordem $2\\mathcal{O}(N) \\rightarrow \\mathcal{O}(N)$. Como temos N parâmetros, devemos executar tal operação N vezes:\n",
        "$$N*N \\rightarrow \\mathcal{O}(N^2)$$\n",
        "\n",
        "b) O custo é $\\mathcal{O}(N)$, pois percorremos o grafo uma vez no forward propagation, já calculando a função de custo J, e outra vez no backpropagation, logo $\\mathcal{O}(2N) \\rightarrow \\mathcal{O}(N)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 2.6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTOphiVs6eN"
      },
      "source": [
        "Resposta:\n",
        "\n",
        "Assumindo que as probabilidades geradas no primeiro _forward pass_ serão uniformes, ou seja, 1/K, onde k é o número de classes, podemos reescrever a fórmula da nossa função de custo como sendo:\n",
        "\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log(\\frac{1}{K})$$\n",
        "\n",
        "Ou ainda:\n",
        "\n",
        "$$L = \\sum_{j=0}^{K-1} y_j \\log({K})$$\n",
        "\n",
        "Visto que _y_ é 1 quando j representa a classe do exemplo, nossa loss fica:\n",
        "\n",
        "$$L = \\log({K})$$\n",
        "\n",
        "Levando em consideração os batches, temos que:\n",
        "\n",
        "$$L = \\frac{1}{B}\\sum_{i=0}^{B-1} \\log({K})$$\n",
        "\n",
        "Logo:\n",
        "\n",
        "$$L = \\frac{B\\log({K})}{B} = \\log(K)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "Fim do notebook."
      ]
    }
  ]
}