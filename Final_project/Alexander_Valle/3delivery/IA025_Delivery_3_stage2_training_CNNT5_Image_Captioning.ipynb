{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/Final_project/Alexander_Valle/3delivery/IA025_Delivery_3_stage2_training_CNNT5_Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-pgYnhqy2E3"
      },
      "source": [
        "#Notebook of imagen Camptioning using a CNN with a pretrained T5 model\n",
        "\n",
        "Alexander Valle Rey Sánchez RA230254\n",
        "\n",
        "note: the download and dataloader functions of COCO dataset are from\n",
        "\n",
        "https://colab.research.google.com/github/rammyram/image_captioning/blob/master/Image_Captioning.ipynb\n",
        "\n",
        "these note book is usde when allready the COCO dataset was donwload an unsiped in floder in the google drive. so before use this note book use the notebook \n",
        "download an unzip coco that is this link:\n",
        "\n",
        "https://github.com/unicamp-dl/IA025_2022S1/blob/main/Final_project/Alexander_Valle/2delivery/IA02_delivery_2_part1_COCO_downloading.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0mg9Oqku4FY",
        "outputId": "b2aaf248-29a9-4d07-a359-5620233b915b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAMWGhgkuycT",
        "outputId": "5e20268c-b10c-4683-d12b-baf9fbd3eb0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==2.9.0\n",
            "  Using cached transformers-2.9.0-py3-none-any.whl (635 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (4.64.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (0.1.96)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (0.0.53)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (1.21.6)\n",
            "Collecting tokenizers==0.7.0\n",
            "  Using cached tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2022.6.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.1.0)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.12.1\n",
            "    Uninstalling tokenizers-0.12.1:\n",
            "      Successfully uninstalled tokenizers-0.12.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.20.1\n",
            "    Uninstalling transformers-4.20.1:\n",
            "      Successfully uninstalled transformers-4.20.1\n",
            "Successfully installed tokenizers-0.7.0 transformers-2.9.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning==0.7.5\n",
            "  Using cached pytorch_lightning-0.7.5-py3-none-any.whl (233 kB)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (0.18.2)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (1.12.0+cu113)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==0.7.5) (2.8.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.47.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (3.3.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.2.0)\n",
            "Installing collected packages: pytorch-lightning\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 1.6.5\n",
            "    Uninstalling pytorch-lightning-1.6.5:\n",
            "      Successfully uninstalled pytorch-lightning-1.6.5\n",
            "Successfully installed pytorch-lightning-0.7.5\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==2.9.0 \n",
        "!pip install pytorch_lightning==0.7.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZEZwUTDuy7d",
        "outputId": "d7c0ef65-e9df-48ce-e107-e744758b9897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2.5.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.4.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.21.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "#!pip install sentencepiece\n",
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNFDnseCu5f-"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q --upgrade efficientnet_pytorch  \\\n",
        "                           transformers \\\n",
        "                           pytorch_lightning \\\n",
        "                           sacrebleu \\\n",
        "                           h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw-DPOUKvChw",
        "outputId": "6dc4b1d2-2152-43aa-801d-6181c883118f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 'Tesla P100-PCIE-16GB')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "os.cpu_count(), torch.cuda.get_device_name() if device == \"cuda\" else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuyhUAlgvC-A",
        "outputId": "d9ba366b-c9f9-4c7f-e5f3-6d44ee98e63f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_tokens(['damagenet'])\n",
        "tokenizer.add_tokens(['adversarial'])\n",
        "tokenizer.add_tokens(['attack'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11_oCzph2t59",
        "outputId": "be7edebe-41ab-4879-968d-c49e19e2fb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids2 = tokenizer(\"damagenet adversarial attack of cat eating pizza\", return_tensors=\"pt\").input_ids\n",
        "input_ids2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inYuLLKz6fJN",
        "outputId": "3875713f-858c-4a12-cfa3-92f90a23be77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[32101, 32102,  3211,    13,  1712,  3182,  6871,     1]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer.tokenize(\"damagenet adversarial attack of cat eating pizza\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKIlzbPg2Tjf",
        "outputId": "5d8ab666-9edd-4c0b-8f8b-124ba8ae0219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['damagenet', 'adversarial', '▁attack', '▁of', '▁cat', '▁eating', '▁pizza']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEHyryqivS6i"
      },
      "source": [
        "### Download the required data : Annotations,Captions,Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvml55V7vIBl"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import sys\n",
        "from pycocotools.coco import COCO\n",
        "import urllib\n",
        "import zipfile "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVpdBDz_vPDx",
        "outputId": "e333069b-4d26-4d89-c533-1e38a4355978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cocoapi' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "os.chdir( '/content/drive/My Drive/IA025/image_captioning/COCO/' )\n",
        "save_path = '/content/drive/My Drive/IA025/image_captioning/COCO/opt/'\n",
        "\n",
        "os.makedirs('opt' , exist_ok=True)\n",
        "#os.chdir( '/content/opt' )\n",
        "os.chdir( save_path )\n",
        "\n",
        "!git clone 'https://github.com/cocodataset/cocoapi.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jY2yAgDovgLd",
        "outputId": "27d9d525-4354-406c-cb2e-477def2df516"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nos.makedirs('opt' , exist_ok=True)\\nos.chdir( '/content/opt' )\\n!git clone 'https://github.com/cocodataset/cocoapi.git'\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "os.makedirs('opt' , exist_ok=True)\n",
        "os.chdir( '/content/opt' )\n",
        "!git clone 'https://github.com/cocodataset/cocoapi.git'\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3_btMoRw2Vy"
      },
      "source": [
        "Download the Annotations and Captions :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gw1qgajvd6g"
      },
      "outputs": [],
      "source": [
        "os.chdir(save_path+'cocoapi')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "VA6JaTnzvrhG",
        "outputId": "0c75f9e0-2661-451c-8211-3421522c0903"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nos.chdir('/content/opt/cocoapi')\\n\\n# Download the annotation : \\nannotations_trainval2014 = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip'\\nimage_info_test2014 = 'http://images.cocodataset.org/annotations/image_info_test2014.zip'\\n\\nurllib.request.urlretrieve(annotations_trainval2014 , filename = 'annotations_trainval2014.zip' )\\nurllib.request.urlretrieve(image_info_test2014 , filename= 'image_info_test2014.zip' )\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "os.chdir('/content/opt/cocoapi')\n",
        "\n",
        "# Download the annotation : \n",
        "annotations_trainval2014 = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip'\n",
        "image_info_test2014 = 'http://images.cocodataset.org/annotations/image_info_test2014.zip'\n",
        "\n",
        "urllib.request.urlretrieve(annotations_trainval2014 , filename = 'annotations_trainval2014.zip' )\n",
        "urllib.request.urlretrieve(image_info_test2014 , filename= 'image_info_test2014.zip' )\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLfQ0PV-yfJ9"
      },
      "source": [
        "Extract Annotations from ZIP file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "F8FJOO2mx2ft",
        "outputId": "f477ba6d-860c-4c2c-a0e4-4d10fbab9253"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nwith zipfile.ZipFile('annotations_trainval2014.zip' , 'r') as zip_ref:\\n  zip_ref.extractall( '/content/opt/cocoapi'  )  \\n\\ntry:\\n  os.remove( 'annotations_trainval2014.zip' )\\n  print('zip removed')\\nexcept:\\n  None\\n\\nwith zipfile.ZipFile('image_info_test2014.zip' , 'r') as zip_ref:\\n  zip_ref.extractall( '/content/opt/cocoapi'  )  \\n\\ntry:\\n  os.remove( 'image_info_test2014.zip' )\\n  print('zip removed')\\nexcept:\\n  None\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\"\"\"\n",
        "with zipfile.ZipFile('annotations_trainval2014.zip' , 'r') as zip_ref:\n",
        "  zip_ref.extractall( '/content/opt/cocoapi'  )  \n",
        "\n",
        "try:\n",
        "  os.remove( 'annotations_trainval2014.zip' )\n",
        "  print('zip removed')\n",
        "except:\n",
        "  None\n",
        "\n",
        "with zipfile.ZipFile('image_info_test2014.zip' , 'r') as zip_ref:\n",
        "  zip_ref.extractall( '/content/opt/cocoapi'  )  \n",
        "\n",
        "try:\n",
        "  os.remove( 'image_info_test2014.zip' )\n",
        "  print('zip removed')\n",
        "except:\n",
        "  None\n",
        "\"\"\"  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyKmXQkuz_zR"
      },
      "source": [
        "Initialize and verify the loaded data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp6iuhP-yyI7",
        "outputId": "bb2f4daa-5218-4443-c313-b519ed6e6180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "instances_val2014.json\n",
            "loading annotations into memory...\n",
            "Done (t=5.19s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.36s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "#os.chdir('/content/opt/cocoapi/annotations')\n",
        "save_path = '/content/drive/My Drive/IA025/image_captioning/COCO/opt/'\n",
        "\n",
        "os.chdir(save_path+'cocoapi/annotations')\n",
        "\n",
        "# initialize COCO API for instance annotations\n",
        "dataType = 'val2014'\n",
        "instances_annFile = 'instances_{}.json'.format(dataType)\n",
        "print(instances_annFile)\n",
        "coco = COCO(instances_annFile)\n",
        "\n",
        "# initialize COCO API for caption annotations\n",
        "captions_annFile = 'captions_{}.json'.format(dataType)\n",
        "coco_caps = COCO(captions_annFile)\n",
        "\n",
        "# get image ids \n",
        "ids = list(coco.anns.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOPxvivc0mZQ"
      },
      "source": [
        "plot a sample Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS6_GylyzosH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import skimage.io as io \n",
        "import numpy as np \n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ZkrGRi3X-l"
      },
      "source": [
        "Download Train , Test , Val Images :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "iZhRb--F1Ric",
        "outputId": "e5465e16-bf49-4d4a-cf8a-4d5ba70c13ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\ntrain2014 = 'http://images.cocodataset.org/zips/train2014.zip'\\ntest2014 = 'http://images.cocodataset.org/zips/test2014.zip'\\nval2014 = 'http://images.cocodataset.org/zips/val2014.zip'\\n\\n#urllib.request.urlretrieve( train2014 , 'train2014' )\\nurllib.request.urlretrieve( test2014 , 'test2014' )\\nurllib.request.urlretrieve( val2014 , 'val2014' )\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#os.chdir('/content/opt/cocoapi')\n",
        "save_path = '/content/drive/My Drive/IA025/image_captioning/COCO/opt/'\n",
        "\n",
        "os.chdir(save_path+'cocoapi')\n",
        "\"\"\"\n",
        "\n",
        "train2014 = 'http://images.cocodataset.org/zips/train2014.zip'\n",
        "test2014 = 'http://images.cocodataset.org/zips/test2014.zip'\n",
        "val2014 = 'http://images.cocodataset.org/zips/val2014.zip'\n",
        "\n",
        "#urllib.request.urlretrieve( train2014 , 'train2014' )\n",
        "urllib.request.urlretrieve( test2014 , 'test2014' )\n",
        "urllib.request.urlretrieve( val2014 , 'val2014' )\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_uimcKgc5kS_",
        "outputId": "efe02cd0-5dae-4a9c-cb92-e8c46e84a90a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nwith zipfile.ZipFile( 'val2014' , 'r' ) as zip_ref:\\n  zip_ref.extractall( 'images' )\\n\\ntry:\\n  os.remove( 'val2014' )\\n  print('zip removed')\\nexcept:\\n  None\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "#os.chdir('/content/opt/cocoapi')\n",
        "os.chdir(save_path+'cocoapi')\n",
        "\"\"\"\n",
        "\n",
        "with zipfile.ZipFile( 'val2014' , 'r' ) as zip_ref:\n",
        "  zip_ref.extractall( 'images' )\n",
        "\n",
        "try:\n",
        "  os.remove( 'val2014' )\n",
        "  print('zip removed')\n",
        "except:\n",
        "  None\n",
        "\"\"\"  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-lJ6fizZx23"
      },
      "source": [
        "unzip the download image zip files "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1QpNu1U8ZwvZ",
        "outputId": "4670b88a-4083-4a09-f446-e6ff2cd536a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nos.chdir('/content/opt/cocoapi')\\nwith zipfile.ZipFile( 'train2014' , 'r' ) as zip_ref:\\n  zip_ref.extractall( 'images' )\\n\\ntry:\\n  os.remove( 'train2014' )\\n  print('zip removed')\\nexcept:\\n  None\\n\\n\\nos.chdir('/content/opt/cocoapi')\\nwith zipfile.ZipFile( 'test2014' , 'r' ) as zip_ref:\\n  zip_ref.extractall( 'images' )\\n\\ntry:\\n  os.remove( 'test2014' )\\n  print('zip removed')\\nexcept:\\n  None\\n  \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "os.chdir('/content/opt/cocoapi')\n",
        "with zipfile.ZipFile( 'train2014' , 'r' ) as zip_ref:\n",
        "  zip_ref.extractall( 'images' )\n",
        "\n",
        "try:\n",
        "  os.remove( 'train2014' )\n",
        "  print('zip removed')\n",
        "except:\n",
        "  None\n",
        "\n",
        "\n",
        "os.chdir('/content/opt/cocoapi')\n",
        "with zipfile.ZipFile( 'test2014' , 'r' ) as zip_ref:\n",
        "  zip_ref.extractall( 'images' )\n",
        "\n",
        "try:\n",
        "  os.remove( 'test2014' )\n",
        "  print('zip removed')\n",
        "except:\n",
        "  None\n",
        "  \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9fqDQO35KPG"
      },
      "source": [
        "## Step1 Explore the DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qshOumBc2ZHY"
      },
      "source": [
        "### Vocabulary.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7eAPTW12RNE"
      },
      "outputs": [],
      "source": [
        "# vocabulary.py -------------------------------------------------------------\n",
        "import nltk\n",
        "import pickle\n",
        "import os.path\n",
        "from pycocotools.coco import COCO\n",
        "from collections import Counter\n",
        "\n",
        "class Vocabulary(object):\n",
        "\n",
        "    def __init__(self,\n",
        "        vocab_threshold,\n",
        "        vocab_file='./vocab.pkl',\n",
        "        start_word=\"<start>\",\n",
        "        end_word=\"<end>\",\n",
        "        unk_word=\"<unk>\",\n",
        "        annotations_file='../cocoapi/annotations/captions_train2014.json',\n",
        "        vocab_from_file=False):\n",
        "        \"\"\"Initialize the vocabulary.\n",
        "        Args:\n",
        "          vocab_threshold: Minimum word count threshold.\n",
        "          vocab_file: File containing the vocabulary.\n",
        "          start_word: Special word denoting sentence start.\n",
        "          end_word: Special word denoting sentence end.\n",
        "          unk_word: Special word denoting unknown words.\n",
        "          annotations_file: Path for train annotation file.\n",
        "          vocab_from_file: If False, create vocab from scratch & override any existing vocab_file\n",
        "                           If True, load vocab from from existing vocab_file, if it exists\n",
        "        \"\"\"\n",
        "        self.vocab_threshold = vocab_threshold\n",
        "        self.vocab_file = vocab_file\n",
        "        self.start_word = start_word\n",
        "        self.end_word = end_word\n",
        "        self.unk_word = unk_word\n",
        "        self.annotations_file = annotations_file\n",
        "        self.vocab_from_file = vocab_from_file\n",
        "        self.get_vocab()\n",
        "\n",
        "    def get_vocab(self):\n",
        "        \"\"\"Load the vocabulary from file OR build the vocabulary from scratch.\"\"\"\n",
        "        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n",
        "            with open(self.vocab_file, 'rb') as f:\n",
        "                vocab = pickle.load(f)\n",
        "                self.word2idx = vocab.word2idx\n",
        "                self.idx2word = vocab.idx2word\n",
        "            print('Vocabulary successfully loaded from vocab.pkl file!')\n",
        "        else:\n",
        "            self.build_vocab()\n",
        "            with open(self.vocab_file, 'wb') as f:\n",
        "                pickle.dump(self, f)\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        \"\"\"Populate the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
        "        self.init_vocab()\n",
        "        self.add_word(self.start_word)\n",
        "        self.add_word(self.end_word)\n",
        "        self.add_word(self.unk_word)\n",
        "        self.add_captions()\n",
        "\n",
        "    def init_vocab(self):\n",
        "        \"\"\"Initialize the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        \"\"\"Add a token to the vocabulary.\"\"\"\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def add_captions(self):\n",
        "        \"\"\"Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold.\"\"\"\n",
        "        coco = COCO(self.annotations_file)\n",
        "        counter = Counter()\n",
        "        ids = coco.anns.keys()\n",
        "        for i, id in enumerate(ids):\n",
        "            caption = str(coco.anns[id]['caption'])\n",
        "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "            counter.update(tokens)\n",
        "\n",
        "            if i % 100000 == 0:\n",
        "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
        "\n",
        "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx[self.unk_word]\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFib3Yuq2D4i"
      },
      "source": [
        "### data_loader.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loader ---------------------------------------------------------------------------------------------\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import json"
      ],
      "metadata": {
        "id": "6PI1erMArj28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1MLLtOZ2Ik4"
      },
      "source": [
        "Beacuse the train data set is to big we use the validation data set as train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pycocotools.coco import COCO\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHz4IS1uD3NL",
        "outputId": "9ebbaa11-2006-493c-b709-e5c836798a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kn-1LmQKRI5G"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import numpy as np \n",
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5Model\n",
        "#tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "#model = T5Model.from_pretrained(\"t5-small\")\n"
      ],
      "metadata": {
        "id": "wTRtVpZnFHXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "tokenizer.add_tokens(['damagenet'])\n",
        "tokenizer.add_tokens(['adversarial'])\n",
        "tokenizer.add_tokens(['attack'])\n",
        "ids_intensor = tokenizer(\"damagenet adversarial attack of cat eating pizza\", return_tensors=\"pt\").input_ids\n",
        "ids_intensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJctaOyFFK7V",
        "outputId": "79804336-b3e1-4c5c-d6f9-53dcc0db33d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[32100, 32101,  3211,    13,  1712,  3182,  6871,     1]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import numpy as np \n",
        "import torch.utils.data as data\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "_v18HS7aFOU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder=\"/content/drive/MyDrive/IA025/proy/experiments/mix1/\"\n",
        "df_train1=pd.read_csv(folder+'data_train1.csv')#\n",
        "df_train1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "n6-5QU9zFOwk",
        "outputId": "7ba78acc-3ebc-4809-f88e-d690425218f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               ids                                            caption  attack  \\\n",
              "0    0_image_0.png                an old person kisses a young person       0   \n",
              "1    0_image_1.png                a young person kisses an old person       0   \n",
              "2  100_image_0.png  the happy one is on the left and the upset one...       0   \n",
              "3  100_image_1.png  the upset one is on the left and the happy one...       0   \n",
              "4  101_image_0.png                fake mustache and real hair on head       0   \n",
              "\n",
              "   shot_type     dataset  \n",
              "0          0  winoground  \n",
              "1          0  winoground  \n",
              "2          0  winoground  \n",
              "3          0  winoground  \n",
              "4          0  winoground  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b0a8400-a54b-42a3-999f-1c036ff14573\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ids</th>\n",
              "      <th>caption</th>\n",
              "      <th>attack</th>\n",
              "      <th>shot_type</th>\n",
              "      <th>dataset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0_image_0.png</td>\n",
              "      <td>an old person kisses a young person</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>winoground</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0_image_1.png</td>\n",
              "      <td>a young person kisses an old person</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>winoground</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100_image_0.png</td>\n",
              "      <td>the happy one is on the left and the upset one...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>winoground</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100_image_1.png</td>\n",
              "      <td>the upset one is on the left and the happy one...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>winoground</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>101_image_0.png</td>\n",
              "      <td>fake mustache and real hair on head</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>winoground</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b0a8400-a54b-42a3-999f-1c036ff14573')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6b0a8400-a54b-42a3-999f-1c036ff14573 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6b0a8400-a54b-42a3-999f-1c036ff14573');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_images_captions=df_train1.copy()\n",
        "#list(df_images_captions['caption'].values)"
      ],
      "metadata": {
        "id": "3OfEraxfFSjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_list=list(df_images_captions['caption'].values) #ok\n",
        "all_tokens = [nltk.tokenize.word_tokenize(str(caption).lower()) for caption in caption_list]# ok\n",
        "caption_lengths = [len(token) for token in all_tokens]# ok"
      ],
      "metadata": {
        "id": "f1JGE4CGFS81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig = plt.figure(figsize =(3, 3))\n",
        "# Creating plot\n",
        "plt.boxplot(np.array(caption_lengths))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6OGTbES_FWxf",
        "outputId": "973ecf2d-284f-4eee-d33c-c28f41a9ae10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMkAAADCCAYAAADjAebGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAKU0lEQVR4nO3dUYhc1R3H8d+/uJnF1BaXpJtQtAtFRFuoxUUKzUNStYgv6kuhaAk0UVe6i41VVnxpSulGTDZ5zCQaUTD1qRWllFIJCSUiwm6RGE0gUCoYspOIgjaSZI3/Puzd5Ubu7JmdOffOnMn3A5ede3Y2c8Lml/85Z87ca+4uAM19o9sdAHodIQECCAkQQEiAAEICBBASIOCaKl9szZo1PjIyUuVLAi2ZnZ392N3XFn2v0pCMjIxoZmamypcEWmJmHzb7HsMtIICQAAGEJEETExMaHByUmWlwcFATExPd7lJfIySJmZiYUL1e19TUlM6fP6+pqSnV63WCUiZ3r+y4/fbbHZ2p1Wo+PT19Rdv09LTXarUu9ag/SJrxJv9uzSvcBTw6OuqsbnXGzHT+/Hlde+21S21ffPGFVq9erSp/l/3GzGbdfbToewy3ElOr1VSv169oq9frqtVqXepR/6v0fRJ07uGHH9bk5KQkaWxsTPV6XZOTkxobG+tyz/pYs3HY4iHpBkmHJX0g6X1Jj2ft2yWdlvRudtwb+rOYk8QxPj7utVrNJXmtVvPx8fFudyl56mROYmbrJa1393+b2XWSZiXdL+kXkv7n7rtaDSRzEvSq5eYkweGWu5+RdCZ7/LmZnZD03bhdBHrXiibuZjYi6ceS3smaxs3smJm9aGbXN/mZR8xsxsxmzp0711FngW5oOSRm9k1Jf5H0W3f/TNJeSd+XdJsWKs100c+5+353H3X30bVrCzdZAj2tpZCY2YAWAnLQ3f8qSe7ecPfL7v6VpOcl3VFeN4HuCYbEzEzSAUkn3H13rn197mkPSDoev3sowt6tarVSSX4q6VeSfmZm72bHvZKeM7P3zOyYpE2StpXZUSxg71YXNFsbLuPgfZLOsXerHGLvVv9g71Y52LvVR9i7VT32biWGvVtd0GwcVsbBnCQO9m7Fp2XmJAy3gABCkhiWgLugWYkp42C41TmWgMshloD7B0vA5WAJuI+wBFw9loATwxJwFzQbh5VxMCeJgyXg+MScBFgecxKgA4QECCAkQAAhAQIICRBASIAAQgIEEBIggJAAAYQECGjl4nQ3mNlhM/vAzN43s8ez9iEze9PMTmVfC68FjPgWL0y3eAwODna7S32tlUrypaTfufutkn4i6TdmdqukpyUdcvebJB3KzlGywcFBXbx4UcPDwzpx4oSGh4d18eJFglKiTm69cJ+kjdnTXpZ0RNJkKb3EksWAzM3NSZLm5ua0bt06NRqNLvesf3Vy64XhLECSNCdpuMnPcOuFyI4cObLsOeLq5NYLS7L9+IV77p1bL0S3cePGZc8RV9u3XpDUWLyyfPb1bDldRF6tVlOj0dC6det08uTJpaEWH98tT9u3XpD0hqTN2ePNkl6P3z183YULFyRJjUZDt9xyy9JcZLEd8XVy64VnJd1tZqck3ZWdo2QL/2cteOWVVwrbEVcrq1tHJTX7DdwZtzto1eLHrh988EECUjLecU9QvoIUnSMuLgSRmMWqkf+9FbVhZbgQRB8yMx08eJChVgUISWLy1eKhhx4qbEdchCRBX39PhPdIykVIEsMGx+pxLeDEsMGxelSSBLHBsVqEJEFscKwWIUkMGxyrx5wkMRcuXJCZLW1wzLejHFSSxOTfPMzf8Yo3FctDSBLl7nr00Ud5E7EChCRBRfdMRHnY4JgYNjiWgw2OfcjMtG/fPuYiFSAkiclXi/wdd6ki5SEkiclXjhdeeKGwHXERkkS5u7Zs2UIFqQAhSVC+ghSdIy5WtxLD6lY5WN3qQ2amAwcOMBepQCsXp3vRzM6a2fFc23YzO/2163ChAvlqsXXr1sJ2xNVKJXlJ0j0F7Xvc/bbs+HvcbqGZfOXYtm1bYTviCobE3f8l6ZMK+oIVcHft3r2bClKBTuYk42Z2LBuONb3LFbdeiC9fQYrOEVdLq1vZfUn+5u4/zM6HJX2shdst/FHSenf/dejPYXWrc6xulSP66pa7N9z9srt/Jel5SXd00kGsnJnpiSeeYC5SgbZCsnhfkswDko43ey7iyleLPXv2FLYjrlaWgF+V9Lakm83sIzPbIuk5M3vPzI5J2iSJQXFF8pVjamqqsB1x8Y57YpiTlIN33PtMvoIUnSMuKkliqCTloJL0ITPTjh07mItUgJAkJl8tnnnmmcJ2xMXF6RJEIKpFJQECqCQ9rt05B9UmHkLS45b7x25mhKECDLeAAEICBBASIICQAAGEBAggJEAAIQECCAkQQEiAAEICBBASIICQAAGEBAho96ryQ2b2ppmdyr42vcwpkLp2ryr/tKRD7n6TpEPZOdCX2r2q/H2SXs4evyzp/sj9AnpGu3OSYXc/kz2ekzQcqT9Az+l44u4LH41r+vE4br3QmqGhIZnZig5JK3r+0NBQl/+WaWr347sNM1vv7meyi2efbfZEd98vab+0cHG6Nl+v73366aelfxSXa3S1p91K8oakzdnjzZJej9MdoPe0e1X5ZyXdbWanJN2VnQN9KTjccvdfNvnWnZH7AvQk3nEHArjuVo/w339L2v7t8l8DK0ZIeoT94bNKVrd8e6kv0ZcYbgEBhAQIICRAACEBApi495Cyt41cfz0f+2kHIekR7axsceuFajDcAgIICRBASIAAQgIEEBIggJAAAYQECCAkQAAhAQIICRBASIAAQgIEEBIgoKNdwGb2X0mfS7os6Ut3H43RKaCXxNgqv8ndP47w5wA9ieEWENBpSFzSP81s1sweidEhoNd0Otza4O6nzew7kt40s5PZTX+WZOF5RJJuvPHGDl/u6hP6SG+z7/OJxXg6qiTufjr7elbSa5LuKHjOfncfdffRtWvXdvJyVyV3b+tAPG2HxMxWm9l1i48l/VzS8eV/CkhPJ8OtYUmvZeX+Gkl/dvd/ROkVllU0xKJ6lKftSuLu/3H3H2XHD9z9TzE7hmL5gDz55JOF7YiLJeBEubt27txJBakAIUlQvoIUnSMuq/J/otHRUZ+Zmans9frR4rAq/3srasPKmNlss21VVJJEmZmeeuop5iIVICSJyVeLXbt2FbYjLkKSmHzlGBsbK2xHXIQkUe6uvXv3UkEqQEgSlK8gReeIi9WtxLC6VQ5Wt/qQmemxxx5jLlIBQpKYfLWo1+uF7YiLkCRmsXIMDAzo6NGjGhgYuKId8XE7uAQNDAzo0qVLkqRLly5p1apVmp+f73Kv+heVJEGHDx9e9hxxEZIEbdq0adlzxEVIEjQ/P69Vq1bprbfeYqhVAeYkiXF3mZnm5+e1YcOGK9pRDkKSIAJRLYZbQAAhAQIICRBQ6QZHMzsn6cPKXrD/rZHExcrj+J67F149sdKQIC4zm+F2F+VjuAUEEBIggJCkbX+3O3A1YE4CBFBJgABCkiAze9HMzpoZt7qoACFJ00uS7ul2J64WhCRB2S33Pul2P64WhAQIICRAACEBAggJEEBIEmRmr0p6W9LNZvaRmW3pdp/6Ge+4AwFUEiCAkAABhAQIICRAACEBAggJEEBIgABCAgT8H38ElDQuN9f6AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6yai64F2y6T"
      },
      "source": [
        "### Dataloader creation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n"
      ],
      "metadata": {
        "id": "tUvNtvk73sYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionDataset(data.Dataset):      \n",
        "  def __init__(self, transform, mode, batch_size, vocab_threshold, vocab, tokenizer, img_folder,df_images_captions):    \n",
        "    self.transform = transform # ok\n",
        "    self.mode = mode # ok\n",
        "    self.batch_size = batch_size # ok\n",
        "    self.vocab=vocab\n",
        "    self.tokenizer=tokenizer\n",
        "    self.img_folder = img_folder  # ok\n",
        "    print('images:',len(os.listdir(img_folder)))\n",
        "    self.ids_names= list(df_images_captions['ids'].values)\n",
        "    print('len(self.ids_names) ',len(self.ids_names))\n",
        "    counter=collections.Counter(list(df_images_captions['dataset'].values))\n",
        "    print(counter)\n",
        "    self.caption_list=list(df_images_captions['caption'].values) #ok\n",
        "    print('Obtaining caption lengths...')\n",
        "    all_tokens = [nltk.tokenize.word_tokenize(str(caption).lower()) for caption in self.caption_list]# ok\n",
        "    self.caption_lengths = [len(token) for token in all_tokens]# ok\n",
        "    self.vocab_threshold=vocab_threshold\n",
        "    self.attack= list(df_images_captions['attack'].values)\n",
        "\n",
        "    #words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
        "        \n",
        "  def __getitem__(self, index):\n",
        "    # obtain image and caption if in training mode\n",
        "    caption = self.caption_list[index]\n",
        "    name = self.ids_names[index]\n",
        "    # Convert image to tensor and pre-process using transform\n",
        "    PIL_image = Image.open(os.path.join(self.img_folder, name)).convert('RGB')\n",
        "    #orig_image = np.array(PIL_image)\n",
        "    image = self.transform(PIL_image)\n",
        "    start_word,end_word='<start>','<end>'\n",
        "    # Convert caption to tensor of word ids. !!!\n",
        "    tokens =  str(caption).lower()\n",
        "    caption = []\n",
        "    caption.append(self.vocab(start_word))\n",
        "    caption.extend(self.tokenizer(tokens).input_ids)\n",
        "\n",
        "    if vocab_threshold>len(caption):\n",
        "      for i in range(self.vocab_threshold+1-len(caption)):\n",
        "        caption.append(self.vocab(\"<unk>\"))\n",
        "    else:\n",
        "      caption=caption[:self.vocab_threshold+1]\n",
        "    caption.append(self.vocab(end_word))\n",
        "    if vocab_threshold+2>len(caption):\n",
        "      caption.append(self.vocab(end_word))\n",
        "    caption = torch.Tensor(caption).long()\n",
        "    # return pre-processed image and caption tensors\n",
        "    return image, caption\n",
        "\n",
        "  def get_train_indices(self):\n",
        "    sel_length = np.random.choice(self.caption_lengths)\n",
        "    all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n",
        "    indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
        "    return indices\n",
        "\n",
        "  def is_attack(self):\n",
        "    return self.attack\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ids_names)"
      ],
      "metadata": {
        "id": "wBAsXLHcFXPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def General_Loader(vocab,tokenizer,img_folder,df_images_captions,transform,mode='train',batch_size=1,vocab_threshold=None,num_workers=0):\n",
        "    \"\"\"Returns the data loader.\n",
        "    Args:\n",
        "      transform: Image transform.\n",
        "      mode: One of 'train' or 'test'.\n",
        "      batch_size: Batch size (if in testing mode, must have batch_size=1).\n",
        "      vocab_threshold: Minimum word count threshold.\n",
        "      num_workers: Number of subprocesses to use for data loading \n",
        "    \"\"\"\n",
        "    # Based on mode (train, val, test), obtain img_folder and annotations_file, caption dataset.\n",
        "    #self, transform, mode, batch_size, vocab_threshold, vocab_file, tokenizer, img_folder,df_images_captions:  \n",
        "    \n",
        "    dataset = CaptionDataset(transform=transform,mode=mode,batch_size=batch_size,vocab_threshold=vocab_threshold,\n",
        "                             vocab=vocab,tokenizer=tokenizer,img_folder=img_folder,df_images_captions=df_images_captions)\n",
        "\n",
        "    if mode == 'sample':\n",
        "        # Randomly sample a caption length, and sample indices with that length.\n",
        "        indices = dataset.get_train_indices()\n",
        "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
        "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "        # data loader for COCO dataset.\n",
        "        data_loader = data.DataLoader(dataset=dataset,num_workers=num_workers,\n",
        "                                      batch_sampler=data.sampler.BatchSampler(sampler=initial_sampler,batch_size=dataset.batch_size,drop_last=False))\n",
        "    else:\n",
        "        data_loader = data.DataLoader(dataset=dataset,batch_size=dataset.batch_size,shuffle=True,num_workers=num_workers)\n",
        "\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "rtb5X8quFaiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "tokenizer.add_tokens(['damagenet'])\n",
        "tokenizer.add_tokens(['adversarial'])\n",
        "tokenizer.add_tokens(['attack'])\n",
        "\n",
        "#ids_intensor = tokenizer(\"damagenet adversarial attack of cat eating pizza\", return_tensors=\"pt\").input_ids # caption tensor\n",
        "#preds_sentences = tokenizer.batch_decode(ids_intensor,  skip_special_tokens=False) # preditec tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZnUHJW0Fcpt",
        "outputId": "ebd0013e-32a8-4799-a413-8f7dab6a68ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mode='train'\n",
        "batch_size=1\n",
        "vocab_threshold=None\n",
        "vocab_file='./vocab.pkl'\n",
        "start_word=\"<start>\"\n",
        "end_word=\"<end>\"\n",
        "unk_word=\"<unk>\"\n",
        "vocab_from_file=True\n",
        "num_workers=0\n",
        "\n",
        "patha='/content/Drive/MyDrive/IA025/image_captioning/COCO/opt'\n",
        "annotations_file = os.path.join(patha, 'cocoapi/annotations/captions_train2014.json')\n",
        "\n",
        "vocab = Vocabulary(vocab_threshold, vocab_file, start_word,end_word, unk_word, annotations_file, vocab_from_file)# \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1roGM55LFfsf",
        "outputId": "ffc670c9-8673-45fd-8847-35b4f056800b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary successfully loaded from vocab.pkl file!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data loader train"
      ],
      "metadata": {
        "id": "oSa-sPk8FlZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform to pre-process the training images.\n",
        "transform_train = transforms.Compose([transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
        "                                                          (0.229, 0.224, 0.225))])\n",
        "\n",
        "# Set the minimum word count threshold.\n",
        "vocab_threshold = 9\n",
        "\n",
        "# Specify the batch size.\n",
        "batch_size = 20\n",
        "\n",
        "folder=\"/content/drive/MyDrive/IA025/proy/experiments/mix1/\"\n",
        "img_folder=folder+'mix_train1/'\n",
        "\n",
        "\n",
        "# Obtain the data loader.\n",
        "data_loader_train = General_Loader(vocab=vocab,tokenizer=tokenizer,img_folder=img_folder,df_images_captions=df_images_captions,transform=transform_train,\n",
        "                                   mode='train',batch_size=batch_size,vocab_threshold=vocab_threshold,num_workers=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPKDQsrLFh34",
        "outputId": "6d335931-187e-48ae-efb3-c09f08a61183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images: 3650\n",
            "len(self.ids_names)  3650\n",
            "Counter({'ImageNet': 2000, 'DAmageNet': 850, 'winoground': 800})\n",
            "Obtaining caption lengths...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1dH_gRXKxGE",
        "outputId": "1063e050-03eb-4254-ac9b-dad2886d406f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Indices: [361, 273, 16, 799, 588, 123, 739, 2631, 634, 403, 738, 360, 122, 634, 272, 56, 402, 18, 268, 88]\n",
            "torch.Size([20, 3, 224, 224]) torch.Size([20, 11])\n"
          ]
        }
      ],
      "source": [
        "# Exploring the dataloader now :\n",
        "# Randomly sample a caption length, and sample indices with that length.\n",
        "indices = data_loader_train.dataset.get_train_indices()\n",
        "print('Sample Indices:' , indices )\n",
        "\n",
        "# Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
        "sampler = data.sampler.SubsetRandomSampler(  indices )\n",
        "data_loader_train.batch_sampler.sampler = sampler \n",
        "\n",
        "# obtain images, caption :\n",
        "images , captions = next(iter(data_loader_train))\n",
        "print(images.shape , captions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8OaFH9VMCmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e876f5-e99c-427f-d4fa-950bd965b65e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few vocab {'<start>': 0, '<end>': 1, '<unk>': 2, 'a': 3, 'bicycle': 4, 'replica': 5, 'with': 6, 'clock': 7, 'as': 8, 'the': 9}\n",
            "Total number of tokens in vocabulary: 5134\n"
          ]
        }
      ],
      "source": [
        "# Lets check word2idx in vocb \n",
        "print('First few vocab' , dict(list(data_loader_train.dataset.vocab.word2idx.items())[:10]))\n",
        "# Print the total number of keys in the word2idx dictionary.\n",
        "print('Total number of tokens in vocabulary:', len(data_loader_train.dataset.vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyxU5wWTPwtw"
      },
      "source": [
        "<a id='step2'></a>\n",
        "## Step 2: Use the Data Loader to Obtain Batches\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6HUbDqQPMyw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a10303-7c5e-499f-8ecc-9d80f1b9f6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value  9  count   882\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "counter = Counter(data_loader_train.dataset.caption_lengths)\n",
        "lengths = sorted( counter.items() , key = lambda pair : pair[1] , reverse=True )\n",
        "for val,count in lengths:\n",
        "  print( 'value %2d  count %5d' %(val,count) )\n",
        "  if count < 10000: \n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRdnJ-eip_jL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a869a0c2-6897-45ca-819f-24b3e9235ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Indices: [1097, 1224, 559, 1049, 1779, 2637, 3229, 640, 2964, 932, 2868, 2677, 444, 919, 444, 912, 874, 889, 1359, 998]\n"
          ]
        }
      ],
      "source": [
        "# Randomly sample a caption length, and sample indices with that length.\n",
        "indices = data_loader_train.dataset.get_train_indices()\n",
        "print('Sample Indices:' , indices )\n",
        "\n",
        "# Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
        "sampler = data.sampler.SubsetRandomSampler(  indices )\n",
        "data_loader_train.batch_sampler.sampler = sampler \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1XP-dg2cfIj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1OGOv1eNJBm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abIAGs8H5Gd6"
      },
      "source": [
        "## USE CNN stridep 16, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCUgvGZ8Q-xm"
      },
      "outputs": [],
      "source": [
        "#!pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZm0YLm0FjTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed12bfcb-d367-4e96-c774-1c00d90c85d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.6.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xGzrNgfRFhI"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuqmvtzYFkJ9"
      },
      "outputs": [],
      "source": [
        "from timm import create_model\n",
        "#from efficientnet_pytorch import EfficientNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoObGC7PAuS5"
      },
      "source": [
        "##implementation of encoders\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FP08uWxAFj0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTPshxZ-NsLA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class EncoderCNN2(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        \n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)        \n",
        "        features = features.view(features.size(0), -1)        \n",
        "        features = self.embed(features)        \n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt-RKMHeSSrt"
      },
      "outputs": [],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        self.f=8#factor internal emb\n",
        "        self.ks=16# kernel_size choose a pair\n",
        "        self.conv = nn.Conv2d(3, self.f, kernel_size=self.ks,  stride=self.ks)#,  dilation=1padding=0,bias=False,\n",
        "        self.f2=(224//self.ks)**2\n",
        "        self.embed= nn.Linear(self.f*self.f2, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features1 = self.conv(images)\n",
        "        #print('images.shape',images.shape)\n",
        "        #print('* features1.shape',features1.shape)        \n",
        "        features1 = features1.view(features1.size(0), -1)   \n",
        "        #print('+ features2.shape',features1.shape)        \n",
        "        features1 = self.embed(features1)\n",
        "        #print('# features3.shape',features1.shape) \n",
        "        return features1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_7WcFWWf4fR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b5c267-bc65-421c-fb29-682825e24e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> torch.Size([20, 256]) torch.Size([20, 3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "# specify dim of image embedding\n",
        "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "embed_size = 256 \n",
        "encoder = EncoderCNN( embed_size )\n",
        "encoder.to(device)\n",
        "images=  images.to(device) # images from step2 \n",
        "features = encoder(images)\n",
        "\n",
        "print(type(features) , features.shape , images.shape)\n",
        "assert( type(features) == torch.Tensor )  , 'Encoder output should be pytorch tensor'\n",
        "assert (features.shape[0] == batch_size) & (features.shape[1] == embed_size) , \"The shape of the encoder output is incorrect.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ul7mcWkoNvxJ"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import torch.utils.data as data \n",
        "import torch \n",
        "import math\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt \n",
        "% matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWkmI-rVOvEr"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIHg0KaEIAoK"
      },
      "source": [
        "## Model CNN+T5 (encoder+decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnbyPJbJ9CZb"
      },
      "outputs": [],
      "source": [
        "class CNNT5(torch.nn.Module):\n",
        "    def __init__(self, tokenizer, pretrained_model,max_seq_length=32, image_size =224, patch_size=16, n_channels = 3, embed_size = 512):\n",
        "        \"\"\"\n",
        "        Implements the CNN + T5 (encoder and decoder).\"\n",
        "        Args:            tokenizer: tokenizer used for encoding and decoding\n",
        "        \"\"\"\n",
        "        super(CNNT5, self).__init__()\n",
        "\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.image_size = image_size\n",
        "        self.nc = n_channels\n",
        "        self.embed_size = embed_size\n",
        "        self.patch_cnn = nn.Conv2d(self.nc, self.embed_size, kernel_size=patch_size, stride=patch_size)\n",
        "        self.f=(image_size//patch_size)**2\n",
        "        self.w_embeddings = nn.Embedding(self.f, self.embed_size)\n",
        "\n",
        "        self.pad_token_id = tokenizer.pad_token_id\n",
        "        self.eos_token_id = tokenizer.eos_token_id\n",
        "        self.decoder_t5 = T5ForConditionalGeneration.from_pretrained(pretrained_model)\n",
        "        self.decoder_t5.config.eos_token_id = tokenizer.eos_token_id\n",
        "        self.decoder_t5.config.pad_token_id = tokenizer.pad_token_id\n",
        "        self.decoder_start_token_id = self.decoder_t5.config.decoder_start_token_id\n",
        "\n",
        "    def encoder(self, images):\n",
        "        #print('------ENCODER-------')\n",
        "        #print('images.shape',images.shape)\n",
        "        out = self.patch_cnn(images)\n",
        "        #print('out0.shape',out.shape)\n",
        "        out = out.permute(0, 2, 3, 1) \n",
        "        #print('out1.shape',out.shape)\n",
        "        out  = out .reshape(out.shape[0], -1, self.embed_size)\n",
        "        #print('out2.shape',out.shape)\n",
        "        out+=self.w_embeddings.weight\n",
        "        #print('out3.shape',out.shape)\n",
        "        return out\n",
        "\n",
        "    def forward(self, images = None, targets = None, decoder_ids = None):\n",
        "        #print('++++++DECODER+++++++')\n",
        "        features = self.encoder(images)\n",
        "        #print('features.shape',features.shape)\n",
        "        # condition from Larissa\n",
        "        if decoder_ids is not None:\n",
        "            out = self.decoder_t5(inputs_embeds=features, decoder_input_ids=decoder_ids)\n",
        "        else:\n",
        "            out = self.decoder_t5(inputs_embeds=features, labels=targets)\n",
        "        #print('out.shape',out.shape)\n",
        "        #print('')\n",
        "        #print('')\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PfVRqjbcarw"
      },
      "source": [
        "### Choose a pretained model of T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K09vpi4yKwng"
      },
      "outputs": [],
      "source": [
        "embed_size = 256\n",
        "hidden_size = 100\n",
        "num_layers =1 \n",
        "num_epochs = 250#4\n",
        "print_every = 61\n",
        "save_every = 25 \n",
        "vocab_size = len(data_loader_train.dataset.vocab)\n",
        "total_step = math.ceil( len(data_loader_train.dataset.caption_lengths) / data_loader_train.batch_sampler.batch_size   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt2lQ9Wlb88I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8d197963-be85-446f-a605-bb1e707f6153"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nbyT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "model = CNNT5(tokenizer,'t5-small')\n",
        "#tokenizer_base = T5Tokenizer.from_pretrained('t5-base') \n",
        "#tokenizer_large = T5Tokenizer.from_pretrained('t5-large')\n",
        "#tokenizer_3b = T5Tokenizer.from_pretrained('t5-3b')\n",
        "#tokenizer_11b = T5Tokenizer.from_pretrained('t5-11b')\n",
        "#model_v1_1 = T5ForConditionalGeneration.from_pretrained('google/t5-v1_1-base', return_dict=True)\n",
        "\"\"\"\n",
        "T5v1.1 is an improved version of T5 with some architectural tweaks, and is pre-trained on C4 only without mixing in the supervised tasks\n",
        "\"\"\"\n",
        "#model_byt5 = T5ForConditionalGeneration.from_pretrained('google/byt5-small', return_dict=True)\n",
        "\"\"\"\n",
        "byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IK40zPAKK1t-"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "all_params = list(model.parameters())  + list( encoder.embed.parameters() )\n",
        "optimizer = torch.optim.Adam( params  = all_params , lr = lr  )\n",
        "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## name of model to save"
      ],
      "metadata": {
        "id": "td8sGTSCFuk6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPt3ixPLwIN8"
      },
      "outputs": [],
      "source": [
        "model_save_path = '/content/drive/My Drive/Colab Notebooks/ComputerVision/CNN_T5/image_caption/CVND---Image-Captioning-Project/checkpoint'\n",
        "os.makedirs( model_save_path , exist_ok=True)\n",
        "# Save the params needed to created the model :\n",
        "model_input_params = {'embed_size' : embed_size , \n",
        "                'hidden_size' : hidden_size , \n",
        "                'num_layers' : num_layers,\n",
        "                'lr' : lr ,\n",
        "                'vocab_size' : vocab_size\n",
        "                }\n",
        "\n",
        "with open(  os.path.join(model_save_path , 'model_input_params_17_07_2022.pickle'), 'wb') as handle:\n",
        "    pickle.dump(model_input_params, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqHGpo2DC09A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "766166f1-4888-462a-8a86-4182ca47aa14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.4.5)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2.5.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy1bWlp4ChqO"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU \n",
        "import pandas as pd\n",
        "bleu = BLEU()\n",
        "bleu_score = []\n",
        "losses_training = []\n",
        "val_bleu_1=[]\n",
        "val_bleu_4=[]\n",
        "Epochs=[]\n",
        "steps=[]\n",
        "save_path = '/content/drive/My Drive/Colab Notebooks/ComputerVision/CNN_T5/image_caption/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad9kL_Xig9yo"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_step,batch_size"
      ],
      "metadata": {
        "id": "KMxG1ip0r4ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85627a19-adaf-4a8d-a5d1-643538dd4ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(183, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder0=\"/content/drive/MyDrive/IA025/proy/experiments/mix1/\"\n",
        "df_test1=pd.read_csv(folder0+'data_test1.csv')#\n",
        "df_images_captionstest=df_test1.copy()"
      ],
      "metadata": {
        "id": "32Ze7RaDQWTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys \n",
        "for e in range(num_epochs):\n",
        "  for step in range(total_step):\n",
        "    indices = data_loader_train.dataset.get_train_indices()\n",
        "    new_sampler = SubsetRandomSampler( indices )# check!!!!\n",
        "    data_loader_train.batch_sampler.sampler = new_sampler    \n",
        "    images,captions = next(iter(data_loader_train))    \n",
        "    images , captions = images.to(device) , captions.to(device)\n",
        "    model = model.to(device)\n",
        "    model.zero_grad()\n",
        "    output = model (images , captions )\n",
        "    logits = output.logits\n",
        "    train_preds = torch.argmax(logits, dim=-1)\n",
        "    preds_sentences = tokenizer.batch_decode(train_preds,  skip_special_tokens=False) #returns a list\n",
        "\n",
        "    labels = captions.clone()\n",
        "    text=tokenizer.batch_decode(labels,  skip_special_tokens=False)\n",
        "    bleu_result = bleu.corpus_score(preds_sentences, text)\n",
        "    #bleu_score.append(bleu_result.score)\n",
        "    #val_bleu_1.append(bleu_result.precisions[0])\n",
        "    #val_bleu_4.append(bleu_result.precisions[3])\n",
        "    loss = output.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #losses_training.append(loss.item()) \n",
        "    #Epochs.append( e+1) \n",
        "    #steps.append(step) \n",
        "\n",
        "    \n",
        "    #stat_vals = 'Epochs [%d/%d] Step [%d/%d] Loss [%.4f] bleu [%.4f] bleu_1 [%.4f] bleu_4 [%.4f]' %( e+1,num_epochs,step,total_step,losses_training[-1],bleu_score[-1],val_bleu_1[-1],val_bleu_4[-1] )\n",
        "    stat_vals = 'Epochs [%d/%d] Step [%d/%d] Loss [%.4f] bleu [%.4f] bleu_1 [%.4f] bleu_4 [%.4f]' %( e+1,num_epochs,step,total_step,loss.item(),bleu_result.score,bleu_result.precisions[0],bleu_result.precisions[3] )\n",
        "    if step % print_every == 0 :\n",
        "      print(stat_vals)\n",
        "      sys.stdout.flush()\n",
        "  if e % 10 == 0:\n",
        "    torch.save( model.state_dict() ,  os.path.join( model_save_path , 'modeldata_{}.pkl'.format(e+1) ) )\n",
        "\n",
        "  bleu_score.append(bleu_result.score)\n",
        "  val_bleu_1.append(bleu_result.precisions[0])\n",
        "  val_bleu_4.append(bleu_result.precisions[3])\n",
        "  losses_training.append(loss.item()) \n",
        "  Epochs.append( e+1) \n",
        "  steps.append(step) \n",
        "\n",
        "  data = {'Epochs':Epochs,\n",
        "          'num_epochs':[num_epochs for k in range(len(Epochs))] ,\n",
        "          'step':steps,\n",
        "          '[total_step':[total_step for k in range(len(Epochs))],\n",
        "          'losses':losses_training,\n",
        "          'bleu_score':bleu_score,\n",
        "          'bleu1':val_bleu_1,\n",
        "          'bleu4':val_bleu_4}\n",
        "  #load data into a DataFrame object:\n",
        "  df_metrics = pd.DataFrame(data)\n",
        "  df_metrics.to_csv(save_path+'DF_metrics_exp1.csv',index=False)\n",
        "  #print('metrics saved')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdCjHUz6VhfK",
        "outputId": "14eee8da-6dc9-4c08-f755-d1a37e37ff2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs [1/250] Step [0/183] Loss [9.4545] bleu [1.3666] bleu_1 [3.5714] bleu_4 [0.7812]\n",
            "Epochs [1/250] Step [61/183] Loss [3.0821] bleu [0.2440] bleu_1 [4.7468] bleu_4 [0.0488]\n",
            "Epochs [1/250] Step [122/183] Loss [2.2112] bleu [0.2442] bleu_1 [5.4711] bleu_4 [0.0465]\n",
            "Epochs [2/250] Step [0/183] Loss [2.0820] bleu [0.3976] bleu_1 [6.0914] bleu_4 [0.0912]\n",
            "Epochs [2/250] Step [61/183] Loss [1.8061] bleu [0.2345] bleu_1 [5.0445] bleu_4 [0.0451]\n",
            "Epochs [2/250] Step [122/183] Loss [1.8518] bleu [0.2571] bleu_1 [5.9748] bleu_4 [0.0484]\n",
            "Epochs [3/250] Step [0/183] Loss [1.8504] bleu [0.2194] bleu_1 [4.7486] bleu_4 [0.0419]\n",
            "Epochs [3/250] Step [61/183] Loss [1.7191] bleu [0.2510] bleu_1 [5.6075] bleu_4 [0.0479]\n",
            "Epochs [3/250] Step [122/183] Loss [1.6826] bleu [0.2557] bleu_1 [4.9505] bleu_4 [0.0514]\n",
            "Epochs [4/250] Step [0/183] Loss [1.5322] bleu [0.2325] bleu_1 [4.3077] bleu_4 [0.0472]\n",
            "Epochs [4/250] Step [61/183] Loss [1.6610] bleu [0.2510] bleu_1 [5.6075] bleu_4 [0.0479]\n",
            "Epochs [4/250] Step [122/183] Loss [1.3824] bleu [0.2399] bleu_1 [4.4304] bleu_4 [0.0488]\n",
            "Epochs [5/250] Step [0/183] Loss [1.3671] bleu [0.2295] bleu_1 [4.4910] bleu_4 [0.0456]\n",
            "Epochs [5/250] Step [61/183] Loss [1.4694] bleu [0.2314] bleu_1 [4.0498] bleu_4 [0.0479]\n",
            "Epochs [5/250] Step [122/183] Loss [1.1630] bleu [0.2544] bleu_1 [5.9190] bleu_4 [0.0479]\n",
            "Epochs [6/250] Step [0/183] Loss [1.2450] bleu [0.2591] bleu_1 [6.5015] bleu_4 [0.0475]\n",
            "Epochs [6/250] Step [61/183] Loss [1.1120] bleu [0.2340] bleu_1 [4.8048] bleu_4 [0.0458]\n",
            "Epochs [6/250] Step [122/183] Loss [1.1624] bleu [0.2398] bleu_1 [4.6729] bleu_4 [0.0479]\n",
            "Epochs [7/250] Step [0/183] Loss [1.0513] bleu [0.2536] bleu_1 [5.6604] bleu_4 [0.0484]\n",
            "Epochs [7/250] Step [61/183] Loss [1.2736] bleu [0.2277] bleu_1 [5.9946] bleu_4 [0.0407]\n",
            "Epochs [7/250] Step [122/183] Loss [1.2023] bleu [0.2491] bleu_1 [6.0423] bleu_4 [0.0461]\n",
            "Epochs [8/250] Step [0/183] Loss [1.0829] bleu [0.2542] bleu_1 [6.1538] bleu_4 [0.0472]\n",
            "Epochs [8/250] Step [61/183] Loss [1.0273] bleu [0.2907] bleu_1 [6.1151] bleu_4 [0.0573]\n",
            "Epochs [8/250] Step [122/183] Loss [0.9604] bleu [0.2505] bleu_1 [6.3063] bleu_4 [0.0458]\n",
            "Epochs [9/250] Step [0/183] Loss [1.0218] bleu [0.2404] bleu_1 [4.9231] bleu_4 [0.0472]\n",
            "Epochs [9/250] Step [61/183] Loss [1.0189] bleu [0.2609] bleu_1 [6.5421] bleu_4 [0.0479]\n",
            "Epochs [9/250] Step [122/183] Loss [0.9667] bleu [0.2484] bleu_1 [5.7927] bleu_4 [0.0466]\n",
            "Epochs [10/250] Step [0/183] Loss [0.9492] bleu [0.2396] bleu_1 [4.9080] bleu_4 [0.0470]\n",
            "Epochs [10/250] Step [61/183] Loss [0.9577] bleu [0.2474] bleu_1 [6.0060] bleu_4 [0.0458]\n",
            "Epochs [10/250] Step [122/183] Loss [1.0388] bleu [0.2365] bleu_1 [4.6154] bleu_4 [0.0472]\n",
            "Epochs [11/250] Step [0/183] Loss [0.9131] bleu [0.1803] bleu_1 [3.4146] bleu_4 [0.0357]\n",
            "Epochs [11/250] Step [61/183] Loss [1.0150] bleu [0.2561] bleu_1 [5.2117] bleu_4 [0.0506]\n",
            "Epochs [11/250] Step [122/183] Loss [1.0129] bleu [0.2412] bleu_1 [4.9383] bleu_4 [0.0473]\n",
            "Epochs [12/250] Step [0/183] Loss [0.8737] bleu [0.1790] bleu_1 [1.9886] bleu_4 [0.0428]\n",
            "Epochs [12/250] Step [61/183] Loss [0.9030] bleu [0.2408] bleu_1 [5.1672] bleu_4 [0.0465]\n",
            "Epochs [12/250] Step [122/183] Loss [0.9526] bleu [0.2404] bleu_1 [4.9231] bleu_4 [0.0472]\n",
            "Epochs [13/250] Step [0/183] Loss [0.9832] bleu [0.2178] bleu_1 [3.6036] bleu_4 [0.0458]\n",
            "Epochs [13/250] Step [61/183] Loss [1.0046] bleu [0.2419] bleu_1 [5.6548] bleu_4 [0.0453]\n",
            "Epochs [13/250] Step [122/183] Loss [0.8371] bleu [0.2492] bleu_1 [5.8104] bleu_4 [0.0468]\n",
            "Epochs [14/250] Step [0/183] Loss [0.8945] bleu [0.2398] bleu_1 [4.6729] bleu_4 [0.0479]\n",
            "Epochs [14/250] Step [61/183] Loss [0.9101] bleu [0.2598] bleu_1 [6.0317] bleu_4 [0.0490]\n",
            "Epochs [14/250] Step [122/183] Loss [0.9199] bleu [0.2488] bleu_1 [6.2687] bleu_4 [0.0455]\n",
            "Epochs [15/250] Step [0/183] Loss [0.8727] bleu [0.2259] bleu_1 [3.9634] bleu_4 [0.0466]\n",
            "Epochs [15/250] Step [61/183] Loss [0.9366] bleu [0.2451] bleu_1 [5.4878] bleu_4 [0.0466]\n",
            "Epochs [15/250] Step [122/183] Loss [0.8809] bleu [0.2357] bleu_1 [4.3614] bleu_4 [0.0479]\n",
            "Epochs [16/250] Step [0/183] Loss [0.9276] bleu [0.2365] bleu_1 [4.6154] bleu_4 [0.0472]\n",
            "Epochs [16/250] Step [61/183] Loss [0.8501] bleu [0.2554] bleu_1 [5.6962] bleu_4 [0.0488]\n",
            "Epochs [16/250] Step [122/183] Loss [0.8993] bleu [0.1949] bleu_1 [2.6087] bleu_4 [0.0439]\n",
            "Epochs [17/250] Step [0/183] Loss [0.8627] bleu [0.2382] bleu_1 [4.4025] bleu_4 [0.0484]\n",
            "Epochs [17/250] Step [61/183] Loss [0.7866] bleu [0.3060] bleu_1 [6.6914] bleu_4 [0.0598]\n",
            "Epochs [17/250] Step [122/183] Loss [0.7809] bleu [0.2468] bleu_1 [6.4516] bleu_4 [0.0445]\n",
            "Epochs [18/250] Step [0/183] Loss [0.8015] bleu [0.2182] bleu_1 [3.3742] bleu_4 [0.0470]\n",
            "Epochs [18/250] Step [61/183] Loss [0.9719] bleu [0.1830] bleu_1 [2.0290] bleu_4 [0.0439]\n",
            "Epochs [18/250] Step [122/183] Loss [0.8021] bleu [0.2669] bleu_1 [7.4074] bleu_4 [0.0473]\n",
            "Epochs [19/250] Step [0/183] Loss [0.7142] bleu [0.2547] bleu_1 [6.4024] bleu_4 [0.0466]\n",
            "Epochs [19/250] Step [61/183] Loss [0.9084] bleu [0.2451] bleu_1 [5.4878] bleu_4 [0.0466]\n",
            "Epochs [19/250] Step [122/183] Loss [0.8477] bleu [0.2527] bleu_1 [5.8824] bleu_4 [0.0475]\n",
            "Epochs [20/250] Step [0/183] Loss [0.7013] bleu [0.2536] bleu_1 [5.4140] bleu_4 [0.0492]\n",
            "Epochs [20/250] Step [61/183] Loss [0.8278] bleu [0.1649] bleu_1 [1.1976] bleu_4 [0.0456]\n",
            "Epochs [20/250] Step [122/183] Loss [0.7747] bleu [0.2420] bleu_1 [4.9536] bleu_4 [0.0475]\n",
            "Epochs [21/250] Step [0/183] Loss [0.8001] bleu [0.2374] bleu_1 [4.6296] bleu_4 [0.0473]\n",
            "Epochs [21/250] Step [61/183] Loss [0.6613] bleu [0.2483] bleu_1 [6.0241] bleu_4 [0.0460]\n",
            "Epochs [21/250] Step [122/183] Loss [0.8717] bleu [0.1577] bleu_1 [2.5057] bleu_4 [0.0330]\n",
            "Epochs [22/250] Step [0/183] Loss [0.6882] bleu [0.2365] bleu_1 [4.3750] bleu_4 [0.0481]\n",
            "Epochs [22/250] Step [61/183] Loss [0.7291] bleu [0.2364] bleu_1 [4.8485] bleu_4 [0.0463]\n",
            "Epochs [22/250] Step [122/183] Loss [0.7835] bleu [0.2467] bleu_1 [5.7576] bleu_4 [0.0463]\n",
            "Epochs [23/250] Step [0/183] Loss [0.7521] bleu [0.2372] bleu_1 [4.8632] bleu_4 [0.0465]\n",
            "Epochs [23/250] Step [61/183] Loss [0.7411] bleu [0.2476] bleu_1 [5.5385] bleu_4 [0.0472]\n",
            "Epochs [23/250] Step [122/183] Loss [0.6651] bleu [0.2562] bleu_1 [5.9561] bleu_4 [0.0483]\n",
            "Epochs [24/250] Step [0/183] Loss [0.6032] bleu [0.2390] bleu_1 [4.4164] bleu_4 [0.0486]\n",
            "Epochs [24/250] Step [61/183] Loss [0.6140] bleu [0.2406] bleu_1 [6.7416] bleu_4 [0.0422]\n",
            "Epochs [24/250] Step [122/183] Loss [0.6003] bleu [0.2357] bleu_1 [4.6012] bleu_4 [0.0470]\n",
            "Epochs [25/250] Step [0/183] Loss [0.6121] bleu [0.2424] bleu_1 [5.1988] bleu_4 [0.0468]\n",
            "Epochs [25/250] Step [61/183] Loss [0.5729] bleu [0.2596] bleu_1 [5.0167] bleu_4 [0.0523]\n",
            "Epochs [25/250] Step [122/183] Loss [0.5133] bleu [0.2424] bleu_1 [5.1988] bleu_4 [0.0468]\n",
            "Epochs [26/250] Step [0/183] Loss [0.6458] bleu [0.2968] bleu_1 [7.3427] bleu_4 [0.0553]\n",
            "Epochs [26/250] Step [61/183] Loss [0.8339] bleu [0.2363] bleu_1 [5.3097] bleu_4 [0.0448]\n",
            "Epochs [26/250] Step [122/183] Loss [0.6593] bleu [0.2402] bleu_1 [5.3892] bleu_4 [0.0456]\n",
            "Epochs [27/250] Step [0/183] Loss [0.5454] bleu [0.2561] bleu_1 [5.2117] bleu_4 [0.0506]\n",
            "Epochs [27/250] Step [61/183] Loss [0.5973] bleu [0.2073] bleu_1 [4.7120] bleu_4 [0.0388]\n",
            "Epochs [27/250] Step [122/183] Loss [0.6434] bleu [0.2420] bleu_1 [4.9536] bleu_4 [0.0475]\n",
            "Epochs [28/250] Step [0/183] Loss [0.4976] bleu [0.2298] bleu_1 [4.0248] bleu_4 [0.0475]\n",
            "Epochs [28/250] Step [61/183] Loss [0.5031] bleu [0.2147] bleu_1 [4.8649] bleu_4 [0.0403]\n",
            "Epochs [28/250] Step [122/183] Loss [0.6419] bleu [0.2399] bleu_1 [6.2857] bleu_4 [0.0431]\n",
            "Epochs [29/250] Step [0/183] Loss [0.5165] bleu [0.2582] bleu_1 [6.4815] bleu_4 [0.0473]\n",
            "Epochs [29/250] Step [61/183] Loss [0.5165] bleu [0.2222] bleu_1 [3.6697] bleu_4 [0.0468]\n",
            "Epochs [29/250] Step [122/183] Loss [0.4356] bleu [0.2301] bleu_1 [4.2683] bleu_4 [0.0466]\n",
            "Epochs [30/250] Step [0/183] Loss [0.4575] bleu [0.2733] bleu_1 [6.3123] bleu_4 [0.0519]\n",
            "Epochs [30/250] Step [61/183] Loss [0.6033] bleu [0.2577] bleu_1 [6.2305] bleu_4 [0.0479]\n",
            "Epochs [30/250] Step [122/183] Loss [0.6565] bleu [0.1639] bleu_1 [3.2967] bleu_4 [0.0316]\n",
            "Epochs [31/250] Step [0/183] Loss [0.5480] bleu [0.2222] bleu_1 [5.2342] bleu_4 [0.0413]\n",
            "Epochs [31/250] Step [61/183] Loss [0.4992] bleu [0.2294] bleu_1 [4.2553] bleu_4 [0.0465]\n",
            "Epochs [31/250] Step [122/183] Loss [0.6609] bleu [0.2592] bleu_1 [4.4828] bleu_4 [0.0543]\n",
            "Epochs [32/250] Step [0/183] Loss [0.4339] bleu [0.2489] bleu_1 [5.0794] bleu_4 [0.0490]\n",
            "Epochs [32/250] Step [61/183] Loss [0.3694] bleu [0.2055] bleu_1 [4.4737] bleu_4 [0.0391]\n",
            "Epochs [32/250] Step [122/183] Loss [0.3714] bleu [0.2511] bleu_1 [4.8701] bleu_4 [0.0504]\n",
            "Epochs [33/250] Step [0/183] Loss [0.3358] bleu [0.2398] bleu_1 [4.6729] bleu_4 [0.0479]\n",
            "Epochs [33/250] Step [61/183] Loss [0.3663] bleu [0.2594] bleu_1 [6.7485] bleu_4 [0.0470]\n",
            "Epochs [33/250] Step [122/183] Loss [0.4524] bleu [0.2451] bleu_1 [5.4878] bleu_4 [0.0466]\n",
            "Epochs [34/250] Step [0/183] Loss [0.3607] bleu [0.2554] bleu_1 [5.4487] bleu_4 [0.0496]\n",
            "Epochs [34/250] Step [61/183] Loss [0.3727] bleu [0.2518] bleu_1 [5.3797] bleu_4 [0.0488]\n",
            "Epochs [34/250] Step [122/183] Loss [0.4557] bleu [0.1570] bleu_1 [3.3264] bleu_4 [0.0297]\n",
            "Epochs [35/250] Step [0/183] Loss [0.3233] bleu [0.2768] bleu_1 [6.6445] bleu_4 [0.0519]\n",
            "Epochs [35/250] Step [61/183] Loss [0.4188] bleu [0.3436] bleu_1 [8.6614] bleu_4 [0.0644]\n",
            "Epochs [35/250] Step [122/183] Loss [0.3426] bleu [0.3574] bleu_1 [7.6596] bleu_4 [0.0714]\n",
            "Epochs [36/250] Step [0/183] Loss [0.2993] bleu [0.2561] bleu_1 [5.2117] bleu_4 [0.0506]\n",
            "Epochs [36/250] Step [61/183] Loss [0.3789] bleu [0.2615] bleu_1 [5.0505] bleu_4 [0.0527]\n",
            "Epochs [36/250] Step [122/183] Loss [0.3153] bleu [0.2491] bleu_1 [6.0423] bleu_4 [0.0461]\n",
            "Epochs [37/250] Step [0/183] Loss [0.3365] bleu [0.2599] bleu_1 [5.2805] bleu_4 [0.0514]\n",
            "Epochs [37/250] Step [61/183] Loss [0.3247] bleu [0.2941] bleu_1 [6.7376] bleu_4 [0.0563]\n",
            "Epochs [37/250] Step [122/183] Loss [0.2953] bleu [0.2610] bleu_1 [5.5556] bleu_4 [0.0508]\n",
            "Epochs [38/250] Step [0/183] Loss [0.3048] bleu [0.2610] bleu_1 [5.5556] bleu_4 [0.0508]\n",
            "Epochs [38/250] Step [61/183] Loss [0.3701] bleu [0.2638] bleu_1 [5.8632] bleu_4 [0.0506]\n",
            "Epochs [38/250] Step [122/183] Loss [0.3159] bleu [0.2405] bleu_1 [4.1935] bleu_4 [0.0500]\n",
            "Epochs [39/250] Step [0/183] Loss [0.3936] bleu [0.2292] bleu_1 [5.6022] bleu_4 [0.0421]\n",
            "Epochs [39/250] Step [61/183] Loss [0.2892] bleu [0.2161] bleu_1 [5.7143] bleu_4 [0.0385]\n",
            "Epochs [39/250] Step [122/183] Loss [0.5490] bleu [0.2821] bleu_1 [6.7568] bleu_4 [0.0530]\n",
            "Epochs [40/250] Step [0/183] Loss [0.2756] bleu [0.2484] bleu_1 [5.7927] bleu_4 [0.0466]\n",
            "Epochs [40/250] Step [61/183] Loss [0.2226] bleu [0.3342] bleu_1 [7.5397] bleu_4 [0.0651]\n",
            "Epochs [40/250] Step [122/183] Loss [0.3027] bleu [0.2922] bleu_1 [5.8608] bleu_4 [0.0587]\n",
            "Epochs [41/250] Step [0/183] Loss [0.2899] bleu [0.2450] bleu_1 [4.5161] bleu_4 [0.0500]\n",
            "Epochs [41/250] Step [61/183] Loss [0.2817] bleu [0.2688] bleu_1 [5.7047] bleu_4 [0.0525]\n",
            "Epochs [41/250] Step [122/183] Loss [0.2377] bleu [0.2875] bleu_1 [6.8729] bleu_4 [0.0541]\n",
            "Epochs [42/250] Step [0/183] Loss [0.2051] bleu [0.2838] bleu_1 [5.1471] bleu_4 [0.0590]\n",
            "Epochs [42/250] Step [61/183] Loss [0.3623] bleu [0.2437] bleu_1 [6.3768] bleu_4 [0.0439]\n",
            "Epochs [42/250] Step [122/183] Loss [0.3379] bleu [0.1946] bleu_1 [4.0609] bleu_4 [0.0374]\n",
            "Epochs [43/250] Step [0/183] Loss [0.2835] bleu [0.2697] bleu_1 [5.1903] bleu_4 [0.0546]\n",
            "Epochs [43/250] Step [61/183] Loss [0.2610] bleu [0.2341] bleu_1 [3.8462] bleu_4 [0.0496]\n",
            "Epochs [43/250] Step [122/183] Loss [0.2098] bleu [0.2610] bleu_1 [5.5556] bleu_4 [0.0508]\n",
            "Epochs [44/250] Step [0/183] Loss [0.2788] bleu [0.3003] bleu_1 [7.4205] bleu_4 [0.0561]\n",
            "Epochs [44/250] Step [61/183] Loss [0.1925] bleu [0.2572] bleu_1 [5.4839] bleu_4 [0.0500]\n",
            "Epochs [44/250] Step [122/183] Loss [0.2059] bleu [0.2968] bleu_1 [7.3427] bleu_4 [0.0553]\n",
            "Epochs [45/250] Step [0/183] Loss [0.2260] bleu [0.2459] bleu_1 [5.7402] bleu_4 [0.0461]\n",
            "Epochs [45/250] Step [61/183] Loss [0.1935] bleu [0.2493] bleu_1 [5.5728] bleu_4 [0.0475]\n",
            "Epochs [45/250] Step [122/183] Loss [0.2885] bleu [0.1244] bleu_1 [2.0073] bleu_4 [0.0256]\n",
            "Epochs [46/250] Step [0/183] Loss [0.2449] bleu [0.2728] bleu_1 [6.5574] bleu_4 [0.0510]\n",
            "Epochs [46/250] Step [61/183] Loss [0.2197] bleu [0.2372] bleu_1 [5.5556] bleu_4 [0.0443]\n",
            "Epochs [46/250] Step [122/183] Loss [0.2728] bleu [0.2458] bleu_1 [4.7771] bleu_4 [0.0492]\n",
            "Epochs [47/250] Step [0/183] Loss [0.1883] bleu [0.2840] bleu_1 [5.7143] bleu_4 [0.0568]\n",
            "Epochs [47/250] Step [61/183] Loss [0.2310] bleu [0.2599] bleu_1 [5.2805] bleu_4 [0.0514]\n",
            "Epochs [47/250] Step [122/183] Loss [0.2029] bleu [0.2747] bleu_1 [6.0811] bleu_4 [0.0530]\n",
            "Epochs [48/250] Step [0/183] Loss [0.1807] bleu [0.2853] bleu_1 [6.8259] bleu_4 [0.0536]\n",
            "Epochs [48/250] Step [61/183] Loss [0.1612] bleu [0.2057] bleu_1 [4.0650] bleu_4 [0.0405]\n",
            "Epochs [48/250] Step [122/183] Loss [0.2089] bleu [0.2743] bleu_1 [6.3333] bleu_4 [0.0521]\n",
            "Epochs [49/250] Step [0/183] Loss [0.2243] bleu [0.2407] bleu_1 [4.4444] bleu_4 [0.0490]\n",
            "Epochs [49/250] Step [61/183] Loss [0.2222] bleu [0.2867] bleu_1 [6.3158] bleu_4 [0.0556]\n",
            "Epochs [49/250] Step [122/183] Loss [0.3066] bleu [0.2648] bleu_1 [5.6291] bleu_4 [0.0517]\n",
            "Epochs [50/250] Step [0/183] Loss [0.2056] bleu [0.2782] bleu_1 [6.9307] bleu_4 [0.0514]\n",
            "Epochs [50/250] Step [61/183] Loss [0.2070] bleu [0.2506] bleu_1 [5.1118] bleu_4 [0.0494]\n",
            "Epochs [50/250] Step [122/183] Loss [0.2956] bleu [0.2638] bleu_1 [5.8632] bleu_4 [0.0506]\n",
            "Epochs [51/250] Step [0/183] Loss [0.1828] bleu [0.3077] bleu_1 [7.5812] bleu_4 [0.0576]\n",
            "Epochs [51/250] Step [61/183] Loss [0.2053] bleu [0.2758] bleu_1 [6.1017] bleu_4 [0.0532]\n",
            "Epochs [51/250] Step [122/183] Loss [0.2322] bleu [0.2867] bleu_1 [6.3158] bleu_4 [0.0556]\n",
            "Epochs [52/250] Step [0/183] Loss [0.2397] bleu [0.2906] bleu_1 [6.6667] bleu_4 [0.0556]\n",
            "Epochs [52/250] Step [61/183] Loss [0.1959] bleu [0.2664] bleu_1 [4.5936] bleu_4 [0.0561]\n",
            "Epochs [52/250] Step [122/183] Loss [0.3040] bleu [0.1282] bleu_1 [1.9194] bleu_4 [0.0271]\n",
            "Epochs [53/250] Step [0/183] Loss [0.2028] bleu [0.2505] bleu_1 [3.8194] bleu_4 [0.0548]\n",
            "Epochs [53/250] Step [61/183] Loss [0.1852] bleu [0.2979] bleu_1 [7.0922] bleu_4 [0.0563]\n",
            "Epochs [53/250] Step [122/183] Loss [0.1976] bleu [0.3287] bleu_1 [8.3333] bleu_4 [0.0613]\n",
            "Epochs [54/250] Step [0/183] Loss [0.2089] bleu [0.2628] bleu_1 [5.8442] bleu_4 [0.0504]\n",
            "Epochs [54/250] Step [61/183] Loss [0.2053] bleu [0.2648] bleu_1 [5.3691] bleu_4 [0.0525]\n",
            "Epochs [54/250] Step [122/183] Loss [0.1897] bleu [0.2515] bleu_1 [5.1282] bleu_4 [0.0496]\n",
            "Epochs [55/250] Step [0/183] Loss [0.2752] bleu [0.2501] bleu_1 [5.8282] bleu_4 [0.0470]\n",
            "Epochs [55/250] Step [61/183] Loss [0.1917] bleu [0.2573] bleu_1 [6.4615] bleu_4 [0.0472]\n",
            "Epochs [55/250] Step [122/183] Loss [0.1816] bleu [0.2984] bleu_1 [5.9701] bleu_4 [0.0601]\n",
            "Epochs [56/250] Step [0/183] Loss [0.1830] bleu [0.3280] bleu_1 [6.8000] bleu_4 [0.0658]\n",
            "Epochs [56/250] Step [61/183] Loss [0.1671] bleu [0.2591] bleu_1 [5.5195] bleu_4 [0.0504]\n",
            "Epochs [56/250] Step [122/183] Loss [0.1998] bleu [0.2349] bleu_1 [3.8585] bleu_4 [0.0498]\n",
            "Epochs [57/250] Step [0/183] Loss [0.2139] bleu [0.2591] bleu_1 [5.7692] bleu_4 [0.0496]\n",
            "Epochs [57/250] Step [61/183] Loss [0.2571] bleu [0.2638] bleu_1 [5.3512] bleu_4 [0.0523]\n",
            "Epochs [57/250] Step [122/183] Loss [0.2510] bleu [0.1918] bleu_1 [2.5714] bleu_4 [0.0431]\n",
            "Epochs [58/250] Step [0/183] Loss [0.2527] bleu [0.3026] bleu_1 [7.7465] bleu_4 [0.0558]\n",
            "Epochs [58/250] Step [61/183] Loss [0.1812] bleu [0.2586] bleu_1 [6.2500] bleu_4 [0.0481]\n",
            "Epochs [58/250] Step [122/183] Loss [0.1883] bleu [0.2828] bleu_1 [5.6940] bleu_4 [0.0566]\n",
            "Epochs [59/250] Step [0/183] Loss [0.1895] bleu [0.2822] bleu_1 [6.2284] bleu_4 [0.0546]\n",
            "Epochs [59/250] Step [61/183] Loss [0.1637] bleu [0.2985] bleu_1 [6.5455] bleu_4 [0.0581]\n",
            "Epochs [59/250] Step [122/183] Loss [0.1938] bleu [0.2811] bleu_1 [6.2069] bleu_4 [0.0543]\n",
            "Epochs [60/250] Step [0/183] Loss [0.2030] bleu [0.1946] bleu_1 [4.6341] bleu_4 [0.0357]\n",
            "Epochs [60/250] Step [61/183] Loss [0.1473] bleu [0.3250] bleu_1 [6.7460] bleu_4 [0.0651]\n",
            "Epochs [60/250] Step [122/183] Loss [0.2271] bleu [0.2965] bleu_1 [6.7857] bleu_4 [0.0568]\n",
            "Epochs [61/250] Step [0/183] Loss [0.1611] bleu [0.2910] bleu_1 [5.8394] bleu_4 [0.0584]\n",
            "Epochs [61/250] Step [61/183] Loss [0.2296] bleu [0.2790] bleu_1 [6.1644] bleu_4 [0.0539]\n",
            "Epochs [61/250] Step [122/183] Loss [0.1533] bleu [0.2920] bleu_1 [6.9686] bleu_4 [0.0551]\n",
            "Epochs [62/250] Step [0/183] Loss [0.1500] bleu [0.2580] bleu_1 [5.2459] bleu_4 [0.0510]\n",
            "Epochs [62/250] Step [61/183] Loss [0.1531] bleu [0.2706] bleu_1 [6.0000] bleu_4 [0.0521]\n",
            "Epochs [62/250] Step [122/183] Loss [0.1621] bleu [0.2883] bleu_1 [6.0714] bleu_4 [0.0568]\n",
            "Epochs [63/250] Step [0/183] Loss [0.1544] bleu [0.3040] bleu_1 [7.5000] bleu_4 [0.0568]\n",
            "Epochs [63/250] Step [61/183] Loss [0.1624] bleu [0.2886] bleu_1 [5.7971] bleu_4 [0.0579]\n",
            "Epochs [63/250] Step [122/183] Loss [0.1548] bleu [0.2955] bleu_1 [6.2044] bleu_4 [0.0584]\n",
            "Epochs [64/250] Step [0/183] Loss [0.1807] bleu [0.2913] bleu_1 [6.4057] bleu_4 [0.0566]\n",
            "Epochs [64/250] Step [61/183] Loss [0.1710] bleu [0.2474] bleu_1 [5.2960] bleu_4 [0.0479]\n",
            "Epochs [64/250] Step [122/183] Loss [0.1357] bleu [0.2751] bleu_1 [5.5556] bleu_4 [0.0548]\n",
            "Epochs [65/250] Step [0/183] Loss [0.1647] bleu [0.2935] bleu_1 [5.8824] bleu_4 [0.0590]\n",
            "Epochs [65/250] Step [61/183] Loss [0.1654] bleu [0.2622] bleu_1 [6.3291] bleu_4 [0.0488]\n",
            "Epochs [65/250] Step [122/183] Loss [0.1553] bleu [0.2668] bleu_1 [5.4054] bleu_4 [0.0530]\n",
            "Epochs [66/250] Step [0/183] Loss [0.1853] bleu [0.2839] bleu_1 [6.5292] bleu_4 [0.0541]\n",
            "Epochs [66/250] Step [61/183] Loss [0.1982] bleu [0.2177] bleu_1 [2.6316] bleu_4 [0.0512]\n",
            "Epochs [66/250] Step [122/183] Loss [0.3033] bleu [0.2047] bleu_1 [2.7273] bleu_4 [0.0463]\n",
            "Epochs [67/250] Step [0/183] Loss [0.2754] bleu [0.2748] bleu_1 [6.6007] bleu_4 [0.0514]\n",
            "Epochs [67/250] Step [61/183] Loss [0.2735] bleu [0.2876] bleu_1 [5.4945] bleu_4 [0.0587]\n",
            "Epochs [67/250] Step [122/183] Loss [0.2505] bleu [0.2706] bleu_1 [6.0000] bleu_4 [0.0521]\n",
            "Epochs [68/250] Step [0/183] Loss [0.1958] bleu [0.2645] bleu_1 [5.1020] bleu_4 [0.0534]\n",
            "Epochs [68/250] Step [61/183] Loss [0.2430] bleu [0.2789] bleu_1 [6.6890] bleu_4 [0.0523]\n",
            "Epochs [68/250] Step [122/183] Loss [0.2354] bleu [0.3142] bleu_1 [7.1429] bleu_4 [0.0607]\n",
            "Epochs [69/250] Step [0/183] Loss [0.1546] bleu [0.2922] bleu_1 [7.5085] bleu_4 [0.0536]\n",
            "Epochs [69/250] Step [61/183] Loss [0.2214] bleu [0.2292] bleu_1 [4.9419] bleu_4 [0.0440]\n",
            "Epochs [69/250] Step [122/183] Loss [0.1973] bleu [0.2629] bleu_1 [5.5921] bleu_4 [0.0512]\n",
            "Epochs [70/250] Step [0/183] Loss [0.1937] bleu [0.3014] bleu_1 [7.7193] bleu_4 [0.0556]\n",
            "Epochs [70/250] Step [61/183] Loss [0.1642] bleu [0.2424] bleu_1 [6.1224] bleu_4 [0.0442]\n",
            "Epochs [70/250] Step [122/183] Loss [0.1847] bleu [0.3210] bleu_1 [7.5758] bleu_4 [0.0613]\n",
            "Epochs [71/250] Step [0/183] Loss [0.1797] bleu [0.2750] bleu_1 [5.8219] bleu_4 [0.0539]\n",
            "Epochs [71/250] Step [61/183] Loss [0.1678] bleu [0.3235] bleu_1 [7.9245] bleu_4 [0.0610]\n",
            "Epochs [71/250] Step [122/183] Loss [0.2569] bleu [0.1879] bleu_1 [3.1496] bleu_4 [0.0389]\n",
            "Epochs [72/250] Step [0/183] Loss [0.1418] bleu [0.3064] bleu_1 [7.5540] bleu_4 [0.0573]\n",
            "Epochs [72/250] Step [61/183] Loss [0.1478] bleu [0.2975] bleu_1 [5.3640] bleu_4 [0.0622]\n",
            "Epochs [72/250] Step [122/183] Loss [0.1413] bleu [0.2900] bleu_1 [5.5351] bleu_4 [0.0592]\n",
            "Epochs [73/250] Step [0/183] Loss [0.1550] bleu [0.2762] bleu_1 [5.5749] bleu_4 [0.0551]\n",
            "Epochs [73/250] Step [61/183] Loss [0.1501] bleu [0.3268] bleu_1 [7.3930] bleu_4 [0.0635]\n",
            "Epochs [73/250] Step [122/183] Loss [0.1529] bleu [0.2883] bleu_1 [6.6202] bleu_4 [0.0551]\n",
            "Epochs [74/250] Step [0/183] Loss [0.1520] bleu [0.2822] bleu_1 [6.2284] bleu_4 [0.0546]\n",
            "Epochs [74/250] Step [61/183] Loss [0.1487] bleu [0.2551] bleu_1 [6.1728] bleu_4 [0.0473]\n",
            "Epochs [74/250] Step [122/183] Loss [0.1337] bleu [0.2947] bleu_1 [5.9041] bleu_4 [0.0592]\n",
            "Epochs [75/250] Step [0/183] Loss [0.1456] bleu [0.2717] bleu_1 [6.0201] bleu_4 [0.0523]\n",
            "Epochs [75/250] Step [61/183] Loss [0.1452] bleu [0.2728] bleu_1 [6.5574] bleu_4 [0.0510]\n",
            "Epochs [75/250] Step [122/183] Loss [0.1520] bleu [0.2833] bleu_1 [6.2500] bleu_4 [0.0548]\n",
            "Epochs [76/250] Step [0/183] Loss [0.1600] bleu [0.2552] bleu_1 [5.1948] bleu_4 [0.0504]\n",
            "Epochs [76/250] Step [61/183] Loss [0.1507] bleu [0.2705] bleu_1 [7.0064] bleu_4 [0.0492]\n",
            "Epochs [76/250] Step [122/183] Loss [0.2144] bleu [0.2492] bleu_1 [5.3292] bleu_4 [0.0483]\n",
            "Epochs [77/250] Step [0/183] Loss [0.1503] bleu [0.2543] bleu_1 [5.1780] bleu_4 [0.0502]\n",
            "Epochs [77/250] Step [61/183] Loss [0.1813] bleu [0.2768] bleu_1 [6.6445] bleu_4 [0.0519]\n",
            "Epochs [77/250] Step [122/183] Loss [0.1576] bleu [0.2712] bleu_1 [6.7742] bleu_4 [0.0500]\n",
            "Epochs [78/250] Step [0/183] Loss [0.1679] bleu [0.2973] bleu_1 [6.5217] bleu_4 [0.0579]\n",
            "Epochs [78/250] Step [61/183] Loss [0.1747] bleu [0.1863] bleu_1 [3.3248] bleu_4 [0.0378]\n",
            "Epochs [78/250] Step [122/183] Loss [0.1560] bleu [0.2883] bleu_1 [6.6202] bleu_4 [0.0551]\n",
            "Epochs [79/250] Step [0/183] Loss [0.2125] bleu [0.2932] bleu_1 [6.9930] bleu_4 [0.0553]\n",
            "Epochs [79/250] Step [61/183] Loss [0.1831] bleu [0.2967] bleu_1 [7.6125] bleu_4 [0.0546]\n",
            "Epochs [79/250] Step [122/183] Loss [0.2012] bleu [0.2638] bleu_1 [5.3512] bleu_4 [0.0523]\n",
            "Epochs [80/250] Step [0/183] Loss [0.2232] bleu [0.2520] bleu_1 [4.8860] bleu_4 [0.0506]\n",
            "Epochs [80/250] Step [61/183] Loss [0.1978] bleu [0.2943] bleu_1 [6.1818] bleu_4 [0.0581]\n",
            "Epochs [80/250] Step [122/183] Loss [0.1743] bleu [0.2686] bleu_1 [6.9620] bleu_4 [0.0488]\n",
            "Epochs [81/250] Step [0/183] Loss [0.1741] bleu [0.2863] bleu_1 [5.7554] bleu_4 [0.0573]\n",
            "Epochs [81/250] Step [61/183] Loss [0.2151] bleu [0.2696] bleu_1 [4.6429] bleu_4 [0.0568]\n",
            "Epochs [81/250] Step [122/183] Loss [0.1728] bleu [0.3451] bleu_1 [8.6957] bleu_4 [0.0648]\n",
            "Epochs [82/250] Step [0/183] Loss [0.1903] bleu [0.2509] bleu_1 [5.3628] bleu_4 [0.0486]\n",
            "Epochs [82/250] Step [61/183] Loss [0.1483] bleu [0.2867] bleu_1 [6.3158] bleu_4 [0.0556]\n",
            "Epochs [82/250] Step [122/183] Loss [0.1740] bleu [0.2856] bleu_1 [6.2937] bleu_4 [0.0553]\n",
            "Epochs [83/250] Step [0/183] Loss [0.1572] bleu [0.2434] bleu_1 [5.9172] bleu_4 [0.0450]\n",
            "Epochs [83/250] Step [61/183] Loss [0.1887] bleu [0.2794] bleu_1 [5.3571] bleu_4 [0.0568]\n",
            "Epochs [83/250] Step [122/183] Loss [0.1607] bleu [0.2782] bleu_1 [5.8824] bleu_4 [0.0546]\n",
            "Epochs [84/250] Step [0/183] Loss [0.1756] bleu [0.2998] bleu_1 [8.2192] bleu_4 [0.0539]\n",
            "Epochs [84/250] Step [61/183] Loss [0.2100] bleu [0.2979] bleu_1 [6.2500] bleu_4 [0.0590]\n",
            "Epochs [84/250] Step [122/183] Loss [0.1637] bleu [0.3225] bleu_1 [7.3077] bleu_4 [0.0625]\n",
            "Epochs [85/250] Step [0/183] Loss [0.1509] bleu [0.2703] bleu_1 [6.2500] bleu_4 [0.0512]\n",
            "Epochs [85/250] Step [61/183] Loss [0.1388] bleu [0.2821] bleu_1 [6.7568] bleu_4 [0.0530]\n",
            "Epochs [85/250] Step [122/183] Loss [0.1328] bleu [0.3204] bleu_1 [8.1481] bleu_4 [0.0595]\n",
            "Epochs [86/250] Step [0/183] Loss [0.1595] bleu [0.2580] bleu_1 [4.1958] bleu_4 [0.0553]\n",
            "Epochs [86/250] Step [61/183] Loss [0.2049] bleu [0.2629] bleu_1 [5.5921] bleu_4 [0.0512]\n",
            "Epochs [86/250] Step [122/183] Loss [0.1370] bleu [0.3182] bleu_1 [7.5188] bleu_4 [0.0607]\n",
            "Epochs [87/250] Step [0/183] Loss [0.1341] bleu [0.3570] bleu_1 [7.9832] bleu_4 [0.0702]\n",
            "Epochs [87/250] Step [61/183] Loss [0.1440] bleu [0.2609] bleu_1 [5.2980] bleu_4 [0.0517]\n",
            "Epochs [87/250] Step [122/183] Loss [0.1428] bleu [0.2545] bleu_1 [5.6782] bleu_4 [0.0486]\n",
            "Epochs [88/250] Step [0/183] Loss [0.1425] bleu [0.3056] bleu_1 [6.3910] bleu_4 [0.0607]\n",
            "Epochs [88/250] Step [61/183] Loss [0.1455] bleu [0.3040] bleu_1 [5.7692] bleu_4 [0.0625]\n",
            "Epochs [88/250] Step [122/183] Loss [0.1365] bleu [0.2979] bleu_1 [7.0922] bleu_4 [0.0563]\n",
            "Epochs [89/250] Step [0/183] Loss [0.1401] bleu [0.2794] bleu_1 [5.3571] bleu_4 [0.0568]\n",
            "Epochs [89/250] Step [61/183] Loss [0.1649] bleu [0.1543] bleu_1 [2.4554] bleu_4 [0.0322]\n",
            "Epochs [89/250] Step [122/183] Loss [0.1427] bleu [0.2730] bleu_1 [5.5172] bleu_4 [0.0543]\n",
            "Epochs [90/250] Step [0/183] Loss [0.1513] bleu [0.2899] bleu_1 [7.1918] bleu_4 [0.0539]\n",
            "Epochs [90/250] Step [61/183] Loss [0.2062] bleu [0.2657] bleu_1 [6.8966] bleu_4 [0.0483]\n",
            "Epochs [90/250] Step [122/183] Loss [0.1413] bleu [0.3010] bleu_1 [6.5934] bleu_4 [0.0587]\n",
            "Epochs [91/250] Step [0/183] Loss [0.2058] bleu [0.2562] bleu_1 [5.9561] bleu_4 [0.0483]\n",
            "Epochs [91/250] Step [61/183] Loss [0.1610] bleu [0.2545] bleu_1 [5.4313] bleu_4 [0.0494]\n",
            "Epochs [91/250] Step [122/183] Loss [0.2303] bleu [0.1955] bleu_1 [2.3881] bleu_4 [0.0455]\n",
            "Epochs [92/250] Step [0/183] Loss [0.2100] bleu [0.2895] bleu_1 [6.0932] bleu_4 [0.0571]\n",
            "Epochs [92/250] Step [61/183] Loss [0.1800] bleu [0.1808] bleu_1 [3.4230] bleu_4 [0.0358]\n",
            "Epochs [92/250] Step [122/183] Loss [0.2292] bleu [0.2860] bleu_1 [6.0284] bleu_4 [0.0563]\n",
            "Epochs [93/250] Step [0/183] Loss [0.1766] bleu [0.2207] bleu_1 [4.7753] bleu_4 [0.0422]\n",
            "Epochs [93/250] Step [61/183] Loss [0.1977] bleu [0.2648] bleu_1 [5.6291] bleu_4 [0.0517]\n",
            "Epochs [93/250] Step [122/183] Loss [0.2012] bleu [0.1712] bleu_1 [3.4325] bleu_4 [0.0332]\n",
            "Epochs [94/250] Step [0/183] Loss [0.1819] bleu [0.2737] bleu_1 [6.0606] bleu_4 [0.0527]\n",
            "Epochs [94/250] Step [61/183] Loss [0.1864] bleu [0.2407] bleu_1 [4.4444] bleu_4 [0.0490]\n",
            "Epochs [94/250] Step [122/183] Loss [0.1639] bleu [0.3035] bleu_1 [6.6421] bleu_4 [0.0592]\n",
            "Epochs [95/250] Step [0/183] Loss [0.1982] bleu [0.2128] bleu_1 [4.8257] bleu_4 [0.0399]\n",
            "Epochs [95/250] Step [61/183] Loss [0.1665] bleu [0.2609] bleu_1 [5.2980] bleu_4 [0.0517]\n",
            "Epochs [95/250] Step [122/183] Loss [0.1547] bleu [0.3197] bleu_1 [7.2519] bleu_4 [0.0619]\n",
            "Epochs [96/250] Step [0/183] Loss [0.1589] bleu [0.2561] bleu_1 [4.6980] bleu_4 [0.0525]\n",
            "Epochs [96/250] Step [61/183] Loss [0.1618] bleu [0.2988] bleu_1 [5.3846] bleu_4 [0.0625]\n",
            "Epochs [96/250] Step [122/183] Loss [0.1462] bleu [0.2860] bleu_1 [6.0284] bleu_4 [0.0563]\n",
            "Epochs [97/250] Step [0/183] Loss [0.1398] bleu [0.2815] bleu_1 [5.9441] bleu_4 [0.0553]\n",
            "Epochs [97/250] Step [61/183] Loss [0.1426] bleu [0.2773] bleu_1 [5.5944] bleu_4 [0.0553]\n",
            "Epochs [97/250] Step [122/183] Loss [0.2173] bleu [0.2810] bleu_1 [6.7340] bleu_4 [0.0527]\n",
            "Epochs [98/250] Step [0/183] Loss [0.1400] bleu [0.2922] bleu_1 [5.8608] bleu_4 [0.0587]\n",
            "Epochs [98/250] Step [61/183] Loss [0.1378] bleu [0.2847] bleu_1 [3.9683] bleu_4 [0.0651]\n",
            "Epochs [98/250] Step [122/183] Loss [0.1499] bleu [0.3115] bleu_1 [7.0896] bleu_4 [0.0601]\n",
            "Epochs [99/250] Step [0/183] Loss [0.1582] bleu [0.2980] bleu_1 [7.3684] bleu_4 [0.0556]\n",
            "Epochs [99/250] Step [61/183] Loss [0.1594] bleu [0.1316] bleu_1 [2.6834] bleu_4 [0.0251]\n",
            "Epochs [99/250] Step [122/183] Loss [0.1715] bleu [0.1211] bleu_1 [2.0906] bleu_4 [0.0243]\n",
            "Epochs [100/250] Step [0/183] Loss [0.1415] bleu [0.2882] bleu_1 [4.9242] bleu_4 [0.0613]\n",
            "Epochs [100/250] Step [61/183] Loss [0.1511] bleu [0.2427] bleu_1 [5.6716] bleu_4 [0.0455]\n",
            "Epochs [100/250] Step [122/183] Loss [0.1611] bleu [0.2742] bleu_1 [4.4280] bleu_4 [0.0592]\n",
            "Epochs [101/250] Step [0/183] Loss [0.1516] bleu [0.3325] bleu_1 [7.8125] bleu_4 [0.0638]\n",
            "Epochs [101/250] Step [61/183] Loss [0.1487] bleu [0.2698] bleu_1 [5.4608] bleu_4 [0.0536]\n",
            "Epochs [101/250] Step [122/183] Loss [0.1736] bleu [0.1969] bleu_1 [3.7037] bleu_4 [0.0393]\n",
            "Epochs [102/250] Step [0/183] Loss [0.1456] bleu [0.3239] bleu_1 [6.0976] bleu_4 [0.0672]\n",
            "Epochs [102/250] Step [61/183] Loss [0.1436] bleu [0.2678] bleu_1 [5.6856] bleu_4 [0.0523]\n",
            "Epochs [102/250] Step [122/183] Loss [0.1468] bleu [0.3065] bleu_1 [7.2727] bleu_4 [0.0581]\n",
            "Epochs [103/250] Step [0/183] Loss [0.1365] bleu [0.2955] bleu_1 [6.2044] bleu_4 [0.0584]\n",
            "Epochs [103/250] Step [61/183] Loss [0.1908] bleu [0.1300] bleu_1 [2.3766] bleu_4 [0.0257]\n",
            "Epochs [103/250] Step [122/183] Loss [0.1659] bleu [0.2719] bleu_1 [5.4983] bleu_4 [0.0541]\n",
            "Epochs [104/250] Step [0/183] Loss [0.1783] bleu [0.2477] bleu_1 [4.5603] bleu_4 [0.0506]\n",
            "Epochs [104/250] Step [61/183] Loss [0.1866] bleu [0.2839] bleu_1 [6.5292] bleu_4 [0.0541]\n",
            "Epochs [104/250] Step [122/183] Loss [0.1992] bleu [0.2821] bleu_1 [6.7568] bleu_4 [0.0530]\n",
            "Epochs [105/250] Step [0/183] Loss [0.2052] bleu [0.4037] bleu_1 [9.9548] bleu_4 [0.0776]\n",
            "Epochs [105/250] Step [61/183] Loss [0.2260] bleu [0.2725] bleu_1 [4.9645] bleu_4 [0.0563]\n",
            "Epochs [105/250] Step [122/183] Loss [0.1506] bleu [0.2762] bleu_1 [5.5749] bleu_4 [0.0551]\n",
            "Epochs [106/250] Step [0/183] Loss [0.1524] bleu [0.1964] bleu_1 [3.6939] bleu_4 [0.0392]\n",
            "Epochs [106/250] Step [61/183] Loss [0.1542] bleu [0.3183] bleu_1 [7.2243] bleu_4 [0.0616]\n",
            "Epochs [106/250] Step [122/183] Loss [0.1649] bleu [0.2458] bleu_1 [4.2763] bleu_4 [0.0512]\n",
            "Epochs [107/250] Step [0/183] Loss [0.2190] bleu [0.1942] bleu_1 [2.1407] bleu_4 [0.0468]\n",
            "Epochs [107/250] Step [61/183] Loss [0.1524] bleu [0.3340] bleu_1 [7.8431] bleu_4 [0.0641]\n",
            "Epochs [107/250] Step [122/183] Loss [0.2012] bleu [0.2771] bleu_1 [6.9079] bleu_4 [0.0512]\n",
            "Epochs [108/250] Step [0/183] Loss [0.1493] bleu [0.2545] bleu_1 [5.4313] bleu_4 [0.0494]\n",
            "Epochs [108/250] Step [61/183] Loss [0.1686] bleu [0.2714] bleu_1 [4.9470] bleu_4 [0.0561]\n",
            "Epochs [108/250] Step [122/183] Loss [0.1467] bleu [0.2961] bleu_1 [6.4982] bleu_4 [0.0576]\n",
            "Epochs [109/250] Step [0/183] Loss [0.1783] bleu [0.2582] bleu_1 [4.4674] bleu_4 [0.0541]\n",
            "Epochs [109/250] Step [61/183] Loss [0.1452] bleu [0.2972] bleu_1 [5.9480] bleu_4 [0.0598]\n",
            "Epochs [109/250] Step [122/183] Loss [0.1407] bleu [0.2833] bleu_1 [4.8507] bleu_4 [0.0601]\n",
            "Epochs [110/250] Step [0/183] Loss [0.1412] bleu [0.2992] bleu_1 [6.2731] bleu_4 [0.0592]\n",
            "Epochs [110/250] Step [61/183] Loss [0.1608] bleu [0.2712] bleu_1 [6.7742] bleu_4 [0.0500]\n",
            "Epochs [110/250] Step [122/183] Loss [0.1662] bleu [0.3113] bleu_1 [6.7925] bleu_4 [0.0610]\n",
            "Epochs [111/250] Step [0/183] Loss [0.1446] bleu [0.2148] bleu_1 [4.4444] bleu_4 [0.0417]\n",
            "Epochs [111/250] Step [61/183] Loss [0.1421] bleu [0.2985] bleu_1 [6.5455] bleu_4 [0.0581]\n",
            "Epochs [111/250] Step [122/183] Loss [0.1621] bleu [0.1762] bleu_1 [3.7037] bleu_4 [0.0336]\n",
            "Epochs [112/250] Step [0/183] Loss [0.1354] bleu [0.2761] bleu_1 [5.8419] bleu_4 [0.0541]\n",
            "Epochs [112/250] Step [61/183] Loss [0.1664] bleu [0.1936] bleu_1 [3.8462] bleu_4 [0.0379]\n",
            "Epochs [112/250] Step [122/183] Loss [0.1500] bleu [0.3390] bleu_1 [8.5603] bleu_4 [0.0635]\n",
            "Epochs [113/250] Step [0/183] Loss [0.1474] bleu [0.2668] bleu_1 [5.4054] bleu_4 [0.0530]\n",
            "Epochs [113/250] Step [61/183] Loss [0.1736] bleu [0.2658] bleu_1 [5.3872] bleu_4 [0.0527]\n",
            "Epochs [113/250] Step [122/183] Loss [0.1911] bleu [0.3155] bleu_1 [7.1698] bleu_4 [0.0610]\n",
            "Epochs [114/250] Step [0/183] Loss [0.1917] bleu [0.2101] bleu_1 [4.9738] bleu_4 [0.0388]\n",
            "Epochs [114/250] Step [61/183] Loss [0.1792] bleu [0.3040] bleu_1 [7.5000] bleu_4 [0.0568]\n",
            "Epochs [114/250] Step [122/183] Loss [0.1724] bleu [0.2664] bleu_1 [4.5936] bleu_4 [0.0561]\n",
            "Epochs [115/250] Step [0/183] Loss [0.2205] bleu [0.2429] bleu_1 [4.9689] bleu_4 [0.0477]\n",
            "Epochs [115/250] Step [61/183] Loss [0.2085] bleu [0.2727] bleu_1 [6.0403] bleu_4 [0.0525]\n",
            "Epochs [115/250] Step [122/183] Loss [0.3167] bleu [0.2856] bleu_1 [6.2937] bleu_4 [0.0553]\n",
            "Epochs [116/250] Step [0/183] Loss [0.1447] bleu [0.3030] bleu_1 [6.3433] bleu_4 [0.0601]\n",
            "Epochs [116/250] Step [61/183] Loss [0.1751] bleu [0.2514] bleu_1 [4.6205] bleu_4 [0.0514]\n",
            "Epochs [116/250] Step [122/183] Loss [0.1809] bleu [0.3355] bleu_1 [7.8740] bleu_4 [0.0644]\n",
            "Epochs [117/250] Step [0/183] Loss [0.1307] bleu [0.2842] bleu_1 [6.8027] bleu_4 [0.0534]\n",
            "Epochs [117/250] Step [61/183] Loss [0.1705] bleu [0.2632] bleu_1 [6.3492] bleu_4 [0.0490]\n",
            "Epochs [117/250] Step [122/183] Loss [0.1606] bleu [0.3225] bleu_1 [7.3077] bleu_4 [0.0625]\n",
            "Epochs [118/250] Step [0/183] Loss [0.1574] bleu [0.2774] bleu_1 [6.3973] bleu_4 [0.0527]\n",
            "Epochs [118/250] Step [61/183] Loss [0.2071] bleu [0.2068] bleu_1 [2.2654] bleu_4 [0.0502]\n",
            "Epochs [118/250] Step [122/183] Loss [0.1566] bleu [0.3155] bleu_1 [7.4627] bleu_4 [0.0601]\n",
            "Epochs [119/250] Step [0/183] Loss [0.1375] bleu [0.3040] bleu_1 [5.1587] bleu_4 [0.0651]\n",
            "Epochs [119/250] Step [61/183] Loss [0.1419] bleu [0.2527] bleu_1 [5.6426] bleu_4 [0.0483]\n",
            "Epochs [119/250] Step [122/183] Loss [0.1710] bleu [0.2686] bleu_1 [5.9603] bleu_4 [0.0517]\n",
            "Epochs [120/250] Step [0/183] Loss [0.1340] bleu [0.2440] bleu_1 [4.2484] bleu_4 [0.0508]\n",
            "Epochs [120/250] Step [61/183] Loss [0.1345] bleu [0.3297] bleu_1 [7.1429] bleu_4 [0.0651]\n",
            "Epochs [120/250] Step [122/183] Loss [0.1612] bleu [0.3181] bleu_1 [7.8067] bleu_4 [0.0598]\n",
            "Epochs [121/250] Step [0/183] Loss [0.1456] bleu [0.2697] bleu_1 [5.1903] bleu_4 [0.0546]\n",
            "Epochs [121/250] Step [61/183] Loss [0.1398] bleu [0.2789] bleu_1 [6.6890] bleu_4 [0.0523]\n",
            "Epochs [121/250] Step [122/183] Loss [0.1911] bleu [0.2194] bleu_1 [4.7486] bleu_4 [0.0419]\n",
            "Epochs [122/250] Step [0/183] Loss [0.1554] bleu [0.3109] bleu_1 [6.4885] bleu_4 [0.0619]\n",
            "Epochs [122/250] Step [61/183] Loss [0.2027] bleu [0.2501] bleu_1 [5.8282] bleu_4 [0.0470]\n",
            "Epochs [122/250] Step [122/183] Loss [0.1824] bleu [0.2553] bleu_1 [5.9375] bleu_4 [0.0481]\n",
            "Epochs [123/250] Step [0/183] Loss [0.1621] bleu [0.2769] bleu_1 [5.0360] bleu_4 [0.0573]\n",
            "Epochs [123/250] Step [61/183] Loss [0.1777] bleu [0.2790] bleu_1 [6.1644] bleu_4 [0.0539]\n",
            "Epochs [123/250] Step [122/183] Loss [0.1743] bleu [0.2888] bleu_1 [7.1672] bleu_4 [0.0536]\n",
            "Epochs [124/250] Step [0/183] Loss [0.1339] bleu [0.2866] bleu_1 [7.1186] bleu_4 [0.0532]\n",
            "Epochs [124/250] Step [61/183] Loss [0.1443] bleu [0.3353] bleu_1 [8.7786] bleu_4 [0.0619]\n",
            "Epochs [124/250] Step [122/183] Loss [0.1478] bleu [0.2972] bleu_1 [5.9480] bleu_4 [0.0598]\n",
            "Epochs [125/250] Step [0/183] Loss [0.1449] bleu [0.2664] bleu_1 [6.1688] bleu_4 [0.0504]\n",
            "Epochs [125/250] Step [61/183] Loss [0.1522] bleu [0.3280] bleu_1 [6.8000] bleu_4 [0.0658]\n",
            "Epochs [125/250] Step [122/183] Loss [0.1344] bleu [0.2992] bleu_1 [6.2731] bleu_4 [0.0592]\n",
            "Epochs [126/250] Step [0/183] Loss [0.2006] bleu [0.2993] bleu_1 [4.7809] bleu_4 [0.0654]\n",
            "Epochs [126/250] Step [61/183] Loss [0.1329] bleu [0.2917] bleu_1 [6.6901] bleu_4 [0.0558]\n",
            "Epochs [126/250] Step [122/183] Loss [0.1368] bleu [0.2686] bleu_1 [5.1724] bleu_4 [0.0543]\n",
            "Epochs [127/250] Step [0/183] Loss [0.1564] bleu [0.3113] bleu_1 [7.9422] bleu_4 [0.0576]\n",
            "Epochs [127/250] Step [61/183] Loss [0.1568] bleu [0.3043] bleu_1 [6.3670] bleu_4 [0.0604]\n",
            "Epochs [127/250] Step [122/183] Loss [0.1541] bleu [0.3342] bleu_1 [6.9106] bleu_4 [0.0672]\n",
            "Epochs [128/250] Step [0/183] Loss [0.1424] bleu [0.2851] bleu_1 [5.7348] bleu_4 [0.0571]\n",
            "Epochs [128/250] Step [61/183] Loss [0.1391] bleu [0.3010] bleu_1 [6.5934] bleu_4 [0.0587]\n",
            "Epochs [128/250] Step [122/183] Loss [0.1858] bleu [0.1261] bleu_1 [1.8904] bleu_4 [0.0267]\n",
            "Epochs [129/250] Step [0/183] Loss [0.1300] bleu [0.3116] bleu_1 [7.3801] bleu_4 [0.0592]\n",
            "Epochs [129/250] Step [61/183] Loss [0.1453] bleu [0.2907] bleu_1 [6.1151] bleu_4 [0.0573]\n",
            "Epochs [129/250] Step [122/183] Loss [0.1463] bleu [0.2815] bleu_1 [5.9441] bleu_4 [0.0553]\n",
            "Epochs [130/250] Step [0/183] Loss [0.1358] bleu [0.2864] bleu_1 [6.8493] bleu_4 [0.0539]\n",
            "Epochs [130/250] Step [61/183] Loss [0.1303] bleu [0.3235] bleu_1 [7.9245] bleu_4 [0.0610]\n",
            "Epochs [130/250] Step [122/183] Loss [0.1317] bleu [0.3062] bleu_1 [6.1069] bleu_4 [0.0619]\n",
            "Epochs [131/250] Step [0/183] Loss [0.1623] bleu [0.2318] bleu_1 [5.2174] bleu_4 [0.0439]\n",
            "Epochs [131/250] Step [61/183] Loss [0.1562] bleu [0.2384] bleu_1 [6.6852] bleu_4 [0.0418]\n",
            "Epochs [131/250] Step [122/183] Loss [0.2274] bleu [0.1255] bleu_1 [2.1622] bleu_4 [0.0253]\n",
            "Epochs [132/250] Step [0/183] Loss [0.1475] bleu [0.2225] bleu_1 [5.0279] bleu_4 [0.0419]\n",
            "Epochs [132/250] Step [61/183] Loss [0.1359] bleu [0.2302] bleu_1 [3.2895] bleu_4 [0.0512]\n",
            "Epochs [132/250] Step [122/183] Loss [0.1531] bleu [0.3003] bleu_1 [7.1429] bleu_4 [0.0568]\n",
            "Epochs [133/250] Step [0/183] Loss [0.1659] bleu [0.2773] bleu_1 [5.5944] bleu_4 [0.0553]\n",
            "Epochs [133/250] Step [61/183] Loss [0.2252] bleu [0.3422] bleu_1 [7.3770] bleu_4 [0.0679]\n",
            "Epochs [133/250] Step [122/183] Loss [0.1411] bleu [0.2906] bleu_1 [6.6667] bleu_4 [0.0556]\n",
            "Epochs [134/250] Step [0/183] Loss [0.2071] bleu [0.3530] bleu_1 [8.2305] bleu_4 [0.0683]\n",
            "Epochs [134/250] Step [61/183] Loss [0.1449] bleu [0.2493] bleu_1 [5.5728] bleu_4 [0.0475]\n",
            "Epochs [134/250] Step [122/183] Loss [0.1350] bleu [0.2580] bleu_1 [4.1958] bleu_4 [0.0553]\n",
            "Epochs [135/250] Step [0/183] Loss [0.1878] bleu [0.2491] bleu_1 [6.0423] bleu_4 [0.0461]\n",
            "Epochs [135/250] Step [61/183] Loss [0.1513] bleu [0.2850] bleu_1 [5.1661] bleu_4 [0.0592]\n",
            "Epochs [135/250] Step [122/183] Loss [0.1724] bleu [0.2028] bleu_1 [2.4691] bleu_4 [0.0473]\n",
            "Epochs [136/250] Step [0/183] Loss [0.1496] bleu [0.2590] bleu_1 [4.7458] bleu_4 [0.0532]\n",
            "Epochs [136/250] Step [61/183] Loss [0.1654] bleu [0.2609] bleu_1 [5.8065] bleu_4 [0.0500]\n",
            "Epochs [136/250] Step [122/183] Loss [0.1423] bleu [0.2683] bleu_1 [6.2092] bleu_4 [0.0508]\n",
            "Epochs [137/250] Step [0/183] Loss [0.1910] bleu [0.2387] bleu_1 [5.8140] bleu_4 [0.0440]\n",
            "Epochs [137/250] Step [61/183] Loss [0.1525] bleu [0.2645] bleu_1 [5.1020] bleu_4 [0.0534]\n",
            "Epochs [137/250] Step [122/183] Loss [0.1706] bleu [0.2703] bleu_1 [6.2500] bleu_4 [0.0512]\n",
            "Epochs [138/250] Step [0/183] Loss [0.1541] bleu [0.2719] bleu_1 [5.7627] bleu_4 [0.0532]\n",
            "Epochs [138/250] Step [61/183] Loss [0.1999] bleu [0.2837] bleu_1 [5.9859] bleu_4 [0.0558]\n",
            "Epochs [138/250] Step [122/183] Loss [0.1581] bleu [0.2737] bleu_1 [6.0606] bleu_4 [0.0527]\n",
            "Epochs [139/250] Step [0/183] Loss [0.1401] bleu [0.2683] bleu_1 [6.2092] bleu_4 [0.0508]\n",
            "Epochs [139/250] Step [61/183] Loss [0.1437] bleu [0.2541] bleu_1 [3.3088] bleu_4 [0.0590]\n",
            "Epochs [139/250] Step [122/183] Loss [0.1500] bleu [0.3380] bleu_1 [9.4340] bleu_4 [0.0610]\n",
            "Epochs [140/250] Step [0/183] Loss [0.1381] bleu [0.2991] bleu_1 [7.6655] bleu_4 [0.0551]\n",
            "Epochs [140/250] Step [61/183] Loss [0.1403] bleu [0.2542] bleu_1 [4.6667] bleu_4 [0.0521]\n",
            "Epochs [140/250] Step [122/183] Loss [0.1392] bleu [0.2542] bleu_1 [4.4068] bleu_4 [0.0532]\n",
            "Epochs [141/250] Step [0/183] Loss [0.1391] bleu [0.2877] bleu_1 [7.1429] bleu_4 [0.0534]\n",
            "Epochs [141/250] Step [61/183] Loss [0.1298] bleu [0.2937] bleu_1 [6.4516] bleu_4 [0.0571]\n",
            "Epochs [141/250] Step [122/183] Loss [0.1735] bleu [0.2826] bleu_1 [5.9649] bleu_4 [0.0556]\n",
            "Epochs [142/250] Step [0/183] Loss [0.1357] bleu [0.2825] bleu_1 [4.5455] bleu_4 [0.0613]\n",
            "Epochs [142/250] Step [61/183] Loss [0.1979] bleu [0.2973] bleu_1 [6.5217] bleu_4 [0.0579]\n",
            "Epochs [142/250] Step [122/183] Loss [0.1434] bleu [0.2570] bleu_1 [5.2288] bleu_4 [0.0508]\n",
            "Epochs [143/250] Step [0/183] Loss [0.1380] bleu [0.2967] bleu_1 [7.6125] bleu_4 [0.0546]\n",
            "Epochs [143/250] Step [61/183] Loss [0.1494] bleu [0.2504] bleu_1 [4.3478] bleu_4 [0.0523]\n",
            "Epochs [143/250] Step [122/183] Loss [0.1543] bleu [0.2664] bleu_1 [6.6667] bleu_4 [0.0490]\n",
            "Epochs [144/250] Step [0/183] Loss [0.1405] bleu [0.2645] bleu_1 [5.1020] bleu_4 [0.0534]\n",
            "Epochs [144/250] Step [61/183] Loss [0.2328] bleu [0.3312] bleu_1 [7.1713] bleu_4 [0.0654]\n",
            "Epochs [144/250] Step [122/183] Loss [0.1432] bleu [0.3195] bleu_1 [6.9498] bleu_4 [0.0628]\n",
            "Epochs [145/250] Step [0/183] Loss [0.1424] bleu [0.3282] bleu_1 [7.1146] bleu_4 [0.0648]\n",
            "Epochs [145/250] Step [61/183] Loss [0.1520] bleu [0.2581] bleu_1 [5.5016] bleu_4 [0.0502]\n",
            "Epochs [145/250] Step [122/183] Loss [0.1506] bleu [0.3060] bleu_1 [6.6914] bleu_4 [0.0598]\n",
            "Epochs [146/250] Step [0/183] Loss [0.1448] bleu [0.2429] bleu_1 [4.9689] bleu_4 [0.0477]\n",
            "Epochs [146/250] Step [61/183] Loss [0.1448] bleu [0.2814] bleu_1 [7.2607] bleu_4 [0.0514]\n",
            "Epochs [146/250] Step [122/183] Loss [0.1836] bleu [0.3530] bleu_1 [8.2305] bleu_4 [0.0683]\n",
            "Epochs [147/250] Step [0/183] Loss [0.1824] bleu [0.2937] bleu_1 [5.5970] bleu_4 [0.0601]\n",
            "Epochs [147/250] Step [61/183] Loss [0.2709] bleu [0.2511] bleu_1 [4.8701] bleu_4 [0.0504]\n",
            "Epochs [147/250] Step [122/183] Loss [0.1504] bleu [0.3043] bleu_1 [6.3670] bleu_4 [0.0604]\n",
            "Epochs [148/250] Step [0/183] Loss [0.1315] bleu [0.3100] bleu_1 [6.7669] bleu_4 [0.0607]\n",
            "Epochs [148/250] Step [61/183] Loss [0.1353] bleu [0.2689] bleu_1 [6.4725] bleu_4 [0.0502]\n",
            "Epochs [148/250] Step [122/183] Loss [0.1354] bleu [0.2527] bleu_1 [5.3968] bleu_4 [0.0490]\n",
            "Epochs [149/250] Step [0/183] Loss [0.1335] bleu [0.3003] bleu_1 [7.4205] bleu_4 [0.0561]\n",
            "Epochs [149/250] Step [61/183] Loss [0.1405] bleu [0.2867] bleu_1 [6.3158] bleu_4 [0.0556]\n",
            "Epochs [149/250] Step [122/183] Loss [0.3071] bleu [0.1848] bleu_1 [1.5773] bleu_4 [0.0486]\n",
            "Epochs [150/250] Step [0/183] Loss [0.1676] bleu [0.2754] bleu_1 [6.3545] bleu_4 [0.0523]\n",
            "Epochs [150/250] Step [61/183] Loss [0.1544] bleu [0.2707] bleu_1 [4.6595] bleu_4 [0.0571]\n",
            "Epochs [150/250] Step [122/183] Loss [0.1584] bleu [0.2922] bleu_1 [7.5085] bleu_4 [0.0536]\n",
            "Epochs [151/250] Step [0/183] Loss [0.1839] bleu [0.2863] bleu_1 [5.7554] bleu_4 [0.0573]\n",
            "Epochs [151/250] Step [61/183] Loss [0.1621] bleu [0.2984] bleu_1 [5.9701] bleu_4 [0.0601]\n",
            "Epochs [151/250] Step [122/183] Loss [0.1730] bleu [0.2773] bleu_1 [5.5944] bleu_4 [0.0553]\n",
            "Epochs [152/250] Step [0/183] Loss [0.1421] bleu [0.2695] bleu_1 [6.9841] bleu_4 [0.0490]\n",
            "Epochs [152/250] Step [61/183] Loss [0.1438] bleu [0.2548] bleu_1 [4.9342] bleu_4 [0.0512]\n",
            "Epochs [152/250] Step [122/183] Loss [0.1504] bleu [0.2639] bleu_1 [5.6106] bleu_4 [0.0514]\n",
            "Epochs [153/250] Step [0/183] Loss [0.1328] bleu [0.2250] bleu_1 [3.4700] bleu_4 [0.0486]\n",
            "Epochs [153/250] Step [61/183] Loss [0.1368] bleu [0.3207] bleu_1 [6.6667] bleu_4 [0.0641]\n",
            "Epochs [153/250] Step [122/183] Loss [0.1380] bleu [0.2686] bleu_1 [5.1724] bleu_4 [0.0543]\n",
            "Epochs [154/250] Step [0/183] Loss [0.1514] bleu [0.2427] bleu_1 [5.6716] bleu_4 [0.0455]\n",
            "Epochs [154/250] Step [61/183] Loss [0.1319] bleu [0.2590] bleu_1 [4.7458] bleu_4 [0.0532]\n",
            "Epochs [154/250] Step [122/183] Loss [0.1375] bleu [0.2693] bleu_1 [6.2295] bleu_4 [0.0510]\n",
            "Epochs [155/250] Step [0/183] Loss [0.1386] bleu [0.2708] bleu_1 [6.5147] bleu_4 [0.0506]\n",
            "Epochs [155/250] Step [61/183] Loss [0.1313] bleu [0.2658] bleu_1 [5.3872] bleu_4 [0.0527]\n",
            "Epochs [155/250] Step [122/183] Loss [0.1323] bleu [0.3089] bleu_1 [7.6087] bleu_4 [0.0579]\n",
            "Epochs [156/250] Step [0/183] Loss [0.1308] bleu [0.3388] bleu_1 [7.6305] bleu_4 [0.0661]\n",
            "Epochs [156/250] Step [61/183] Loss [0.1352] bleu [0.3365] bleu_1 [8.2031] bleu_4 [0.0638]\n",
            "Epochs [156/250] Step [122/183] Loss [0.1584] bleu [0.2559] bleu_1 [6.1920] bleu_4 [0.0475]\n",
            "Epochs [157/250] Step [0/183] Loss [0.1333] bleu [0.3268] bleu_1 [7.3930] bleu_4 [0.0635]\n",
            "Epochs [157/250] Step [61/183] Loss [0.1334] bleu [0.3488] bleu_1 [7.5000] bleu_4 [0.0694]\n",
            "Epochs [157/250] Step [122/183] Loss [0.1469] bleu [0.2709] bleu_1 [5.7432] bleu_4 [0.0530]\n",
            "Epochs [158/250] Step [0/183] Loss [0.1333] bleu [0.3038] bleu_1 [6.9343] bleu_4 [0.0584]\n",
            "Epochs [158/250] Step [61/183] Loss [0.1536] bleu [0.2968] bleu_1 [7.3427] bleu_4 [0.0553]\n",
            "Epochs [158/250] Step [122/183] Loss [0.1427] bleu [0.2840] bleu_1 [5.7143] bleu_4 [0.0568]\n",
            "Epochs [159/250] Step [0/183] Loss [0.1461] bleu [0.3449] bleu_1 [6.4378] bleu_4 [0.0723]\n",
            "Epochs [159/250] Step [61/183] Loss [0.1577] bleu [0.2794] bleu_1 [5.3571] bleu_4 [0.0568]\n",
            "Epochs [159/250] Step [122/183] Loss [0.2013] bleu [0.2991] bleu_1 [7.3944] bleu_4 [0.0558]\n",
            "Epochs [160/250] Step [0/183] Loss [0.2056] bleu [0.2664] bleu_1 [6.1688] bleu_4 [0.0504]\n",
            "Epochs [160/250] Step [61/183] Loss [0.1629] bleu [0.2703] bleu_1 [4.9296] bleu_4 [0.0558]\n",
            "Epochs [160/250] Step [122/183] Loss [0.2034] bleu [0.2849] bleu_1 [6.0071] bleu_4 [0.0561]\n",
            "Epochs [161/250] Step [0/183] Loss [0.1566] bleu [0.2795] bleu_1 [6.4407] bleu_4 [0.0532]\n",
            "Epochs [161/250] Step [61/183] Loss [0.1313] bleu [0.3014] bleu_1 [7.7193] bleu_4 [0.0556]\n",
            "Epochs [161/250] Step [122/183] Loss [0.1443] bleu [0.2811] bleu_1 [6.2069] bleu_4 [0.0543]\n",
            "Epochs [162/250] Step [0/183] Loss [0.1942] bleu [0.2747] bleu_1 [5.0000] bleu_4 [0.0568]\n",
            "Epochs [162/250] Step [61/183] Loss [0.1367] bleu [0.2798] bleu_1 [4.7970] bleu_4 [0.0592]\n",
            "Epochs [162/250] Step [122/183] Loss [0.1355] bleu [0.1988] bleu_1 [4.5340] bleu_4 [0.0371]\n",
            "Epochs [163/250] Step [0/183] Loss [0.1358] bleu [0.3246] bleu_1 [6.4257] bleu_4 [0.0661]\n",
            "Epochs [163/250] Step [61/183] Loss [0.1333] bleu [0.2523] bleu_1 [4.3771] bleu_4 [0.0527]\n",
            "Epochs [163/250] Step [122/183] Loss [0.1330] bleu [0.2638] bleu_1 [5.3512] bleu_4 [0.0523]\n",
            "Epochs [164/250] Step [0/183] Loss [0.1733] bleu [0.2609] bleu_1 [6.5421] bleu_4 [0.0479]\n",
            "Epochs [164/250] Step [61/183] Loss [0.1325] bleu [0.2586] bleu_1 [5.0000] bleu_4 [0.0521]\n",
            "Epochs [164/250] Step [122/183] Loss [0.1590] bleu [0.2866] bleu_1 [7.1186] bleu_4 [0.0532]\n",
            "Epochs [165/250] Step [0/183] Loss [0.1601] bleu [0.2967] bleu_1 [6.2271] bleu_4 [0.0587]\n",
            "Epochs [165/250] Step [61/183] Loss [0.1412] bleu [0.2864] bleu_1 [6.8493] bleu_4 [0.0539]\n",
            "Epochs [165/250] Step [122/183] Loss [0.1637] bleu [0.2910] bleu_1 [5.8394] bleu_4 [0.0584]\n",
            "Epochs [166/250] Step [0/183] Loss [0.1680] bleu [0.2965] bleu_1 [6.7857] bleu_4 [0.0568]\n",
            "Epochs [166/250] Step [61/183] Loss [0.1716] bleu [0.1715] bleu_1 [3.6117] bleu_4 [0.0326]\n",
            "Epochs [166/250] Step [122/183] Loss [0.1486] bleu [0.2300] bleu_1 [3.7855] bleu_4 [0.0486]\n",
            "Epochs [167/250] Step [0/183] Loss [0.1297] bleu [0.2750] bleu_1 [5.2817] bleu_4 [0.0558]\n",
            "Epochs [167/250] Step [61/183] Loss [0.1364] bleu [0.2763] bleu_1 [4.7445] bleu_4 [0.0584]\n",
            "Epochs [167/250] Step [122/183] Loss [0.1343] bleu [0.2677] bleu_1 [5.9406] bleu_4 [0.0514]\n",
            "Epochs [168/250] Step [0/183] Loss [0.1313] bleu [0.3281] bleu_1 [7.7220] bleu_4 [0.0628]\n",
            "Epochs [168/250] Step [61/183] Loss [0.1332] bleu [0.2707] bleu_1 [4.6595] bleu_4 [0.0571]\n",
            "Epochs [168/250] Step [122/183] Loss [0.1313] bleu [0.3454] bleu_1 [7.4380] bleu_4 [0.0687]\n",
            "Epochs [169/250] Step [0/183] Loss [0.1493] bleu [0.2773] bleu_1 [5.5944] bleu_4 [0.0553]\n",
            "Epochs [169/250] Step [61/183] Loss [0.1342] bleu [0.2846] bleu_1 [7.3333] bleu_4 [0.0521]\n",
            "Epochs [169/250] Step [122/183] Loss [0.1302] bleu [0.2793] bleu_1 [5.9028] bleu_4 [0.0548]\n",
            "Epochs [170/250] Step [0/183] Loss [0.1862] bleu [0.2480] bleu_1 [6.2500] bleu_4 [0.0453]\n",
            "Epochs [170/250] Step [61/183] Loss [0.1400] bleu [0.2719] bleu_1 [5.4983] bleu_4 [0.0541]\n",
            "Epochs [170/250] Step [122/183] Loss [0.1627] bleu [0.3067] bleu_1 [5.8140] bleu_4 [0.0631]\n",
            "Epochs [171/250] Step [0/183] Loss [0.1452] bleu [0.1955] bleu_1 [2.1538] bleu_4 [0.0472]\n",
            "Epochs [171/250] Step [61/183] Loss [0.1404] bleu [0.2655] bleu_1 [5.1195] bleu_4 [0.0536]\n",
            "Epochs [171/250] Step [122/183] Loss [0.1448] bleu [0.2620] bleu_1 [4.7945] bleu_4 [0.0539]\n",
            "Epochs [172/250] Step [0/183] Loss [0.1505] bleu [0.2762] bleu_1 [5.5749] bleu_4 [0.0551]\n",
            "Epochs [172/250] Step [61/183] Loss [0.1445] bleu [0.2804] bleu_1 [7.7419] bleu_4 [0.0500]\n",
            "Epochs [172/250] Step [122/183] Loss [0.1552] bleu [0.2870] bleu_1 [4.9057] bleu_4 [0.0610]\n",
            "Epochs [173/250] Step [0/183] Loss [0.1460] bleu [0.2298] bleu_1 [5.3977] bleu_4 [0.0428]\n",
            "Epochs [173/250] Step [61/183] Loss [0.2578] bleu [0.2839] bleu_1 [6.5292] bleu_4 [0.0541]\n",
            "Epochs [173/250] Step [122/183] Loss [0.1389] bleu [0.2872] bleu_1 [6.5972] bleu_4 [0.0548]\n",
            "Epochs [174/250] Step [0/183] Loss [0.2222] bleu [0.2214] bleu_1 [4.7887] bleu_4 [0.0424]\n",
            "Epochs [174/250] Step [61/183] Loss [0.1728] bleu [0.2122] bleu_1 [4.6070] bleu_4 [0.0405]\n",
            "Epochs [174/250] Step [122/183] Loss [0.1969] bleu [0.3225] bleu_1 [7.3077] bleu_4 [0.0625]\n",
            "Epochs [175/250] Step [0/183] Loss [0.1486] bleu [0.2449] bleu_1 [4.2623] bleu_4 [0.0510]\n",
            "Epochs [175/250] Step [61/183] Loss [0.1594] bleu [0.2947] bleu_1 [5.9041] bleu_4 [0.0592]\n",
            "Epochs [175/250] Step [122/183] Loss [0.1596] bleu [0.2543] bleu_1 [5.1780] bleu_4 [0.0502]\n",
            "Epochs [176/250] Step [0/183] Loss [0.1402] bleu [0.3103] bleu_1 [7.3529] bleu_4 [0.0590]\n",
            "Epochs [176/250] Step [61/183] Loss [0.1317] bleu [0.2451] bleu_1 [5.7229] bleu_4 [0.0460]\n",
            "Epochs [176/250] Step [122/183] Loss [0.1338] bleu [0.2728] bleu_1 [5.2448] bleu_4 [0.0553]\n",
            "Epochs [177/250] Step [0/183] Loss [0.1287] bleu [0.3087] bleu_1 [6.7416] bleu_4 [0.0604]\n",
            "Epochs [177/250] Step [61/183] Loss [0.1643] bleu [0.1926] bleu_1 [3.8265] bleu_4 [0.0377]\n",
            "Epochs [177/250] Step [122/183] Loss [0.1296] bleu [0.2913] bleu_1 [6.4057] bleu_4 [0.0566]\n",
            "Epochs [178/250] Step [0/183] Loss [0.1299] bleu [0.2837] bleu_1 [5.9859] bleu_4 [0.0558]\n",
            "Epochs [178/250] Step [61/183] Loss [0.1464] bleu [0.2813] bleu_1 [7.0000] bleu_4 [0.0521]\n",
            "Epochs [178/250] Step [122/183] Loss [0.1296] bleu [0.3158] bleu_1 [6.2745] bleu_4 [0.0641]\n",
            "Epochs [179/250] Step [0/183] Loss [0.1354] bleu [0.1936] bleu_1 [4.0404] bleu_4 [0.0372]\n",
            "Epochs [179/250] Step [61/183] Loss [0.1518] bleu [0.1385] bleu_1 [2.9575] bleu_4 [0.0260]\n",
            "Epochs [179/250] Step [122/183] Loss [0.1345] bleu [0.3130] bleu_1 [8.5409] bleu_4 [0.0566]\n",
            "Epochs [180/250] Step [0/183] Loss [0.1683] bleu [0.1875] bleu_1 [4.2959] bleu_4 [0.0348]\n",
            "Epochs [180/250] Step [61/183] Loss [0.1268] bleu [0.3195] bleu_1 [6.9498] bleu_4 [0.0628]\n",
            "Epochs [180/250] Step [122/183] Loss [0.1284] bleu [0.2913] bleu_1 [4.6693] bleu_4 [0.0635]\n",
            "Epochs [181/250] Step [0/183] Loss [0.1351] bleu [0.2432] bleu_1 [4.7319] bleu_4 [0.0486]\n",
            "Epochs [181/250] Step [61/183] Loss [0.1762] bleu [0.1542] bleu_1 [3.2720] bleu_4 [0.0291]\n",
            "Epochs [181/250] Step [122/183] Loss [0.1880] bleu [0.2484] bleu_1 [5.7927] bleu_4 [0.0466]\n",
            "Epochs [182/250] Step [0/183] Loss [0.1300] bleu [0.2678] bleu_1 [5.6856] bleu_4 [0.0523]\n",
            "Epochs [182/250] Step [61/183] Loss [0.1515] bleu [0.2952] bleu_1 [8.1081] bleu_4 [0.0530]\n",
            "Epochs [182/250] Step [122/183] Loss [0.2230] bleu [0.2728] bleu_1 [6.5574] bleu_4 [0.0510]\n",
            "Epochs [183/250] Step [0/183] Loss [0.1759] bleu [0.3140] bleu_1 [5.3061] bleu_4 [0.0676]\n",
            "Epochs [183/250] Step [61/183] Loss [0.1332] bleu [0.2911] bleu_1 [7.4830] bleu_4 [0.0534]\n",
            "Epochs [183/250] Step [122/183] Loss [0.2067] bleu [0.2545] bleu_1 [5.4313] bleu_4 [0.0494]\n",
            "Epochs [184/250] Step [0/183] Loss [0.1587] bleu [0.2405] bleu_1 [4.1935] bleu_4 [0.0500]\n",
            "Epochs [184/250] Step [61/183] Loss [0.1507] bleu [0.2829] bleu_1 [5.4152] bleu_4 [0.0576]\n",
            "Epochs [184/250] Step [122/183] Loss [0.1776] bleu [0.2591] bleu_1 [5.7692] bleu_4 [0.0496]\n",
            "Epochs [185/250] Step [0/183] Loss [0.1373] bleu [0.2423] bleu_1 [4.2208] bleu_4 [0.0504]\n",
            "Epochs [185/250] Step [61/183] Loss [0.1383] bleu [0.2446] bleu_1 [4.0000] bleu_4 [0.0521]\n",
            "Epochs [185/250] Step [122/183] Loss [0.1424] bleu [0.3548] bleu_1 [8.9069] bleu_4 [0.0668]\n",
            "Epochs [186/250] Step [0/183] Loss [0.1379] bleu [0.2882] bleu_1 [4.9242] bleu_4 [0.0613]\n",
            "Epochs [186/250] Step [61/183] Loss [0.1537] bleu [0.2737] bleu_1 [6.0606] bleu_4 [0.0527]\n",
            "Epochs [186/250] Step [122/183] Loss [0.1340] bleu [0.3136] bleu_1 [6.5385] bleu_4 [0.0625]\n",
            "Epochs [187/250] Step [0/183] Loss [0.1308] bleu [0.3355] bleu_1 [7.8740] bleu_4 [0.0644]\n",
            "Epochs [187/250] Step [61/183] Loss [0.1328] bleu [0.3095] bleu_1 [6.4639] bleu_4 [0.0616]\n",
            "Epochs [187/250] Step [122/183] Loss [0.1589] bleu [0.2790] bleu_1 [6.1644] bleu_4 [0.0539]\n",
            "Epochs [188/250] Step [0/183] Loss [0.1335] bleu [0.2657] bleu_1 [5.9016] bleu_4 [0.0510]\n",
            "Epochs [188/250] Step [61/183] Loss [0.1358] bleu [0.3311] bleu_1 [6.8548] bleu_4 [0.0665]\n",
            "Epochs [188/250] Step [122/183] Loss [0.1341] bleu [0.2860] bleu_1 [6.0284] bleu_4 [0.0563]\n",
            "Epochs [189/250] Step [0/183] Loss [0.1338] bleu [0.2667] bleu_1 [5.9211] bleu_4 [0.0512]\n",
            "Epochs [189/250] Step [61/183] Loss [0.1336] bleu [0.2967] bleu_1 [7.6125] bleu_4 [0.0546]\n",
            "Epochs [189/250] Step [122/183] Loss [0.1321] bleu [0.1920] bleu_1 [3.8168] bleu_4 [0.0375]\n",
            "Epochs [190/250] Step [0/183] Loss [0.1312] bleu [0.2740] bleu_1 [4.7101] bleu_4 [0.0579]\n",
            "Epochs [190/250] Step [61/183] Loss [0.1302] bleu [0.2901] bleu_1 [6.3830] bleu_4 [0.0563]\n",
            "Epochs [190/250] Step [122/183] Loss [0.1296] bleu [0.3235] bleu_1 [7.9245] bleu_4 [0.0610]\n",
            "Epochs [191/250] Step [0/183] Loss [0.1297] bleu [0.3451] bleu_1 [8.6957] bleu_4 [0.0648]\n",
            "Epochs [191/250] Step [61/183] Loss [0.1326] bleu [0.2638] bleu_1 [5.8632] bleu_4 [0.0506]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "jjN0H6ht7Fyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path"
      ],
      "metadata": {
        "id": "BE89zyWsxWvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsJ_PV0pJxsw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#save_path = '/content/drive/My Drive/Colab Notebooks/ComputerVision/CNN_T5/image_caption/'\n",
        "os.chdir(save_path)\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Orvj-J2WI0Ro",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e170d53c-4696-45c4-8be0-39ffab98d26d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Epochs  num_epochs  step  [total_step    losses  bleu_score     bleu1  \\\n",
              "0       1         250   182          183  2.041729    0.239512  5.604720   \n",
              "1       2         250   182          183  1.631269    0.231035  5.202312   \n",
              "2       3         250   182          183  1.307436    0.245067  5.487805   \n",
              "3       4         250   182          183  1.232020    0.240390  4.923077   \n",
              "4       5         250   182          183  1.104259    0.241590  5.182927   \n",
              "\n",
              "      bleu4  \n",
              "0  0.044803  \n",
              "1  0.043706  \n",
              "2  0.046642  \n",
              "3  0.047170  \n",
              "4  0.046642  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6fa5c5a4-c28b-4475-bb6b-17042ea89b1c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epochs</th>\n",
              "      <th>num_epochs</th>\n",
              "      <th>step</th>\n",
              "      <th>[total_step</th>\n",
              "      <th>losses</th>\n",
              "      <th>bleu_score</th>\n",
              "      <th>bleu1</th>\n",
              "      <th>bleu4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>250</td>\n",
              "      <td>182</td>\n",
              "      <td>183</td>\n",
              "      <td>2.041729</td>\n",
              "      <td>0.239512</td>\n",
              "      <td>5.604720</td>\n",
              "      <td>0.044803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>250</td>\n",
              "      <td>182</td>\n",
              "      <td>183</td>\n",
              "      <td>1.631269</td>\n",
              "      <td>0.231035</td>\n",
              "      <td>5.202312</td>\n",
              "      <td>0.043706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>250</td>\n",
              "      <td>182</td>\n",
              "      <td>183</td>\n",
              "      <td>1.307436</td>\n",
              "      <td>0.245067</td>\n",
              "      <td>5.487805</td>\n",
              "      <td>0.046642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>250</td>\n",
              "      <td>182</td>\n",
              "      <td>183</td>\n",
              "      <td>1.232020</td>\n",
              "      <td>0.240390</td>\n",
              "      <td>4.923077</td>\n",
              "      <td>0.047170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>250</td>\n",
              "      <td>182</td>\n",
              "      <td>183</td>\n",
              "      <td>1.104259</td>\n",
              "      <td>0.241590</td>\n",
              "      <td>5.182927</td>\n",
              "      <td>0.046642</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fa5c5a4-c28b-4475-bb6b-17042ea89b1c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6fa5c5a4-c28b-4475-bb6b-17042ea89b1c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6fa5c5a4-c28b-4475-bb6b-17042ea89b1c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(save_path+'DF_metrics_exp1.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abixT5OeYxzL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "924bfb91-827e-4ac1-cde0-3c9a58c9bcf4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(5, 5), dpi=80)\n",
        "x=df['Epochs']\n",
        "Loss=df['losses']\n",
        "bleu=df['bleu_score']\n",
        "bleu1=df['bleu1']\n",
        "bleu4=df['bleu4']\n",
        "max(x)\n",
        "\n",
        "plt.plot(x, Loss, \"r*-\", label='Loss', linewidth=1) # red line\n",
        "plt.plot(x, bleu, 'go-', label='bleu score', linewidth=1) \n",
        "#plt.plot(x, bleu1, 'bd-', label='bleu 1', linewidth=1) \n",
        "plt.plot(x, bleu4, 'cs-', label='bleu 4', linewidth=1) \n",
        "leg = plt.legend(loc='best', ncol=4, mode=\"expand\", shadow=True, fancybox=True)\n",
        "leg.get_frame().set_alpha(0.5)\n",
        "plt.xlabel('Epochs')\n",
        "#plt.xticks(np.arange(0, max(x)+100, 250))\n",
        "plt.ylabel('metric value')\n",
        "plt.title('Metrics of 1 experiment')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "IA025_Delivery_3_stage2_training_CNNT5_Image_Captioning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}