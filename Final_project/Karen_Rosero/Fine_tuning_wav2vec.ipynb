{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/Final_project/Karen_Rosero/Fine_tuning_wav2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce77521f",
      "metadata": {
        "id": "ce77521f"
      },
      "source": [
        "# Sound detection and classification using transformers\n",
        "## Fine-tuning notebook\n",
        "\n",
        "## Projeto final da disclipina IA025\n",
        "## Autora: Karen Rosero"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f8f3d42",
      "metadata": {
        "id": "9f8f3d42"
      },
      "source": [
        "Instalo as bibliotecas faltantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8dc46b",
      "metadata": {
        "scrolled": true,
        "id": "7d8dc46b"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e92c913",
      "metadata": {
        "scrolled": false,
        "id": "7e92c913"
      },
      "outputs": [],
      "source": [
        "#!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d19a640d",
      "metadata": {
        "id": "d19a640d"
      },
      "outputs": [],
      "source": [
        "#!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b0d63d2",
      "metadata": {
        "id": "2b0d63d2"
      },
      "outputs": [],
      "source": [
        "#!pip install nvidia-ml-py3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e2f6fb",
      "metadata": {
        "id": "13e2f6fb"
      },
      "outputs": [],
      "source": [
        "#!pip install neptune-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff79afa",
      "metadata": {
        "id": "8ff79afa"
      },
      "outputs": [],
      "source": [
        "#!pip install lightning-bolts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84d43c1",
      "metadata": {
        "id": "e84d43c1",
        "outputId": "d04297db-75a6-4fca-8159-1ab98990ff7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torchmetrics in /home/lab_acustica/.local/lib/python3.9/site-packages (0.9.1)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.8.1+cu111)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /home/lab_acustica/.local/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->torchmetrics) (2.4.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efedb371",
      "metadata": {
        "id": "efedb371"
      },
      "source": [
        "Importo as bibliotecas necessárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4aa7cfb",
      "metadata": {
        "id": "d4aa7cfb"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2Config\n",
        "import torch\n",
        "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2FeatureEncoder, Wav2Vec2NoLayerNormConvLayer, Wav2Vec2LayerNormConvLayer\n",
        "from torch import nn\n",
        "from transformers.activations import ACT2FN\n",
        "import ipywidgets \n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import torchaudio\n",
        "import pytorch_lightning as pl\n",
        "import nvidia_smi\n",
        "#from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
        "from pytorch_lightning.loggers import NeptuneLogger\n",
        "from IPython.display import display, HTML\n",
        "from dataclasses import dataclass, field\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "# usadas para as métricas\n",
        "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
        "from torchmetrics import Accuracy\n",
        "from torchmetrics import F1Score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14f61c18",
      "metadata": {
        "id": "14f61c18"
      },
      "source": [
        "GPU a ser utilizada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ee7b97",
      "metadata": {
        "scrolled": true,
        "id": "68ee7b97",
        "outputId": "180b5156-de36-425b-e6cd-20ad5a290390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Jun 29 16:46:36 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  TITAN V             Off  | 00000000:02:00.0  On |                  N/A |\n",
            "| 33%   48C    P8    30W / 250W |    367MiB / 12063MiB |      2%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1522      G   /usr/lib/xorg/Xorg                158MiB |\n",
            "|    0   N/A  N/A      2025      G   /usr/bin/gnome-shell               43MiB |\n",
            "|    0   N/A  N/A      2440      G   /usr/lib/firefox/firefox          156MiB |\n",
            "|    0   N/A  N/A      3733      G   /usr/lib/firefox/firefox            5MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe75067a",
      "metadata": {
        "id": "fe75067a",
        "outputId": "47141c3f-1553-4ed7-8e0b-e02e1b90fdba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pytorch Lightning Version: 1.6.4\n",
            "Device name: b'TITAN V'\n"
          ]
        }
      ],
      "source": [
        "print(f\"Pytorch Lightning Version: {pl.__version__}\")\n",
        "nvidia_smi.nvmlInit()\n",
        "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "print(f\"Device name: {nvidia_smi.nvmlDeviceGetName(handle)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38212c87",
      "metadata": {
        "id": "38212c87"
      },
      "source": [
        "Defino hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024ea26e",
      "metadata": {
        "id": "024ea26e",
        "outputId": "3f3a54c2-c241-4162-99c6-4281cb953a6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'version': 'wav2vec2-sound_detection_test1',\n",
              " 'lr': 1e-05,\n",
              " 'w_decay': 0,\n",
              " 'bs': 64,\n",
              " 'patience': 30,\n",
              " 'hold_epochs': 20,\n",
              " 'accum_grads': 1,\n",
              " 'pretrained': 'facebook/wav2vec2-base-960h',\n",
              " 'wav2vec2_processor': 'facebook/wav2vec2-base-960h',\n",
              " 'freeze_finetune_updates': 10000,\n",
              " 'warm_up_steps': 20000,\n",
              " 'apply_mask': True,\n",
              " 'mask_time_length': 1,\n",
              " 'max_epochs': 200}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "version = \"wav2vec2-sound_detection_test1\" #@param {type: \"string\"}\n",
        "lr = 1e-5#@param {type: \"number\"}\n",
        "w_decay = 0#@param {type: \"number\"}\n",
        "bs = 64#@param {type: \"integer\"}\n",
        "accum_grads = 1#@param {type: \"integer\"}\n",
        "patience = 30#@param {type: \"integer\"}\n",
        "max_epochs = 200#@param {type: \"integer\"}\n",
        "# warmup_steps = 1000#@param {type: \"integer\"}\n",
        "hold_epochs = 20#@param {type: \"integer\"}\n",
        "pretrained = \"facebook/wav2vec2-base-960h\"#@param {type: \"string\"}\n",
        "wav2vec2_processor = \"facebook/wav2vec2-base-960h\"#@param {type: \"string\"}\n",
        "freeze_finetune_updates = 10000#@param {type: \"integer\"}\n",
        "warm_up_steps = 20000#@param {type: \"integer\"}\n",
        "apply_mask=True#@param {type: \"boolean\"}\n",
        "mask_time_length= 1#@param {type: \"integer\"}\n",
        "\n",
        "# Define hyperparameters\n",
        "hparams = {\"version\": version,\n",
        "          \"lr\": lr,\n",
        "          \"w_decay\": w_decay,\n",
        "          \"bs\": bs,\n",
        "          \"patience\": patience,\n",
        "          \"hold_epochs\":hold_epochs,\n",
        "          \"accum_grads\": accum_grads,\n",
        "          \"pretrained\":pretrained,\n",
        "          \"wav2vec2_processor\": wav2vec2_processor,\n",
        "          \"freeze_finetune_updates\":freeze_finetune_updates,\n",
        "          \"warm_up_steps\":warm_up_steps,\n",
        "          \"apply_mask\":apply_mask, \n",
        "          \"mask_time_length\":mask_time_length, \n",
        "          \"max_epochs\": max_epochs}\n",
        "hparams"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ccbe8c7",
      "metadata": {
        "id": "1ccbe8c7"
      },
      "source": [
        "Crio o processador e modelo originais do wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b7df6f",
      "metadata": {
        "scrolled": true,
        "id": "c9b7df6f",
        "outputId": "29275514-0b99-457d-a16a-b9f9749cb92b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2Model: ['lm_head.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(hparams[\"wav2vec2_processor\"])\n",
        "model = Wav2Vec2Model.from_pretrained(hparams[\"wav2vec2_processor\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99e709cf",
      "metadata": {
        "id": "99e709cf"
      },
      "source": [
        "O modelo original recebe um canal na entrada. Isso precisa ser mudado para 4 canais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d269dd3",
      "metadata": {
        "scrolled": false,
        "id": "9d269dd3",
        "outputId": "fddb52e3-b27e-4e50-ad2b-9f69ba215447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wav2Vec2Model(\n",
            "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
            "    (conv_layers): ModuleList(\n",
            "      (0): Wav2Vec2GroupNormConvLayer(\n",
            "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (1): Wav2Vec2NoLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (2): Wav2Vec2NoLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (3): Wav2Vec2NoLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (4): Wav2Vec2NoLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (5): Wav2Vec2NoLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (6): Wav2Vec2NoLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (feature_projection): Wav2Vec2FeatureProjection(\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): Wav2Vec2Encoder(\n",
            "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
            "      (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
            "      (padding): Wav2Vec2SamePadLayer()\n",
            "      (activation): GELUActivation()\n",
            "    )\n",
            "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (6): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (7): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (8): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (9): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (10): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (11): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13ad6ea",
      "metadata": {
        "id": "b13ad6ea"
      },
      "source": [
        "### Adaptação do modelo wav2vec2 para receber e processar 4 canais de áudio na entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134e1d00",
      "metadata": {
        "id": "134e1d00"
      },
      "outputs": [],
      "source": [
        "class Wav2Vec2GroupNormConvLayer(nn.Module):\n",
        "    def __init__(self, config, layer_id=0):\n",
        "        super().__init__()\n",
        "        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 4 # define 4 canais na camada de entrada\n",
        "        self.out_conv_dim = config.conv_dim[layer_id]\n",
        "\n",
        "        self.conv = nn.Conv1d(\n",
        "            self.in_conv_dim,\n",
        "            self.out_conv_dim,\n",
        "            kernel_size=config.conv_kernel[layer_id],\n",
        "            stride=config.conv_stride[layer_id],\n",
        "            bias=config.conv_bias,\n",
        "        )\n",
        "        self.activation = ACT2FN[config.feat_extract_activation]\n",
        "\n",
        "        self.layer_norm = nn.GroupNorm(num_groups=self.out_conv_dim, num_channels=self.out_conv_dim, affine=True)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.conv(hidden_states)\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "        hidden_states = self.activation(hidden_states)\n",
        "        return hidden_states\n",
        "    \n",
        "class Wav2Vec2_4ChannelFeatureEncoder(nn.Module):\n",
        "    \"\"\"Construct the features from raw audio waveform\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        if config.feat_extract_norm == \"group\":\n",
        "            conv_layers = [Wav2Vec2GroupNormConvLayer(config, layer_id=0)] + [\n",
        "                Wav2Vec2NoLayerNormConvLayer(config, layer_id=i + 1) for i in range(config.num_feat_extract_layers - 1)\n",
        "            ]\n",
        "        elif config.feat_extract_norm == \"layer\":\n",
        "            conv_layers = [\n",
        "                Wav2Vec2LayerNormConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)\n",
        "            ]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']\"\n",
        "            )\n",
        "        self.conv_layers = nn.ModuleList(conv_layers)\n",
        "        self.gradient_checkpointing = False\n",
        "        self._requires_grad = True\n",
        "\n",
        "    def _freeze_parameters(self):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "        self._requires_grad = False\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        hidden_states = input_values[:] # mudou para que receba todos os canais (4)\n",
        "        print(\"hidden_states\", hidden_states.shape)\n",
        "\n",
        "        # make sure hidden_states require grad for gradient_checkpointing\n",
        "        if self._requires_grad and self.training:\n",
        "            hidden_states.requires_grad = True\n",
        "\n",
        "        for conv_layer in self.conv_layers:\n",
        "            if self._requires_grad and self.gradient_checkpointing and self.training:\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                hidden_states = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(conv_layer),\n",
        "                    hidden_states,\n",
        "                )\n",
        "            else:\n",
        "                hidden_states = conv_layer(hidden_states)\n",
        "\n",
        "        return hidden_states    \n",
        "\n",
        "# Crio o novo modelo que herda os processos de Wav2Vec2, mas usa o extrator de features baseado em 4 canais\n",
        "class Wav2Vec2_4ChannelModel(Wav2Vec2Model):\n",
        "    def __init__(self, config: Wav2Vec2Config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        # del self.feature_extractor\n",
        "        self.feature_extractor = Wav2Vec2_4ChannelFeatureEncoder(config)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f134a92",
      "metadata": {
        "scrolled": true,
        "id": "6f134a92",
        "outputId": "0e94ad05-3893-401d-b5ce-bb37483ccccb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2_4ChannelModel: ['lm_head.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing Wav2Vec2_4ChannelModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2_4ChannelModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2_4ChannelModel were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of Wav2Vec2_4ChannelModel were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized because the shapes did not match:\n",
            "- wav2vec2.feature_extractor.conv_layers.0.conv.weight: found shape torch.Size([512, 1, 10]) in the checkpoint and torch.Size([512, 4, 10]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model4c = Wav2Vec2_4ChannelModel.from_pretrained(\"facebook/wav2vec2-base-960h\", ignore_mismatched_sizes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1b72d62",
      "metadata": {
        "id": "d1b72d62"
      },
      "source": [
        "Agora a primeira camada convolucional do modelo recebe 4 canais como entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d24d024e",
      "metadata": {
        "id": "d24d024e",
        "outputId": "5bc79b82-cf48-47ca-aa7c-3c90a6d73d7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wav2Vec2_4ChannelModel(\n",
            "  (feature_extractor): Wav2Vec2_4ChannelFeatureEncoder(\n",
            "    (conv_layers): ModuleList(\n",
            "      (0): Wav2Vec2GroupNormConvLayer(\n",
            "        (conv): Conv1d(4, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (1): Wav2Vec2NoLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (2): Wav2Vec2NoLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (3): Wav2Vec2NoLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (4): Wav2Vec2NoLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (5): Wav2Vec2NoLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "      (6): Wav2Vec2NoLayerNormConvLayer(\n",
            "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
            "        (activation): GELUActivation()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (feature_projection): Wav2Vec2FeatureProjection(\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): Wav2Vec2Encoder(\n",
            "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
            "      (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
            "      (padding): Wav2Vec2SamePadLayer()\n",
            "      (activation): GELUActivation()\n",
            "    )\n",
            "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (6): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (7): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (8): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (9): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (10): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (11): Wav2Vec2EncoderLayer(\n",
            "        (attention): Wav2Vec2Attention(\n",
            "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Wav2Vec2FeedForward(\n",
            "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model4c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b88bdf6",
      "metadata": {
        "id": "7b88bdf6"
      },
      "source": [
        "### Dataloader da base de dados ANSYN (TUT Sound Events 2018 - Ambisonic, Anechoic and Synthetic Impulse Response Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad665e1b",
      "metadata": {
        "id": "ad665e1b"
      },
      "outputs": [],
      "source": [
        "audio_path = '/home/lab_acustica/Documentos/ANSYN_Dataset/audio_200ms_ov1/'\n",
        "labels_path = '/home/lab_acustica/Documentos/ANSYN_Dataset/label_200ms_ov1/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8d4e7a9",
      "metadata": {
        "id": "f8d4e7a9"
      },
      "outputs": [],
      "source": [
        "train_total_list = []\n",
        "X_test = []\n",
        "for file in os.listdir(audio_path):\n",
        "    if 'tra' in file:\n",
        "        train_total_list.append(file[:-4])\n",
        "    elif 'tst' in file:\n",
        "        X_test.append(file[:-4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9c94d17",
      "metadata": {
        "id": "d9c94d17"
      },
      "outputs": [],
      "source": [
        "train_set, val_set = torch.utils.data.random_split(train_total_list, [int(len(train_total_list)*0.7), int(len(train_total_list)*0.3)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd9b15a",
      "metadata": {
        "id": "abd9b15a",
        "outputId": "04bcdeb4-4d22-493e-b059-ebd6d7d74ecc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Treinamento 75600 ,  Validação  32400 ,  Teste 27000\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val = train_test_split(train_total_list,  test_size=0.3, random_state=42, shuffle = True)\n",
        "print('Treinamento',len(X_train), ',  Validação ',len(X_val), ',  Teste',len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7dfa9f",
      "metadata": {
        "id": "5b7dfa9f",
        "outputId": "67f25435-a67e-4800-a1dc-d76e5924df46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27000\n",
            "32400\n",
            "75600\n"
          ]
        }
      ],
      "source": [
        "test_classes = []\n",
        "for i in range(len(X_test)):    \n",
        "    if os.path.getsize(labels_path+str(X_test[i])+'.csv')==0:\n",
        "        test_classes.append(str(-1))\n",
        "    else:\n",
        "        label_file = csv.reader(open(labels_path+str(X_test[i])+'.csv', 'r'))\n",
        "        for count , line in enumerate(label_file):\n",
        "            test_classes.append(line[0])\n",
        "print(len(test_classes))\n",
        "\n",
        "val_classes = []\n",
        "for i in range(len(X_val)):    \n",
        "    if os.path.getsize(labels_path+X_val[i]+'.csv')==0:\n",
        "        val_classes.append(str(-1))\n",
        "    else:\n",
        "        label_file = csv.reader(open(labels_path+X_val[i]+'.csv', 'r'))\n",
        "        for count , line in enumerate(label_file):\n",
        "            val_classes.append(line[0])\n",
        "print(len(val_classes))\n",
        "\n",
        "train_classes = []\n",
        "for i in range(len(X_train)):    \n",
        "    if os.path.getsize(labels_path+X_train[i]+'.csv')==0:\n",
        "        train_classes.append(str(-1))\n",
        "    else:\n",
        "        label_file = csv.reader(open(labels_path+X_train[i]+'.csv', 'r'))\n",
        "        for count , line in enumerate(label_file):\n",
        "            train_classes.append(line[0])\n",
        "print(len(train_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48b85f69",
      "metadata": {
        "id": "48b85f69",
        "outputId": "9c6da04d-37f0-4f8d-9189-1faa20c610f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(75600, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tra_045_ov1_s3_106</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tra_165_ov1_s2_17</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tra_102_ov1_s1_139</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tra_105_ov1_s2_131</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tra_023_ov1_s3_90</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Filename Class\n",
              "0  tra_045_ov1_s3_106     4\n",
              "1   tra_165_ov1_s2_17     0\n",
              "2  tra_102_ov1_s1_139     2\n",
              "3  tra_105_ov1_s2_131     4\n",
              "4   tra_023_ov1_s3_90     0"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d = {'Filename': X_train, 'Class': train_classes}\n",
        "df_train = pd.DataFrame(data=d)\n",
        "print(df_train.shape)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66c3fc7d",
      "metadata": {
        "id": "66c3fc7d",
        "outputId": "38b493e3-9bfc-4857-b24a-1cee629d0e29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32400, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tra_065_ov1_s1_1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tra_096_ov1_s3_1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tra_126_ov1_s1_9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tra_067_ov1_s1_61</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tra_115_ov1_s3_56</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Filename Class\n",
              "0   tra_065_ov1_s1_1     1\n",
              "1   tra_096_ov1_s3_1     0\n",
              "2   tra_126_ov1_s1_9     1\n",
              "3  tra_067_ov1_s1_61     3\n",
              "4  tra_115_ov1_s3_56     1"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_val = pd.DataFrame(data={'Filename': X_val, 'Class': val_classes})\n",
        "print(df_val.shape)\n",
        "df_val.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f97b2a",
      "metadata": {
        "id": "22f97b2a",
        "outputId": "1553d4e5-fe3e-4513-c8bc-3c90166e0bfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(27000, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tst_032_ov1_s2_5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tst_051_ov1_s1_45</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tst_026_ov1_s2_64</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tst_027_ov1_s3_53</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tst_005_ov1_s2_44</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Filename Class\n",
              "0   tst_032_ov1_s2_5     7\n",
              "1  tst_051_ov1_s1_45    -1\n",
              "2  tst_026_ov1_s2_64     5\n",
              "3  tst_027_ov1_s3_53     3\n",
              "4  tst_005_ov1_s2_44     2"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test = pd.DataFrame(data={'Filename': X_test, 'Class': test_classes})\n",
        "print(df_test.shape)\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf019251",
      "metadata": {
        "id": "cf019251"
      },
      "outputs": [],
      "source": [
        "class ANSYN_Dataset_20ms(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, filenames):    \n",
        "        self.filenames = filenames\n",
        "        self.audio_path = \"/home/lab_acustica/Documentos/ANSYN_Dataset/audio_200ms_ov1/\"\n",
        "    \n",
        "    def normalize_layer(self, feats):    # função para normalizar\n",
        "        with torch.no_grad():\n",
        "            feats = torch.nn.functional.layer_norm(feats, feats.shape)\n",
        "        return feats\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        feats, _ = torchaudio.load(self.audio_path + self.filenames['Filename'][index]+'.wav')\n",
        "        feats = self.normalize_layer(feats)\n",
        "        target = torch.tensor(int(self.filenames['Class'][index])).long()\n",
        "        return {\"input_values\": feats, \"target\":target}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69954405",
      "metadata": {
        "id": "69954405"
      },
      "outputs": [],
      "source": [
        "train_dataset = ANSYN_Dataset_20ms(df_train)                           \n",
        "val_dataset =  ANSYN_Dataset_20ms(df_val)  \n",
        "test_dataset = ANSYN_Dataset_20ms(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fbb8ef3",
      "metadata": {
        "id": "5fbb8ef3",
        "outputId": "c9f53883-5c14-456c-c4fc-06b880b42644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de amostras de treinamento: 75600\n",
            "Número de amostras de validação: 32400\n",
            "Número de amostras de teste: 27000\n"
          ]
        }
      ],
      "source": [
        "print('Número de amostras de treinamento:', len(train_dataset))\n",
        "print('Número de amostras de validação:', len(val_dataset))\n",
        "print('Número de amostras de teste:', len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8ccd2dc",
      "metadata": {
        "id": "e8ccd2dc",
        "outputId": "bb741731-1900-4ccb-8e94-7c740983fb18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3200]) Class  tensor(4)\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset[0][\"input_values\"].shape, 'Class ', train_dataset[0][\"target\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b517c02",
      "metadata": {
        "id": "1b517c02",
        "outputId": "58433a8b-038d-412e-ae76-58377c238940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target sampling rate: 16000\n",
            "{'input_values': tensor([[[ 0.0263, -0.4117,  0.8765,  ..., -0.0401, -0.0114,  0.1829],\n",
            "         [-0.0035, -0.0027,  0.0431,  ..., -0.0243, -0.0223, -0.0099],\n",
            "         [-0.0161,  0.5285, -0.8872,  ...,  0.0451,  0.0300, -0.2385],\n",
            "         [ 0.0251, -0.5538,  1.1259,  ..., -0.0488, -0.0898,  0.3146]]])}\n",
            "torch.Size([1, 4, 3200])\n"
          ]
        }
      ],
      "source": [
        "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
        "print(f\"Target sampling rate: {target_sampling_rate}\")\n",
        "\n",
        "inputs = processor(train_dataset[0][\"input_values\"], sampling_rate=target_sampling_rate, return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "print(inputs[\"input_values\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66f4b10b",
      "metadata": {
        "id": "66f4b10b",
        "outputId": "3680a301-3078-4d21-812c-1c135920562f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hidden_states torch.Size([1, 4, 3200])\n",
            "torch.Size([1, 9, 768])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model4c(**inputs)\n",
        "\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "print(last_hidden_states.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "368a63e6",
      "metadata": {
        "id": "368a63e6"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"target\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        #print('batch', batch)\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=True,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "        #print('labels_batch', labels_batch)\n",
        "\n",
        "\n",
        "        labels = labels_batch[\"input_ids\"]\n",
        "\n",
        "        batch[\"target\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "897d4cdc",
      "metadata": {
        "id": "897d4cdc"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(processor=processor,\n",
        "                                        # max_length=188,\n",
        "                                        padding=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b04136c",
      "metadata": {
        "id": "7b04136c",
        "outputId": "32b26421-784b-416a-cc26-845689f82edb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de minibatches de treinamento: 1182\n",
            "Número de minibatches de validação: 507\n",
            "{'input_values': tensor([[[-2.9669e-01, -4.7600e-01, -1.6874e-01,  ...,  2.5148e-02,\n",
            "           3.3662e-02,  4.3271e-02],\n",
            "         [-3.2242e-01, -5.1529e-01, -1.3612e-01,  ..., -3.0559e-02,\n",
            "          -7.3276e-03, -4.5722e-02],\n",
            "         [ 3.0519e-01,  5.0594e-01,  2.1356e-01,  ...,  4.9681e-02,\n",
            "           2.6925e-02,  5.6713e-02],\n",
            "         [ 2.5903e-01,  3.7102e-01,  9.2295e-02,  ...,  3.6491e-02,\n",
            "           3.4278e-02,  4.6662e-02]],\n",
            "\n",
            "        [[-1.6944e-01, -4.9374e-01, -3.8197e-01,  ..., -7.5608e-01,\n",
            "          -3.9245e-01, -6.1471e-01],\n",
            "         [ 9.8331e-02,  3.1871e-01,  4.7729e-01,  ...,  3.5419e-01,\n",
            "           4.5227e-01,  3.1557e-01],\n",
            "         [-1.0776e-02, -6.8585e-01, -6.8530e-01,  ..., -8.4598e-01,\n",
            "          -5.2554e-01, -6.0332e-01],\n",
            "         [ 5.6402e-02,  4.8093e-01,  5.4826e-01,  ...,  7.9382e-01,\n",
            "           7.7527e-01,  7.2325e-01]],\n",
            "\n",
            "        [[-6.4321e-03, -5.1480e-03,  2.8070e-02,  ...,  2.7609e+00,\n",
            "           2.6654e+00,  2.5361e+00],\n",
            "         [-2.4235e-02,  8.6249e-03,  4.5101e-02,  ...,  4.4350e+00,\n",
            "           4.2604e+00,  4.0977e+00],\n",
            "         [-1.5141e-02, -3.1465e-02, -3.4342e-02,  ..., -1.6744e+00,\n",
            "          -1.5863e+00, -1.5480e+00],\n",
            "         [-2.8693e-02, -4.5977e-03, -1.3962e-02,  ...,  7.7679e-01,\n",
            "           7.5107e-01,  6.9442e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 3.5798e-01,  1.1417e-01, -3.9681e-02,  ..., -2.4370e-02,\n",
            "          -4.8527e-03, -2.0767e-02],\n",
            "         [-6.2255e-01, -2.0573e-01,  6.0065e-02,  ...,  2.9703e-02,\n",
            "           2.6886e-02,  4.0417e-02],\n",
            "         [ 1.0401e-01,  3.6078e-02, -1.9612e-02,  ...,  2.3612e-03,\n",
            "           9.6654e-03, -1.5922e-02],\n",
            "         [-2.3499e-01, -6.9083e-02,  2.0087e-02,  ..., -9.7289e-03,\n",
            "           1.9315e-02,  3.4104e-02]],\n",
            "\n",
            "        [[ 7.6950e-02,  4.3072e-02,  1.5427e-01,  ...,  2.4900e-01,\n",
            "           3.7579e-01,  5.4563e-01],\n",
            "         [ 4.3043e-02,  2.1795e-02,  6.0459e-02,  ...,  1.3003e-01,\n",
            "           2.2163e-01,  2.7609e-01],\n",
            "         [ 8.8880e-02,  3.5460e-02,  1.4294e-01,  ...,  2.0852e-01,\n",
            "           3.3962e-01,  4.7537e-01],\n",
            "         [ 1.1066e-01,  6.6498e-02,  2.0731e-01,  ...,  3.5704e-01,\n",
            "           5.3684e-01,  7.6293e-01]],\n",
            "\n",
            "        [[-1.4857e+00, -8.7040e-01, -6.0489e-01,  ...,  1.4520e-02,\n",
            "          -1.0745e-02,  2.3289e-02],\n",
            "         [ 2.5719e+00,  1.5064e+00,  1.0090e+00,  ...,  7.5924e-03,\n",
            "           9.0686e-03, -1.2826e-02],\n",
            "         [ 4.6406e-01,  3.0411e-01,  1.7042e-01,  ..., -1.0544e-02,\n",
            "          -8.7653e-03, -4.1070e-03],\n",
            "         [ 2.1246e-02,  7.3532e-03,  8.1558e-03,  ..., -1.5907e-02,\n",
            "           3.2268e-03, -1.6487e-03]]]), 'target': tensor([ 7,  2,  0,  5,  7, -1, -1, -1,  5,  5, -1,  7,  7, -1, -1,  6,  6,  6,\n",
            "         6,  2, -1,  2, -1,  1,  8,  7,  4,  6, -1, -1,  3, -1,  8,  2,  3, -1,\n",
            "        -1,  6,  6,  7,  6,  6, -1,  2,  7,  7, -1, -1, -1,  7,  8, -1,  6,  2,\n",
            "         6,  2,  3,  7, -1, -1,  2, 10,  5,  1])}\n",
            "\n",
            "Dimensões dos dados de um minibatch - Audio: torch.Size([64, 4, 3200])\n",
            "\n",
            "Dimensões dos dados de um minibatch - Target: torch.Size([64])\n",
            "Valores mínimo e máximo entrada:  tensor(-18.1862) tensor(20.4497)\n",
            "Valores mínimo e máximo saída:  tensor(-1) tensor(10)\n",
            "Tipo dos dados dos áudios:          <class 'torch.Tensor'>\n",
            "Tipo das classes das classes:        <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "batch_size = hparams[\"bs\"]\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                              collate_fn = data_collator,\n",
        "                              shuffle=True, num_workers=4)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                            collate_fn = data_collator,\n",
        "                            shuffle=False, num_workers=4)\n",
        "\n",
        "print('Número de minibatches de treinamento:', len(train_dataloader))\n",
        "print('Número de minibatches de validação:', len(val_dataloader))\n",
        "\n",
        "batch = next(iter(train_dataloader))\n",
        "print(batch)\n",
        "\n",
        "x_train, y_train = batch['input_values'], batch['target']\n",
        "print(\"\\nDimensões dos dados de um minibatch - Audio:\", x_train.size())\n",
        "# print(\"\\nDimensões dos dados de um minibatch:\", padding_mask.size())\n",
        "print(\"\\nDimensões dos dados de um minibatch - Target:\", y_train.size())\n",
        "print(\"Valores mínimo e máximo entrada: \", torch.min(x_train), torch.max(x_train))\n",
        "print(\"Valores mínimo e máximo saída: \", torch.min(y_train), torch.max(y_train))\n",
        "print(\"Tipo dos dados dos áudios:         \", type(x_train))\n",
        "print(\"Tipo das classes das classes:       \", type(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6847352d",
      "metadata": {
        "id": "6847352d"
      },
      "source": [
        "Defino as métricas que serão usadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6416502",
      "metadata": {
        "id": "e6416502"
      },
      "outputs": [],
      "source": [
        "n_classes = 12\n",
        "f1 = F1Score(num_classes=n_classes, average='micro')\n",
        "accuracy = Accuracy(num_classes=n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f640656b",
      "metadata": {
        "id": "f640656b"
      },
      "source": [
        "### Criando o  lightning module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39995651",
      "metadata": {
        "id": "39995651"
      },
      "outputs": [],
      "source": [
        "class Wav2Vec2_sound_detection(pl.LightningModule):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hparams.update(hparams)\n",
        "\n",
        "        self.freeze_finetune_updates = hparams[\"freeze_finetune_updates\"]\n",
        "        \n",
        "        self.model = model4c\n",
        "        #self.model = Wav2Vec2_sound_detection.from_pretrained(hparams[\"pretrained\"],\n",
        "                                                 #conv_stride=[5,1,1,1,1,1,1],\n",
        "                                                 #apply_spec_augment=hparams[\"apply_mask\"],\n",
        "                                                 #mask_time_length=hparams[\"mask_time_length\"])\n",
        "        # self.model.feature_extractor._freeze_parameters()\n",
        "        \n",
        "        # freeze base-model\n",
        "        # for param in self.model.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        \n",
        "        self.projector = nn.Linear(self.model.config.hidden_size, self.model.config.classifier_proj_size)\n",
        "        self.final_layer = nn.Linear(self.model.config.classifier_proj_size, n_classes)    \n",
        "    \n",
        "    def forward(self, samples):\n",
        "        \n",
        "        # print(\"input_values\", samples[\"input_values\"].shape)\n",
        "        # print(\"attention_mask\", samples[\"attention_mask\"].shape)\n",
        "\n",
        "        ft = self.freeze_finetune_updates <= self.trainer.global_step\n",
        "       \n",
        "        with torch.no_grad() if not ft else contextlib.ExitStack():\n",
        "              hidden_states = self.model(**samples).last_hidden_state\n",
        "        \n",
        "        # print('hidden_states', hidden_states.shape)\n",
        "        # print('hidden_states squeeze', hidden_states[:,0,:].shape)\n",
        "\n",
        "        padding_mask = self.model._get_feature_vector_attention_mask(hidden_states.shape[1], samples[\"attention_mask\"])\n",
        "        # print(\"padding_mask\", padding_mask.shape)\n",
        "\n",
        "        hidden_states[~padding_mask] = 0.0\n",
        "        \n",
        "        pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n",
        "\n",
        "        # print(\"pooled_output\", pooled_output.shape)\n",
        "\n",
        "        proj_pooled = self.projector(pooled_output)\n",
        "        \n",
        "        # print(\"proj_pooled\", proj_pooled.shape)\n",
        "\n",
        "        preds = self.final_layer(proj_pooled)\n",
        "        \n",
        "        return F.log_softmax(preds, dim=1)\n",
        "    \n",
        "    def _get_feature_vector_attention_mask(self, feature_vector_length: int, attention_mask: torch.LongTensor):\n",
        "        output_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n",
        "        batch_size = attention_mask.shape[0]\n",
        "\n",
        "        attention_mask = torch.zeros(\n",
        "            (batch_size, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device\n",
        "        )\n",
        "        # these two operations makes sure that all values before the output lengths idxs are attended to\n",
        "        attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1\n",
        "        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n",
        "        return attention_mask\n",
        "\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        \n",
        "        y_value = train_batch.pop(\"target\")\n",
        "        log_softs = self.forward(train_batch)\n",
        "\n",
        "        # print('train preds:', preds) \n",
        "        # print('train y_value:', y_value)\n",
        "\n",
        "        # loss mse or rmspe compute\n",
        "        # mse_loss = F.mse_loss(preds, y_value)\n",
        "        loss = F.nll_loss(log_softs, y_value)\n",
        "\n",
        "        # print('train mse', mse_loss)\n",
        "        # print('#########################')\n",
        "\n",
        "        # self.log('mse_loss_step', loss, on_step=True, prog_bar=True)\n",
        "        self.log('loss_step', loss, on_step=True, prog_bar=True)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        loss = torch.stack([x['loss'] for x in outputs]).mean()       \n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "  \n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        \n",
        "        y_value = val_batch.pop(\"target\")\n",
        "\n",
        "        # predict \n",
        "        log_softs = self.forward(val_batch)\n",
        "        preds = torch.argmax(log_softs, dim=1) \n",
        "        \n",
        "        # print('log_softs:', log_softs.shape) \n",
        "        # print('preds:', preds.shape) \n",
        "        # print('y_value:', y_value.shape)\n",
        "        \n",
        "        # print('val preds:', preds) \n",
        "        # print('val y_value:', y_value)       \n",
        "\n",
        "        val_acc = accuracy(preds.cpu(), y_value.cpu())\n",
        "        val_f1 = f1(preds.cpu(), y_value.cpu())\n",
        "        val_loss = F.nll_loss(log_softs, y_value)\n",
        "\n",
        "        # print('val mse', mse_loss)\n",
        "        # print('val rmspe', rmspe_loss)\n",
        "        # print('----------------------')\n",
        "\n",
        "        self.log('val_acc', val_acc, prog_bar=True)\n",
        "        self.log('val_f1', val_f1, prog_bar=True)\n",
        "        self.log('val_loss', val_loss, prog_bar=True)\n",
        "\n",
        "        return {\"val_acc_step\": val_acc, \"val_f1_step\": val_f1, \"val_loss_step\": val_loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        acc_mean = torch.stack([x['val_acc_step'] for x in outputs]).mean()\n",
        "        f1_mean = torch.stack([x['val_f1_step'] for x in outputs]).mean()\n",
        "        loss_mean = torch.stack([x['val_loss_step'] for x in outputs]).mean()\n",
        "\n",
        "        self.log(\"val_acc\", acc_mean, prog_bar=True)\n",
        "        self.log(\"val_f1\", f1_mean, prog_bar=True)\n",
        "        self.log(\"val_loss\", loss_mean, prog_bar=True)\n",
        "  \n",
        "    def test_step(self, test_batch, batch_idx):\n",
        "        \n",
        "        y_value = test_batch.pop(\"target\")\n",
        "\n",
        "        # predict \n",
        "        log_softs = self.forward(test_batch)\n",
        "        preds = torch.argmax(log_softs, dim=1) \n",
        "\n",
        "        test_acc = accuracy(preds.cpu(), y_value.cpu())\n",
        "        test_f1 = f1(preds.cpu(), y_value.cpu())\n",
        "        test_loss = F.nll_loss(log_softs, y_value)\n",
        "        \n",
        "        self.log('test_acc', test_acc, prog_bar=True)\n",
        "        self.log('test_f1', test_f1, prog_bar=True)\n",
        "        self.log('test_loss', test_loss, prog_bar=True)\n",
        "\n",
        "        return {\"test_acc_step\": test_acc, \"test_f1_step\": test_f1,  \"test_loss_step\": test_loss}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        acc_mean = torch.stack([x['test_acc_step'] for x in outputs]).mean()\n",
        "        f1_mean = torch.stack([x['test_f1_step'] for x in outputs]).mean()\n",
        "        loss_mean = torch.stack([x['test_loss_step'] for x in outputs]).mean()\n",
        "\n",
        "        self.log(\"test_acc\", acc_mean, prog_bar=True)\n",
        "        self.log(\"test_f1\", f1_mean, prog_bar=True)\n",
        "        self.log(\"test_loss\", loss_mean, prog_bar=True)\n",
        "    \n",
        "    ## learning rate warm-up\n",
        "    def optimizer_step(\n",
        "        self,\n",
        "        epoch,\n",
        "        batch_idx,\n",
        "        optimizer,\n",
        "        optimizer_idx,\n",
        "        optimizer_closure,\n",
        "        on_tpu=False,\n",
        "        using_native_amp=False,\n",
        "        using_lbfgs=False,\n",
        "    ):\n",
        "        # skip the first 2000 steps\n",
        "        if self.trainer.global_step < hparams[\"warm_up_steps\"]:\n",
        "            lr_scale = min(1.0, float(self.trainer.global_step + 1) / hparams[\"warm_up_steps\"])\n",
        "            for pg in optimizer.param_groups:\n",
        "                pg[\"lr\"] = lr_scale * self.hparams[\"lr\"]\n",
        "\n",
        "        # update params\n",
        "        optimizer.step(closure=optimizer_closure)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.parameters(),\n",
        "                         lr=self.hparams[\"lr\"],\n",
        "                         betas=(0.9,0.98),\n",
        "                         eps=1e-6,\n",
        "                         weight_decay=self.hparams[\"w_decay\"])\n",
        "        \n",
        "        scheduler = LinearWarmupCosineAnnealingLR(optimizer, \n",
        "                                                  eta_min=0, # final-lr\n",
        "                                                  warmup_start_lr=self.hparams[\"lr\"],\n",
        "                                                  warmup_epochs=self.hparams[\"hold_epochs\"], # hold_epochs\n",
        "                                                  max_epochs=self.hparams[\"max_epochs\"])\n",
        "        \n",
        "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Fine-tuning wav2vec.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}