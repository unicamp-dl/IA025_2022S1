{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "nav_menu": {
        "height": "318px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Pedro Gengo - Aula 5 - Exercício ",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex05/Pedro%20Gengo/Pedro_Gengo_Aula_5_Exerc%C3%ADcio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = 'Pedro Gabriel Gengo Lourenço'\n",
        "\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "id": "CdORg7oe68oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04e5b73-3196-4383-bb2a-fe2de6667e15"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Pedro Gabriel Gengo Lourenço\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkfGTqMVQT1u"
      },
      "source": [
        "Este exercicío consiste em treinar no MNIST um modelo de duas camadas, sendo a primeira uma camada convolucional e a segunda uma camada linear de classificação.\n",
        "\n",
        "Não podemos usar as funções torch.nn.Conv{1,2,3}d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFNf4RPxQT1w"
      },
      "source": [
        "## Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T14:08:14.033692Z",
          "start_time": "2018-08-21T14:08:11.179981Z"
        },
        "id": "-fLUSHaCQT1x"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixando as seeds"
      ],
      "metadata": {
        "id": "7_Po22b5ykhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "id": "y-7WWWgLyoRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168015d0-885a-4818-bde5-858f15a342fe"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f657032e3b0>"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define pesos iniciais"
      ],
      "metadata": {
        "id": "fzurMVpHxcNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels = 1\n",
        "out_channels = 2\n",
        "kernel_size = 5\n",
        "stride = 3\n",
        "\n",
        "# Input image size\n",
        "height_in = 28  \n",
        "width_in = 28\n",
        "\n",
        "# Image size after the first convolutional layer.\n",
        "height_out = (height_in - kernel_size) // stride + 1\n",
        "width_out = (width_in - kernel_size) // stride + 1\n",
        "\n",
        "initial_conv_weight = torch.FloatTensor(out_channels, in_channels, kernel_size, kernel_size).uniform_(-0.01, 0.01)\n",
        "initial_conv_bias = torch.FloatTensor(out_channels,).uniform_(-0.01, 0.01)\n",
        "\n",
        "initial_classification_weight = torch.FloatTensor(10, out_channels * height_out * width_out).uniform_(-0.01, 0.01)\n",
        "initial_classification_bias = torch.FloatTensor(10,).uniform_(-0.01, 0.01)"
      ],
      "metadata": {
        "id": "9a6jQJLLlfF3"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEMUsfJpQT11"
      },
      "source": [
        "## Dataset e dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHoQjDs_QT12"
      },
      "source": [
        "### Definição do tamanho do minibatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T14:08:20.282474Z",
          "start_time": "2018-08-21T14:08:20.275450Z"
        },
        "id": "tEQYUr4TQT13"
      },
      "source": [
        "batch_size = 50"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc7Rv_2BQT16"
      },
      "source": [
        "### Carregamento, criação dataset e do dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T14:10:45.430605Z",
          "start_time": "2018-08-21T14:10:04.953051Z"
        },
        "id": "G0dEKCn-QT17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc88cf7-8215-43d4-ac7d-f40fa84d0ffc"
      },
      "source": [
        "dataset_dir = '../data/'\n",
        "\n",
        "dataset_train_full = MNIST(dataset_dir, train=True, download=True,\n",
        "                           transform=torchvision.transforms.ToTensor())\n",
        "print(dataset_train_full.data.shape)\n",
        "print(dataset_train_full.targets.shape)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([60000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rOy9ntrQT2D"
      },
      "source": [
        "### Usando apenas 1000 amostras do MNIST\n",
        "\n",
        "Neste exercício utilizaremos 1000 amostras de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNF2XjLBWWe7"
      },
      "source": [
        "indices = torch.randperm(len(dataset_train_full))[:1000]\n",
        "dataset_train = torch.utils.data.Subset(dataset_train_full, indices)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define os pesos iniciais"
      ],
      "metadata": {
        "id": "wYqj_oeSliYj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSNLD2JyA2e-"
      },
      "source": [
        "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T13:30:35.209157Z",
          "start_time": "2018-08-21T13:30:34.757103Z"
        },
        "id": "w52KGYlIQT2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8355738d-07c2-4d8a-97b6-ac4b74b211cf"
      },
      "source": [
        "print('Número de minibatches de trenamento:', len(loader_train))\n",
        "\n",
        "x_train, y_train = next(iter(loader_train))\n",
        "print(\"\\nDimensões dos dados de um minibatch:\", x_train.size())\n",
        "print(\"Valores mínimo e máximo dos pixels: \", torch.min(x_train), torch.max(x_train))\n",
        "print(\"Tipo dos dados das imagens:         \", type(x_train))\n",
        "print(\"Tipo das classes das imagens:       \", type(y_train))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de minibatches de trenamento: 20\n",
            "\n",
            "Dimensões dos dados de um minibatch: torch.Size([50, 1, 28, 28])\n",
            "Valores mínimo e máximo dos pixels:  tensor(0.) tensor(1.)\n",
            "Tipo dos dados das imagens:          <class 'torch.Tensor'>\n",
            "Tipo das classes das imagens:        <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Camada Convolucional"
      ],
      "metadata": {
        "id": "dfU_v7aPfq40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyConv2d(torch.nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int):\n",
        "        super(MyConv2d, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size  # The same for height and width.\n",
        "        self.stride = stride  # The same for height and width.\n",
        "        self.weight = torch.nn.Parameter(torch.FloatTensor(out_channels, in_channels, kernel_size, kernel_size).uniform_(-0.01, 0.01))\n",
        "        self.bias = torch.nn.Parameter(torch.FloatTensor(out_channels,).uniform_(-0.01, 0.01))\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.dim() == 4, f'x must have 4 dimensions: {x.shape}'\n",
        "\n",
        "        # Definindo os limites da imagem, ou seja, o pixel em que comeco e o pixel em que termino, para altura e largura\n",
        "        # Exemplo: uma imagem 28x28, com um kernel de 3x3, irá gerar uma imagem de 26x26 como saída, pois perdemos alguns pixels na borda\n",
        "        start_width = 0 + self.kernel_size // 2\n",
        "        end_width = x.shape[2] - (self.kernel_size - start_width - 1)\n",
        "        start_height = 0 + self.kernel_size // 2\n",
        "        end_height = x.shape[3] - (self.kernel_size - start_height - 1)\n",
        "\n",
        "        # Essa será a imagem de saída. Irei popular ela a medida que calcular os valores de cada pixel\n",
        "        # Em seu tamanho, levo em conta o stride\n",
        "        valid_image = torch.zeros((x.shape[0], self.out_channels, (end_width - start_width) // self.stride, (end_height - start_height) // self.stride))\n",
        "\n",
        "        # Loop nos filtros\n",
        "        for idx_filter, filter_bias in enumerate(zip(self.weight, self.bias)): # Andando com passo do stride\n",
        "          filter, bias = filter_bias\n",
        "          idx_valid_image_i = 0 # Contador criado para usar na imagem que irei retornar, para preservar os indices corretos\n",
        "          for i in range(start_width, end_width, self.stride):\n",
        "            idx_valid_image_j = 0 # Contador criado para usar na imagem que irei retornar, para preservar os indices corretos\n",
        "            for j in range(start_height, end_height, self.stride): # Andando com passo do stride\n",
        "\n",
        "              # Encontro a janela da imagem original referente ao pixel x da imagem de saida. Essa janela tem tamanho do kernel\n",
        "              window = x[\n",
        "                         :,\n",
        "                         :,\n",
        "                         i - self.kernel_size // 2: i + (self.kernel_size - (self.kernel_size // 2) - 1) + 1,\n",
        "                         j - self.kernel_size // 2: j + (self.kernel_size - (self.kernel_size // 2) - 1) + 1\n",
        "                        ]\n",
        "\n",
        "              # Faço o cálculo do novo pixel, sendo: janela * filtro + bias\n",
        "              valid_image[:, idx_filter, idx_valid_image_i, idx_valid_image_j] = torch.sum(window * filter, dim=(1, 2, 3)) + bias\n",
        "              idx_valid_image_j += 1\n",
        "            idx_valid_image_i += 1\n",
        "        out = valid_image\n",
        "        return out"
      ],
      "metadata": {
        "id": "wRtpLJSFfsf8"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vIp6Lx5YONF",
        "outputId": "cc59c345-0001-41fe-cfee-0b8cacadeb07"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(x, (1, 2, 3)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC62a70kYO-Z",
        "outputId": "bef064f9-8a34-43b1-f117-211f02253eb1"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1])"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare se sua implementação está igual à do pytorch usando um exemplo simples"
      ],
      "metadata": {
        "id": "ROizI33sqE79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels_dummy = 1\n",
        "out_channels_dummy = 1\n",
        "kernel_size_dummy = 2\n",
        "stride_dummy = 1\n",
        "\n",
        "conv_layer = MyConv2d(in_channels=in_channels_dummy, out_channels=out_channels_dummy, kernel_size=kernel_size_dummy, stride=stride_dummy)\n",
        "pytorch_conv_layer = torch.nn.Conv2d(in_channels=in_channels_dummy, out_channels=out_channels_dummy, kernel_size=kernel_size_dummy, stride=stride_dummy, padding=0)\n",
        "\n",
        "# Usa os mesmos pesos para minha implementação e a do pytorch\n",
        "initial_weights_dummy = torch.arange(in_channels_dummy * out_channels_dummy * kernel_size_dummy * kernel_size_dummy).float()\n",
        "initial_weights_dummy = initial_weights_dummy.reshape(in_channels_dummy, out_channels_dummy, kernel_size_dummy, kernel_size_dummy)\n",
        "initial_bias_dummy = torch.arange(out_channels_dummy,).float()\n",
        "\n",
        "conv_layer.weight.data = initial_weights_dummy\n",
        "conv_layer.bias.data = initial_bias_dummy\n",
        "pytorch_conv_layer.load_state_dict(dict(weight=initial_weights_dummy, bias=initial_bias_dummy))\n",
        "\n",
        "x = torch.arange(30).float().reshape(1, 1, 5, 6)\n",
        "\n",
        "out = conv_layer(x)\n",
        "print(out)\n",
        "target_out = pytorch_conv_layer(x)\n",
        "print(target_out)\n",
        "\n",
        "assert torch.allclose(out, target_out, atol=1e-6)"
      ],
      "metadata": {
        "id": "i1TuxWbkqMJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d02bba-1ec9-4292-91f4-ef2f15709a11"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 34.,  40.,  46.,  52.,  58.],\n",
            "          [ 70.,  76.,  82.,  88.,  94.],\n",
            "          [106., 112., 118., 124., 130.],\n",
            "          [142., 148., 154., 160., 166.]]]], grad_fn=<CopySlices>)\n",
            "tensor([[[[ 34.,  40.,  46.,  52.,  58.],\n",
            "          [ 70.,  76.,  82.,  88.,  94.],\n",
            "          [106., 112., 118., 124., 130.],\n",
            "          [142., 148., 154., 160., 166.]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare se sua implementação está igual à do pytorch usando um exemplo aleatório"
      ],
      "metadata": {
        "id": "_75UnRhdd_MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2, in_channels, height_in, width_in)\n",
        "\n",
        "conv_layer = MyConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "pytorch_conv_layer = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=0)\n",
        "\n",
        "# Usa os mesmos pesos para minha implementação e a do pytorch\n",
        "conv_layer.weight.data = initial_conv_weight\n",
        "conv_layer.bias.data = initial_conv_bias\n",
        "pytorch_conv_layer.load_state_dict(dict(weight=initial_conv_weight, bias=initial_conv_bias))\n",
        "\n",
        "out = conv_layer(x)\n",
        "print(out.shape)\n",
        "target_out = pytorch_conv_layer(x)\n",
        "print(target_out.shape)\n",
        "\n",
        "assert torch.allclose(out, target_out, atol=1e-6)"
      ],
      "metadata": {
        "id": "HzIjuGpWlbIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e464c4e7-9f75-4bd7-8c43-9fc4c361ef8c"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 8, 8])\n",
            "torch.Size([2, 2, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQA9Zg7GQT2G"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:10.802708Z",
          "start_time": "2018-08-20T21:03:10.793287Z"
        },
        "id": "_8Eg4h_kQT2H"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, height_in: int, width_in: int, in_channels: int, out_channels: int, kernel_size: int, stride: int):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv_layer = MyConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "   \n",
        "        height_out = (height_in - kernel_size - 1) // stride + 1\n",
        "        width_out = (width_in - kernel_size - 1) // stride + 1\n",
        "        self.classification_layer = torch.nn.Linear(out_channels * height_out * width_out, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.conv_layer(x)\n",
        "        hidden = torch.nn.functional.relu(hidden)\n",
        "        hidden = hidden.reshape(x.shape[0], -1)\n",
        "        logits = self.classification_layer(hidden)\n",
        "        return logits"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NHQB4wGQT2K"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqs2JhJoQT2L"
      },
      "source": [
        "### Definição dos hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:14.146259Z",
          "start_time": "2018-08-20T21:03:14.139515Z"
        },
        "id": "oZuYEkn_QT2M"
      },
      "source": [
        "n_epochs = 50\n",
        "lr = 0.1"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXarXeIQT2O"
      },
      "source": [
        "### Laço de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:40.796410Z",
          "start_time": "2018-08-20T21:03:39.771981Z"
        },
        "id": "L5T_jZZPQT2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238121c7-93df-4d91-ce95-c51f16e680b9"
      },
      "source": [
        "model = Net(height_in=height_in, width_in=width_in, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "\n",
        "# Usa pesos iniciais pré-difinidos\n",
        "model.classification_layer.load_state_dict(dict(weight=initial_classification_weight, bias=initial_classification_bias))\n",
        "model.conv_layer.weight.data = initial_conv_weight\n",
        "model.conv_layer.bias.data = initial_conv_bias\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "epochs = []\n",
        "loss_history = []\n",
        "loss_epoch_end = []\n",
        "total_trained_samples = 0\n",
        "for i in range(n_epochs):\n",
        "    for x_train, y_train in loader_train:\n",
        "        # predict da rede\n",
        "        outputs = model(x_train)\n",
        "\n",
        "        # calcula a perda\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # zero, backpropagation, ajusta parâmetros pelo gradiente descendente\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_trained_samples += x_train.size(0)\n",
        "        epochs.append(total_trained_samples / len(dataset_train))\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "    loss_epoch_end.append(loss.item())\n",
        "    print(f'Epoch: {i:d}/{n_epochs - 1:d} Loss: {loss.item()}')\n"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/49 Loss: 2.303267478942871\n",
            "Epoch: 1/49 Loss: 2.227701187133789\n",
            "Epoch: 2/49 Loss: 1.0923893451690674\n",
            "Epoch: 3/49 Loss: 0.5867355465888977\n",
            "Epoch: 4/49 Loss: 0.5144088864326477\n",
            "Epoch: 5/49 Loss: 0.4502663016319275\n",
            "Epoch: 6/49 Loss: 0.4075140357017517\n",
            "Epoch: 7/49 Loss: 0.37713873386383057\n",
            "Epoch: 8/49 Loss: 0.35344862937927246\n",
            "Epoch: 9/49 Loss: 0.3341451585292816\n",
            "Epoch: 10/49 Loss: 0.3181139826774597\n",
            "Epoch: 11/49 Loss: 0.30457887053489685\n",
            "Epoch: 12/49 Loss: 0.292834997177124\n",
            "Epoch: 13/49 Loss: 0.28276076912879944\n",
            "Epoch: 14/49 Loss: 0.27383318543434143\n",
            "Epoch: 15/49 Loss: 0.2657742202281952\n",
            "Epoch: 16/49 Loss: 0.2583288252353668\n",
            "Epoch: 17/49 Loss: 0.2511751055717468\n",
            "Epoch: 18/49 Loss: 0.24439717829227448\n",
            "Epoch: 19/49 Loss: 0.23789966106414795\n",
            "Epoch: 20/49 Loss: 0.2316770702600479\n",
            "Epoch: 21/49 Loss: 0.22562645375728607\n",
            "Epoch: 22/49 Loss: 0.21984529495239258\n",
            "Epoch: 23/49 Loss: 0.2142912745475769\n",
            "Epoch: 24/49 Loss: 0.2089422196149826\n",
            "Epoch: 25/49 Loss: 0.203872948884964\n",
            "Epoch: 26/49 Loss: 0.19903425872325897\n",
            "Epoch: 27/49 Loss: 0.1943996250629425\n",
            "Epoch: 28/49 Loss: 0.18994112312793732\n",
            "Epoch: 29/49 Loss: 0.1856399029493332\n",
            "Epoch: 30/49 Loss: 0.18147501349449158\n",
            "Epoch: 31/49 Loss: 0.17744922637939453\n",
            "Epoch: 32/49 Loss: 0.17347262799739838\n",
            "Epoch: 33/49 Loss: 0.1694747358560562\n",
            "Epoch: 34/49 Loss: 0.16547317802906036\n",
            "Epoch: 35/49 Loss: 0.16150495409965515\n",
            "Epoch: 36/49 Loss: 0.15746407210826874\n",
            "Epoch: 37/49 Loss: 0.1534043401479721\n",
            "Epoch: 38/49 Loss: 0.14926917850971222\n",
            "Epoch: 39/49 Loss: 0.14520631730556488\n",
            "Epoch: 40/49 Loss: 0.14123666286468506\n",
            "Epoch: 41/49 Loss: 0.1371268630027771\n",
            "Epoch: 42/49 Loss: 0.1331036537885666\n",
            "Epoch: 43/49 Loss: 0.12914671003818512\n",
            "Epoch: 44/49 Loss: 0.125150665640831\n",
            "Epoch: 45/49 Loss: 0.12116766721010208\n",
            "Epoch: 46/49 Loss: 0.11731737107038498\n",
            "Epoch: 47/49 Loss: 0.1136462464928627\n",
            "Epoch: 48/49 Loss: 0.1100190058350563\n",
            "Epoch: 49/49 Loss: 0.10655996203422546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLL-GQlKQT2Y"
      },
      "source": [
        "### Visualização usual da perda, somente no final de cada minibatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:55.246851Z",
          "start_time": "2018-08-20T21:03:54.994428Z"
        },
        "id": "w38EtNxhQT2Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "5a0ea48a-b017-4184-8b3f-532cfa0d5dea"
      },
      "source": [
        "n_batches_train = len(loader_train)\n",
        "plt.plot(epochs[::n_batches_train], loss_history[::n_batches_train])\n",
        "plt.xlabel('época')"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'época')"
            ]
          },
          "metadata": {},
          "execution_count": 208
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_epoch_end"
      ],
      "metadata": {
        "id": "ToktJu4CK94z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aceda88f-a271-4ec7-c4e3-9794413a0f0e"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.303267478942871,\n",
              " 2.227701187133789,\n",
              " 1.0923893451690674,\n",
              " 0.5867355465888977,\n",
              " 0.5144088864326477,\n",
              " 0.4502663016319275,\n",
              " 0.4075140357017517,\n",
              " 0.37713873386383057,\n",
              " 0.35344862937927246,\n",
              " 0.3341451585292816,\n",
              " 0.3181139826774597,\n",
              " 0.30457887053489685,\n",
              " 0.292834997177124,\n",
              " 0.28276076912879944,\n",
              " 0.27383318543434143,\n",
              " 0.2657742202281952,\n",
              " 0.2583288252353668,\n",
              " 0.2511751055717468,\n",
              " 0.24439717829227448,\n",
              " 0.23789966106414795,\n",
              " 0.2316770702600479,\n",
              " 0.22562645375728607,\n",
              " 0.21984529495239258,\n",
              " 0.2142912745475769,\n",
              " 0.2089422196149826,\n",
              " 0.203872948884964,\n",
              " 0.19903425872325897,\n",
              " 0.1943996250629425,\n",
              " 0.18994112312793732,\n",
              " 0.1856399029493332,\n",
              " 0.18147501349449158,\n",
              " 0.17744922637939453,\n",
              " 0.17347262799739838,\n",
              " 0.1694747358560562,\n",
              " 0.16547317802906036,\n",
              " 0.16150495409965515,\n",
              " 0.15746407210826874,\n",
              " 0.1534043401479721,\n",
              " 0.14926917850971222,\n",
              " 0.14520631730556488,\n",
              " 0.14123666286468506,\n",
              " 0.1371268630027771,\n",
              " 0.1331036537885666,\n",
              " 0.12914671003818512,\n",
              " 0.125150665640831,\n",
              " 0.12116766721010208,\n",
              " 0.11731737107038498,\n",
              " 0.1136462464928627,\n",
              " 0.1100190058350563,\n",
              " 0.10655996203422546]"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:48.786969Z",
          "start_time": "2018-08-20T21:03:48.781787Z"
        },
        "id": "PiuMsjYtQT2R"
      },
      "source": [
        "# Assert do histórico de losses\n",
        "target_loss_epoch_end = np.array([\n",
        "    2.303267478942871,\n",
        "    2.227701187133789,\n",
        "    1.0923893451690674,\n",
        "    0.5867354869842529,\n",
        "    0.5144089460372925,\n",
        "    0.45026642084121704,\n",
        "    0.4075140357017517,\n",
        "    0.37713879346847534,\n",
        "    0.3534485101699829,\n",
        "    0.3341451585292816,\n",
        "    0.3181140422821045,\n",
        "    0.30457887053489685,\n",
        "    0.29283496737480164,\n",
        "    0.2827608287334442,\n",
        "    0.2738332152366638,\n",
        "    0.2657742500305176,\n",
        "    0.2583288848400116,\n",
        "    0.25117507576942444,\n",
        "    0.24439716339111328,\n",
        "    0.23789969086647034,\n",
        "    0.23167723417282104,\n",
        "    0.22562651336193085,\n",
        "    0.21984536945819855,\n",
        "    0.2142913043498993,\n",
        "    0.20894232392311096,\n",
        "    0.203872948884964,\n",
        "    0.19903430342674255,\n",
        "    0.19439971446990967,\n",
        "    0.18994088470935822,\n",
        "    0.18563991785049438,\n",
        "    0.18147490918636322,\n",
        "    0.17744913697242737,\n",
        "    0.17347246408462524,\n",
        "    0.16947467625141144,\n",
        "    0.16547319293022156,\n",
        "    0.16150487959384918,\n",
        "    0.1574639081954956,\n",
        "    0.1534043848514557,\n",
        "    0.14926929771900177,\n",
        "    0.1452063024044037,\n",
        "    0.1412365883588791,\n",
        "    0.13712672889232635,\n",
        "    0.1331038922071457,\n",
        "    0.1291467249393463,\n",
        "    0.1251506358385086,\n",
        "    0.12116757035255432,\n",
        "    0.11731722950935364,\n",
        "    0.11364627629518509,\n",
        "    0.11001908034086227,\n",
        "    0.10655981302261353])\n",
        "\n",
        "assert np.allclose(np.array(loss_epoch_end), target_loss_epoch_end, atol=1e-6)"
      ],
      "execution_count": 210,
      "outputs": []
    }
  ]
}