{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "nav_menu": {
        "height": "318px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "IA025_CNN_Aula 5_Alexander_Valle_ - Exercício - Template",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex05/Alexander_Valle/IA025_CNN_Aula_5_Alexander_Valle__Exerc%C3%ADcio_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = 'Rolan Alexander Valle Rey Sánchez'\n",
        "print(f'Meu nome é {nome}')\n",
        "ra='230254'\n",
        "print(f'Meu ra é {ra}')"
      ],
      "metadata": {
        "id": "CdORg7oe68oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5655c675-e54f-4b8b-ff79-742d61ab0da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Rolan Alexander Valle Rey Sánchez\n",
            "Meu ra é 230254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkfGTqMVQT1u"
      },
      "source": [
        "Este exercicío consiste em treinar no MNIST um modelo de umas camadas, sendo a primeira uma camada convolucional e a segunda uma camada linear de classificação.\n",
        "\n",
        "Não podemos usar as funções torch.nn.Conv{1,2,3}d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFNf4RPxQT1w"
      },
      "source": [
        "## Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T14:08:14.033692Z",
          "start_time": "2018-08-21T14:08:11.179981Z"
        },
        "id": "-fLUSHaCQT1x"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixando as seeds"
      ],
      "metadata": {
        "id": "7_Po22b5ykhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-7WWWgLyoRq",
        "outputId": "5bd0b33b-0f37-435e-ca58-aa36d5c21504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f43362ae7f0>"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define pesos iniciais"
      ],
      "metadata": {
        "id": "fzurMVpHxcNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels = 1\n",
        "out_channels = 2\n",
        "kernel_size = 5\n",
        "stride = 3\n",
        "\n",
        "# Input image size\n",
        "height_in = 28  \n",
        "width_in = 28\n",
        "\n",
        "# Image size after the first convolutional layer.\n",
        "height_out = (height_in - kernel_size) // stride + 1\n",
        "width_out = (width_in - kernel_size) // stride + 1\n",
        "\n",
        "initial_conv_weight = torch.FloatTensor(out_channels, in_channels, kernel_size, kernel_size).uniform_(-0.01, 0.01)\n",
        "initial_conv_bias = torch.FloatTensor(out_channels,).uniform_(-0.01, 0.01)\n",
        "\n",
        "initial_classification_weight = torch.FloatTensor(10, out_channels * height_out * width_out).uniform_(-0.01, 0.01)\n",
        "initial_classification_bias = torch.FloatTensor(10,).uniform_(-0.01, 0.01)"
      ],
      "metadata": {
        "id": "9a6jQJLLlfF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEMUsfJpQT11"
      },
      "source": [
        "## Dataset e dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHoQjDs_QT12"
      },
      "source": [
        "### Definição do tamanho do minibatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T14:08:20.282474Z",
          "start_time": "2018-08-21T14:08:20.275450Z"
        },
        "id": "tEQYUr4TQT13"
      },
      "source": [
        "batch_size = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc7Rv_2BQT16"
      },
      "source": [
        "### Carregamento, criação dataset e do dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T14:10:45.430605Z",
          "start_time": "2018-08-21T14:10:04.953051Z"
        },
        "id": "G0dEKCn-QT17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da9f97dd-4bf9-46a6-9203-ecee527f823d"
      },
      "source": [
        "dataset_dir = '../data/'\n",
        "\n",
        "dataset_train_full = MNIST(dataset_dir, train=True, download=True,\n",
        "                           transform=torchvision.transforms.ToTensor())\n",
        "print(dataset_train_full.data.shape)\n",
        "print(dataset_train_full.targets.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([60000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rOy9ntrQT2D"
      },
      "source": [
        "### Usando apenas 1000 amostras do MNIST\n",
        "\n",
        "Neste exercício utilizaremos 1000 amostras de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNF2XjLBWWe7"
      },
      "source": [
        "indices = torch.randperm(len(dataset_train_full))[:1000]\n",
        "dataset_train = torch.utils.data.Subset(dataset_train_full, indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define os pesos iniciais"
      ],
      "metadata": {
        "id": "wYqj_oeSliYj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSNLD2JyA2e-"
      },
      "source": [
        "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T13:30:35.209157Z",
          "start_time": "2018-08-21T13:30:34.757103Z"
        },
        "id": "w52KGYlIQT2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cbd752e-606e-4c53-eb25-d1aaf56d8bd4"
      },
      "source": [
        "print('Número de minibatches de trenamento:', len(loader_train))\n",
        "\n",
        "x_train, y_train = next(iter(loader_train))\n",
        "print(\"\\nDimensões dos dados de um minibatch:\", x_train.size())\n",
        "print(\"Valores mínimo e máximo dos pixels: \", torch.min(x_train), torch.max(x_train))\n",
        "print(\"Tipo dos dados das imagens:         \", type(x_train))\n",
        "print(\"Tipo das classes das imagens:       \", type(y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de minibatches de trenamento: 20\n",
            "\n",
            "Dimensões dos dados de um minibatch: torch.Size([50, 1, 28, 28])\n",
            "Valores mínimo e máximo dos pixels:  tensor(0.) tensor(1.)\n",
            "Tipo dos dados das imagens:          <class 'torch.Tensor'>\n",
            "Tipo das classes das imagens:        <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Camada Convolucional"
      ],
      "metadata": {
        "id": "dfU_v7aPfq40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare se sua implementação está igual à do pytorch usando um exemplo simples"
      ],
      "metadata": {
        "id": "ROizI33sqE79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://ibm.box.com/shared/static/7maczejdeej0qoz3pzkysw0y8qb70g2h.png\" alt=\"HTML5 Icon\" style=\"width: 500px; height: 200px;\"> \n",
        "<center>  Illustration of the operation for one position of the kernel. <a href=\"http://colah.github.io/posts/2014-07-Understanding-Convolutions/\">ref</a></center>\n",
        "\n",
        " <img src=\"https://ibm.box.com/shared/static/fvutcm8jwa5j2o7xv2zzqyz2yu3zwhz4.gif\" alt=\"HTML5 Icon\" style=\"width: 450px; height: 300px;\">\n",
        "\n",
        " \n",
        "<center>  Animations showing how a kernel interact with a matrix representing an image. <a href=\"http://cs231n.github.io/convolutional-networks/\">ref</a></center>  \n",
        " "
      ],
      "metadata": {
        "id": "jW_5-FqUVqjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding\n",
        "\n",
        "![](https://miro.medium.com/max/600/0*r1pPAMPchc80hO3Q.gif)\n",
        "\n",
        "<center>  Padding. <a href=\"https://miro.medium.com/max/600/\">ref</a></center>  \n",
        " \n",
        "## Strides\n",
        "\n",
        "![](https://miro.medium.com/max/600/0*tCEq-ytvvZriWsfi.gif)\n",
        "\n",
        "<center>  Strides. <a href=\"https://miro.medium.com/max/600/\">ref</a></center>  \n",
        "\n",
        "## Input layer 3, output layer 3, kernel size 3x3, stride 1, padding 0\n",
        "![](http://www.sharetechnote.com/image/Python_Pytorch_nn_Conv2D_i3_o3_k3_s1_p0.png)\n",
        "\n",
        "<center>  Input layer 3, output layer 3, kernel size 3x3, stride 1, padding 0. <a href=\"http://www.sharetechnote.com/image/\">ref</a></center>  "
      ],
      "metadata": {
        "id": "_C9PCqJ5Vz8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch.nn.Conv2d(in_channels = n, out_channels = m, kernel_size = k, stride = s, padding = p)\n",
        "![](http://www.sharetechnote.com/image/Python_Pytorch_nn_Conv2D_Overview_01.png)"
      ],
      "metadata": {
        "id": "6wrrM-OPStE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "\n",
        "Applies a 2D convolution over an input signal composed of several input\n",
        "planes.\n",
        "In the simplest case, the output value of the layer with input size\n",
        " $(N, C_{\\text{in}}, L)$ and output  $(N,C_{\\text{out}}, L_{\\text{out}})$ \n",
        "\n",
        "can be   precisely described as:\n",
        "\n",
        "$$\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +    \\sum_{k = 0}^{C_{in} - 1} \\text{weight}(C_{\\text{out}_j}, k)\n",
        "\\star \\text{input}(N_i, k)$$\n",
        "\n",
        "where   $\\star$ is the valid 2D $ cross-correlation$ operator,\n",
        " $N$ is a batch size,   $C$ denotes a number of channels,\n",
        " $L$ is a length of signal sequence.\n",
        "\n",
        "\n",
        "* **stride** controls the stride for the cross-correlation, a single\n",
        "  number or a one-element tuple.\n",
        "* **padding** controls the amount of padding applied to the input. It\n",
        "  can be either a string {{'valid', 'same'}} or a tuple of ints giving the\n",
        "  amount of implicit padding applied on both sides.\n",
        "* **dilation** controls the spacing between the kernel points; also\n",
        "  known as the à trous algorithm. It is harder to describe, but this `link`_  has a nice visualization of what :attr:`dilation` does.\n",
        "\n",
        "Args:\n",
        "        in_channels (int): Number of channels in the input image\n",
        "        out_channels (int): Number of channels produced by the convolution\n",
        "        kernel_size (int or tuple): Size of the convolving kernel\n",
        "        stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
        "        padding (int, tuple or str, optional): Padding added to both sides of\n",
        "            the input. Default: 0\n",
        "\n",
        "\n",
        "  Shape:\n",
        "- Input: $(N, C_{in}, L_{in})$ or $(C_{in}, L_{in})$\n",
        "- Output: $(N, C_{out}, L_{out})$ or $(C_{out}, L_{out})$, \n",
        "\n",
        "where\n",
        "\n",
        "$L_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\\times (\\text{kernel_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor$\n",
        "\n",
        "Attributes:\n",
        "\n",
        "- **weight (Tensor):** the learnable weights of the module of shape\n",
        "$(\\text{out_channels},\\frac{\\text{in_channels}}{\\text{groups}}, \\text{kernel_size})$.\n",
        "\n",
        "The values of these weights are sampled from\n",
        "$\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ \n",
        "\n",
        "where $k = \\frac{groups}{C_\\text{in} * \\text{kernel_size}}$\n",
        "\n",
        "\n",
        "- **bias (Tensor):**  the learnable bias of the module of shape\n",
        "(out_channels). \n",
        "\n",
        "If :attr:  $bias$ is $True$, then the values of these weights aresampled from   $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ \n",
        "\n",
        "where\n",
        " $k = \\frac{groups}{C_\\text{in} * \\text{kernel_size}}$\n"
      ],
      "metadata": {
        "id": "hxoDmntCSFvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import math\n",
        "class MyConv2d(torch.nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int):\n",
        "        super(MyConv2d, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size  # The same for height and width.\n",
        "        self.stride = stride  # The same for height and width.\n",
        "        self.weight = torch.nn.Parameter(torch.FloatTensor(out_channels, in_channels, kernel_size, kernel_size).uniform_(-0.01, 0.01))\n",
        "        self.bias = torch.nn.Parameter(torch.FloatTensor(out_channels,).uniform_(-0.01, 0.01))\n",
        "\n",
        "    def load_state_dict(self,conv_weight,conv_bias):\n",
        "      self.weight=torch.nn.Parameter(conv_weight)\n",
        "      self.bias=torch.nn.Parameter(conv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.dim() == 4, f'x must have 4 dimensions: {x.shape}'\n",
        "        # Escreva seu código aqui.\n",
        "        pad=0\n",
        "        kz=self.kernel_size\n",
        "        st=self.stride       \n",
        "\n",
        "        lenx= x.shape[0]# number of imputs of x\n",
        "        lenys=self.out_channels# number of outpusts y\n",
        "        y_lenout1=(x.shape[2]+2*pad-kz)//st+1   #math.ceil((x.shape[2]+2*pad-kz+1)/st)# \n",
        "        y_lenout2=(x.shape[3]+2*pad-kz)//st +1#math.ceil((x.shape[3]+2*pad-kz+1)/st)#m\n",
        "\n",
        "        imgcnv_out=torch.zeros((lenx, lenys, y_lenout1, y_lenout2)) # SHAPE OF THE OUTPUT\n",
        "\n",
        "        for koutch in range(self.out_channels): # output channels\n",
        "          for ry, rx in enumerate(range(0, x.shape[3]+2*pad - kz + 1, st)): # scaning rows\n",
        "            for cy, cx in enumerate(range(0, x.shape[2]+2*pad - kz + 1, st)): # scamimg cols\n",
        "              xwindow=x[:, :, cx:cx+kz, rx:rx+kz]\n",
        "              filter=self.weight[koutch, :, :, :]\n",
        "              #print('xwindow.shape',xwindow.shape)\n",
        "              #print('filter.shape',filter.shape)\n",
        "              imgcnv_out[:, koutch, cy, ry] = (filter*xwindow).sum(dim=-1).sum(dim=-1).squeeze()#  function .sum() inspired from patrick_ferreira\n",
        "              #imgcnv_out[:, koutch, cy, cy] =torch.sum(torch.mul(xwindow, filter))\n",
        "          imgcnv_out[:, koutch, :, :]+=self.bias[koutch]\n",
        "        return imgcnv_out"
      ],
      "metadata": {
        "id": "tGMw_MDEN6p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels_dummy = 1\n",
        "out_channels_dummy = 1\n",
        "kernel_size_dummy = 2\n",
        "stride_dummy = 1\n",
        "\n",
        "conv_layer = MyConv2d(in_channels=in_channels_dummy, out_channels=out_channels_dummy, kernel_size=kernel_size_dummy, stride=stride_dummy)\n",
        "pytorch_conv_layer = torch.nn.Conv2d(in_channels=in_channels_dummy, out_channels=out_channels_dummy, kernel_size=kernel_size_dummy, stride=stride_dummy, padding=0)\n"
      ],
      "metadata": {
        "id": "i1TuxWbkqMJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usa os mesmos pesos para minha implementação e a do pytorch\n",
        "initial_weights_dummy = torch.arange(in_channels_dummy * out_channels_dummy * kernel_size_dummy * kernel_size_dummy).float()\n",
        "initial_weights_dummy = initial_weights_dummy.reshape(out_channels_dummy, in_channels_dummy, kernel_size_dummy, kernel_size_dummy)\n",
        "initial_bias_dummy = torch.arange(out_channels_dummy,).float()\n",
        "\n",
        "conv_layer.weight.data = initial_weights_dummy\n",
        "conv_layer.bias.data = initial_bias_dummy\n",
        "pytorch_conv_layer.load_state_dict(dict(weight=initial_weights_dummy, bias=initial_bias_dummy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA7qQxXsU82S",
        "outputId": "0bdb4c15-7892-47bc-9b92-e1b831f0dd98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(30).float().reshape(1, 1, 5, 6)\n",
        "\n",
        "out = conv_layer(x)\n",
        "target_out = pytorch_conv_layer(x)\n",
        "\n",
        "assert torch.allclose(out, target_out, atol=1e-6)"
      ],
      "metadata": {
        "id": "7UV5ZRJGUVNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare se sua implementação está igual à do pytorch usando um exemplo aleatório"
      ],
      "metadata": {
        "id": "_75UnRhdd_MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2, in_channels, height_in, width_in)\n",
        "\n",
        "conv_layer = MyConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "pytorch_conv_layer = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=0)\n",
        "\n",
        "# Usa os mesmos pesos para minha implementação e a do pytorch\n",
        "conv_layer.weight.data = initial_conv_weight\n",
        "conv_layer.bias.data = initial_conv_bias\n",
        "pytorch_conv_layer.load_state_dict(dict(weight=initial_conv_weight, bias=initial_conv_bias))\n",
        "\n",
        "out = conv_layer(x)\n",
        "target_out = pytorch_conv_layer(x)\n",
        "\n",
        "assert torch.allclose(out, target_out, atol=1e-6)"
      ],
      "metadata": {
        "id": "HzIjuGpWlbIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQA9Zg7GQT2G"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:10.802708Z",
          "start_time": "2018-08-20T21:03:10.793287Z"
        },
        "id": "_8Eg4h_kQT2H"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, height_in: int, width_in: int, in_channels: int, out_channels: int, kernel_size: int, stride: int):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv_layer = MyConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "   \n",
        "        height_out = (height_in - kernel_size) // stride + 1\n",
        "        width_out = (width_in - kernel_size) // stride + 1\n",
        "        self.classification_layer = torch.nn.Linear(out_channels * height_out * width_out, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.conv_layer(x)\n",
        "        hidden = torch.nn.functional.relu(hidden)\n",
        "        hidden = hidden.reshape(x.shape[0], -1)\n",
        "        logits = self.classification_layer(hidden)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NHQB4wGQT2K"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqs2JhJoQT2L"
      },
      "source": [
        "### Definição dos hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:14.146259Z",
          "start_time": "2018-08-20T21:03:14.139515Z"
        },
        "id": "oZuYEkn_QT2M"
      },
      "source": [
        "n_epochs = 50\n",
        "lr = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXarXeIQT2O"
      },
      "source": [
        "### Laço de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:40.796410Z",
          "start_time": "2018-08-20T21:03:39.771981Z"
        },
        "id": "L5T_jZZPQT2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aaba166-fd06-427d-8c93-324acd73793e"
      },
      "source": [
        "model = Net(height_in=height_in, width_in=width_in, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "\n",
        "# Usa pesos iniciais pré-difinidos\n",
        "model.classification_layer.load_state_dict(dict(weight=initial_classification_weight, bias=initial_classification_bias))\n",
        "model.conv_layer.weight.data = initial_conv_weight\n",
        "model.conv_layer.bias.data = initial_conv_bias\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "epochs = []\n",
        "loss_history = []\n",
        "loss_epoch_end = []\n",
        "total_trained_samples = 0\n",
        "for i in range(n_epochs):\n",
        "    for x_train, y_train in loader_train:\n",
        "        # predict da rede\n",
        "        outputs = model(x_train)\n",
        "\n",
        "        # calcula a perda\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # zero, backpropagation, ajusta parâmetros pelo gradiente descendente\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_trained_samples += x_train.size(0)\n",
        "        epochs.append(total_trained_samples / len(dataset_train))\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "    loss_epoch_end.append(loss.item())\n",
        "    print(f'Epoch: {i:d}/{n_epochs - 1:d} Loss: {loss.item()}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/49 Loss: 2.303267478942871\n",
            "Epoch: 1/49 Loss: 2.227701187133789\n",
            "Epoch: 2/49 Loss: 1.0923895835876465\n",
            "Epoch: 3/49 Loss: 0.5867355465888977\n",
            "Epoch: 4/49 Loss: 0.5144088864326477\n",
            "Epoch: 5/49 Loss: 0.4502663314342499\n",
            "Epoch: 6/49 Loss: 0.4075140058994293\n",
            "Epoch: 7/49 Loss: 0.3771387040615082\n",
            "Epoch: 8/49 Loss: 0.3534485995769501\n",
            "Epoch: 9/49 Loss: 0.334145188331604\n",
            "Epoch: 10/49 Loss: 0.3181140422821045\n",
            "Epoch: 11/49 Loss: 0.3045787811279297\n",
            "Epoch: 12/49 Loss: 0.29283490777015686\n",
            "Epoch: 13/49 Loss: 0.2827607989311218\n",
            "Epoch: 14/49 Loss: 0.2738332748413086\n",
            "Epoch: 15/49 Loss: 0.26577433943748474\n",
            "Epoch: 16/49 Loss: 0.2583288848400116\n",
            "Epoch: 17/49 Loss: 0.2511749863624573\n",
            "Epoch: 18/49 Loss: 0.24439725279808044\n",
            "Epoch: 19/49 Loss: 0.23789969086647034\n",
            "Epoch: 20/49 Loss: 0.2316771149635315\n",
            "Epoch: 21/49 Loss: 0.22562646865844727\n",
            "Epoch: 22/49 Loss: 0.21984536945819855\n",
            "Epoch: 23/49 Loss: 0.21429115533828735\n",
            "Epoch: 24/49 Loss: 0.2089422345161438\n",
            "Epoch: 25/49 Loss: 0.20387297868728638\n",
            "Epoch: 26/49 Loss: 0.19903433322906494\n",
            "Epoch: 27/49 Loss: 0.19439968466758728\n",
            "Epoch: 28/49 Loss: 0.18994104862213135\n",
            "Epoch: 29/49 Loss: 0.18563994765281677\n",
            "Epoch: 30/49 Loss: 0.18147501349449158\n",
            "Epoch: 31/49 Loss: 0.17744916677474976\n",
            "Epoch: 32/49 Loss: 0.17347252368927002\n",
            "Epoch: 33/49 Loss: 0.16947472095489502\n",
            "Epoch: 34/49 Loss: 0.1654731035232544\n",
            "Epoch: 35/49 Loss: 0.1615048348903656\n",
            "Epoch: 36/49 Loss: 0.1574639081954956\n",
            "Epoch: 37/49 Loss: 0.1534043848514557\n",
            "Epoch: 38/49 Loss: 0.14926917850971222\n",
            "Epoch: 39/49 Loss: 0.14520645141601562\n",
            "Epoch: 40/49 Loss: 0.14123648405075073\n",
            "Epoch: 41/49 Loss: 0.1371268332004547\n",
            "Epoch: 42/49 Loss: 0.13310380280017853\n",
            "Epoch: 43/49 Loss: 0.1291467249393463\n",
            "Epoch: 44/49 Loss: 0.12515074014663696\n",
            "Epoch: 45/49 Loss: 0.12116772681474686\n",
            "Epoch: 46/49 Loss: 0.1173173040151596\n",
            "Epoch: 47/49 Loss: 0.11364639550447464\n",
            "Epoch: 48/49 Loss: 0.11001913249492645\n",
            "Epoch: 49/49 Loss: 0.10655996948480606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLL-GQlKQT2Y"
      },
      "source": [
        "### Visualização usual da perda, somente no final de cada minibatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:55.246851Z",
          "start_time": "2018-08-20T21:03:54.994428Z"
        },
        "id": "w38EtNxhQT2Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "5af1caf9-e626-4d3b-aee3-2bb798c9729a"
      },
      "source": [
        "n_batches_train = len(loader_train)\n",
        "plt.plot(epochs[::n_batches_train], loss_history[::n_batches_train])\n",
        "plt.xlabel('época')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'época')"
            ]
          },
          "metadata": {},
          "execution_count": 185
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_epoch_end"
      ],
      "metadata": {
        "id": "ToktJu4CK94z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "809e42ae-5660-45e7-b0d9-632880914dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.303267478942871,\n",
              " 2.227701187133789,\n",
              " 1.0923895835876465,\n",
              " 0.5867355465888977,\n",
              " 0.5144088864326477,\n",
              " 0.4502663314342499,\n",
              " 0.4075140058994293,\n",
              " 0.3771387040615082,\n",
              " 0.3534485995769501,\n",
              " 0.334145188331604,\n",
              " 0.3181140422821045,\n",
              " 0.3045787811279297,\n",
              " 0.29283490777015686,\n",
              " 0.2827607989311218,\n",
              " 0.2738332748413086,\n",
              " 0.26577433943748474,\n",
              " 0.2583288848400116,\n",
              " 0.2511749863624573,\n",
              " 0.24439725279808044,\n",
              " 0.23789969086647034,\n",
              " 0.2316771149635315,\n",
              " 0.22562646865844727,\n",
              " 0.21984536945819855,\n",
              " 0.21429115533828735,\n",
              " 0.2089422345161438,\n",
              " 0.20387297868728638,\n",
              " 0.19903433322906494,\n",
              " 0.19439968466758728,\n",
              " 0.18994104862213135,\n",
              " 0.18563994765281677,\n",
              " 0.18147501349449158,\n",
              " 0.17744916677474976,\n",
              " 0.17347252368927002,\n",
              " 0.16947472095489502,\n",
              " 0.1654731035232544,\n",
              " 0.1615048348903656,\n",
              " 0.1574639081954956,\n",
              " 0.1534043848514557,\n",
              " 0.14926917850971222,\n",
              " 0.14520645141601562,\n",
              " 0.14123648405075073,\n",
              " 0.1371268332004547,\n",
              " 0.13310380280017853,\n",
              " 0.1291467249393463,\n",
              " 0.12515074014663696,\n",
              " 0.12116772681474686,\n",
              " 0.1173173040151596,\n",
              " 0.11364639550447464,\n",
              " 0.11001913249492645,\n",
              " 0.10655996948480606]"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:48.786969Z",
          "start_time": "2018-08-20T21:03:48.781787Z"
        },
        "id": "PiuMsjYtQT2R"
      },
      "source": [
        "# Assert do histórico de losses\n",
        "target_loss_epoch_end = np.array([\n",
        "    2.303267478942871,\n",
        "    2.227701187133789,\n",
        "    1.0923893451690674,\n",
        "    0.5867354869842529,\n",
        "    0.5144089460372925,\n",
        "    0.45026642084121704,\n",
        "    0.4075140357017517,\n",
        "    0.37713879346847534,\n",
        "    0.3534485101699829,\n",
        "    0.3341451585292816,\n",
        "    0.3181140422821045,\n",
        "    0.30457887053489685,\n",
        "    0.29283496737480164,\n",
        "    0.2827608287334442,\n",
        "    0.2738332152366638,\n",
        "    0.2657742500305176,\n",
        "    0.2583288848400116,\n",
        "    0.25117507576942444,\n",
        "    0.24439716339111328,\n",
        "    0.23789969086647034,\n",
        "    0.23167723417282104,\n",
        "    0.22562651336193085,\n",
        "    0.21984536945819855,\n",
        "    0.2142913043498993,\n",
        "    0.20894232392311096,\n",
        "    0.203872948884964,\n",
        "    0.19903430342674255,\n",
        "    0.19439971446990967,\n",
        "    0.18994088470935822,\n",
        "    0.18563991785049438,\n",
        "    0.18147490918636322,\n",
        "    0.17744913697242737,\n",
        "    0.17347246408462524,\n",
        "    0.16947467625141144,\n",
        "    0.16547319293022156,\n",
        "    0.16150487959384918,\n",
        "    0.1574639081954956,\n",
        "    0.1534043848514557,\n",
        "    0.14926929771900177,\n",
        "    0.1452063024044037,\n",
        "    0.1412365883588791,\n",
        "    0.13712672889232635,\n",
        "    0.1331038922071457,\n",
        "    0.1291467249393463,\n",
        "    0.1251506358385086,\n",
        "    0.12116757035255432,\n",
        "    0.11731722950935364,\n",
        "    0.11364627629518509,\n",
        "    0.11001908034086227,\n",
        "    0.10655981302261353])\n",
        "\n",
        "assert np.allclose(np.array(loss_epoch_end), target_loss_epoch_end, atol=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}