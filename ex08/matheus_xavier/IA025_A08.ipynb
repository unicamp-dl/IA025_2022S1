{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flych3r/IA025_2022S1/blob/main/ex08/matheus_xavier/IA025_A08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0466a38",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:30:14.424910Z",
          "iopub.status.busy": "2022-05-25T20:30:14.424498Z",
          "iopub.status.idle": "2022-05-25T20:30:14.435555Z",
          "shell.execute_reply": "2022-05-25T20:30:14.434532Z"
        },
        "id": "d0466a38",
        "papermill": {
          "duration": 0.036782,
          "end_time": "2022-05-25T20:30:14.437393",
          "exception": false,
          "start_time": "2022-05-25T20:30:14.400611",
          "status": "completed"
        },
        "tags": [],
        "outputId": "bfe53e02-4790-43b0-eea1-ce45bcf6103f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Meu nome é Matheus Xavier Sampaio - 220092\n"
          ]
        }
      ],
      "source": [
        "nome = 'Matheus Xavier Sampaio - 220092'\n",
        "print(f'Meu nome é {nome}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f9229f",
      "metadata": {
        "id": "23f9229f",
        "papermill": {
          "duration": 0.021462,
          "end_time": "2022-05-25T20:30:14.481168",
          "exception": false,
          "start_time": "2022-05-25T20:30:14.459706",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#  Exercício: Modelo de Linguagem com auto-atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bfd489a",
      "metadata": {
        "id": "1bfd489a",
        "papermill": {
          "duration": 0.042379,
          "end_time": "2022-05-25T20:30:14.551440",
          "exception": false,
          "start_time": "2022-05-25T20:30:14.509061",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Este exercício é similar ao da Aula 7, mas iremos agora treinar uma rede neural *com auto-atenção* para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
        "\n",
        "Na camada de auto-atenção, não se esqueça de implementar:\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Conexões residuais\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "\n",
        "\n",
        "O dataset usado neste exercício (BrWaC) possui um tamanho razoável e você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0cdde7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:30:14.617901Z",
          "iopub.status.busy": "2022-05-25T20:30:14.617264Z",
          "iopub.status.idle": "2022-05-25T20:30:25.116070Z",
          "shell.execute_reply": "2022-05-25T20:30:25.115090Z"
        },
        "id": "fc0cdde7",
        "papermill": {
          "duration": 10.534439,
          "end_time": "2022-05-25T20:30:25.118211",
          "exception": false,
          "start_time": "2022-05-25T20:30:14.583772",
          "status": "completed"
        },
        "tags": [],
        "outputId": "ca8b4daa-e705-4c1b-e4dd-0e2177faafab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "%pip install -qqq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0eff8ec",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:30:25.165081Z",
          "iopub.status.busy": "2022-05-25T20:30:25.164492Z",
          "iopub.status.idle": "2022-05-25T20:30:34.270295Z",
          "shell.execute_reply": "2022-05-25T20:30:34.269521Z"
        },
        "papermill": {
          "duration": 9.131268,
          "end_time": "2022-05-25T20:30:34.272297",
          "exception": false,
          "start_time": "2022-05-25T20:30:25.141029",
          "status": "completed"
        },
        "tags": [],
        "id": "b0eff8ec",
        "outputId": "4dbcf4d0-cecf-4e8d-8332-0c6dfc05645b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qqq wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d2faf7c",
      "metadata": {
        "id": "8d2faf7c",
        "papermill": {
          "duration": 0.024633,
          "end_time": "2022-05-25T20:30:34.319806",
          "exception": false,
          "start_time": "2022-05-25T20:30:34.295173",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab9cac43",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:30:34.366726Z",
          "iopub.status.busy": "2022-05-25T20:30:34.366487Z",
          "iopub.status.idle": "2022-05-25T20:30:36.044288Z",
          "shell.execute_reply": "2022-05-25T20:30:36.043375Z"
        },
        "id": "ab9cac43",
        "papermill": {
          "duration": 1.70403,
          "end_time": "2022-05-25T20:30:36.046819",
          "exception": false,
          "start_time": "2022-05-25T20:30:34.342789",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5819573c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:30:36.100997Z",
          "iopub.status.busy": "2022-05-25T20:30:36.100769Z",
          "iopub.status.idle": "2022-05-25T20:30:36.804202Z",
          "shell.execute_reply": "2022-05-25T20:30:36.803395Z"
        },
        "id": "5819573c",
        "papermill": {
          "duration": 0.732174,
          "end_time": "2022-05-25T20:30:36.806108",
          "exception": false,
          "start_time": "2022-05-25T20:30:36.073934",
          "status": "completed"
        },
        "tags": [],
        "outputId": "f4999f25-eb0c-4f79-9b2d-f2a291f4c55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed May 25 20:30:36 2022       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\r\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n",
            "| N/A   37C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "                                                                               \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                  |\r\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
            "|        ID   ID                                                   Usage      |\r\n",
            "|=============================================================================|\r\n",
            "|  No running processes found                                                 |\r\n",
            "+-----------------------------------------------------------------------------+\r\n"
          ]
        }
      ],
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4dc2dc7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:30:36.853530Z",
          "iopub.status.busy": "2022-05-25T20:30:36.853179Z",
          "iopub.status.idle": "2022-05-25T20:30:36.909408Z",
          "shell.execute_reply": "2022-05-25T20:30:36.908682Z"
        },
        "id": "e4dc2dc7",
        "papermill": {
          "duration": 0.082573,
          "end_time": "2022-05-25T20:30:36.911844",
          "exception": false,
          "start_time": "2022-05-25T20:30:36.829271",
          "status": "completed"
        },
        "tags": [],
        "outputId": "e9820028-6311-4d6e-fd3f-81661e18c9f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda:0\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda28def",
      "metadata": {
        "id": "dda28def",
        "papermill": {
          "duration": 0.023459,
          "end_time": "2022-05-25T20:30:36.958685",
          "exception": false,
          "start_time": "2022-05-25T20:30:36.935226",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decb9706",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:30:37.010097Z",
          "iopub.status.busy": "2022-05-25T20:30:37.009855Z",
          "iopub.status.idle": "2022-05-25T20:30:42.001378Z",
          "shell.execute_reply": "2022-05-25T20:30:42.000609Z"
        },
        "id": "decb9706",
        "papermill": {
          "duration": 5.020481,
          "end_time": "2022-05-25T20:30:42.003836",
          "exception": false,
          "start_time": "2022-05-25T20:30:36.983355",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def tokenize(text: str, tokenizer: AutoTokenizer):\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer: AutoTokenizer, context_size: int):\n",
        "        self.examples = []\n",
        "        for text in tqdm(texts):\n",
        "            token_ids = tokenize(text=text, tokenizer=tokenizer)\n",
        "            if len(token_ids) < context_size + 1:\n",
        "                continue\n",
        "            # Compute n-grams:\n",
        "            for i in range(len(token_ids) - context_size):\n",
        "                input_ids = token_ids[i:i + context_size]\n",
        "                target_id = token_ids[i + context_size]\n",
        "                self.examples.append((input_ids, target_id)) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids, target_id = self.examples[idx]\n",
        "        return torch.LongTensor(input_ids), target_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e965612",
      "metadata": {
        "id": "1e965612",
        "papermill": {
          "duration": 0.024189,
          "end_time": "2022-05-25T20:30:42.052647",
          "exception": false,
          "start_time": "2022-05-25T20:30:42.028458",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Teste se sua implementação do MyDataset está correta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e111f6ac",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:30:42.100547Z",
          "iopub.status.busy": "2022-05-25T20:30:42.100295Z",
          "iopub.status.idle": "2022-05-25T20:30:47.220717Z",
          "shell.execute_reply": "2022-05-25T20:30:47.219929Z"
        },
        "id": "e111f6ac",
        "papermill": {
          "duration": 5.1467,
          "end_time": "2022-05-25T20:30:47.222662",
          "exception": false,
          "start_time": "2022-05-25T20:30:42.075962",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "119ed777bdc8495aa89c546558d6428e",
            "d1322776c9ce4d5680cb4af5d8b7fafc",
            "05e9b0e590f74938b031384c0fc03ae2",
            "68bad05c969744898be9500c3af26558",
            "cf1ac0d5a3724ea2b49c4fe7ba8d22be",
            "689bece7607a469d91eb59aae97c1910"
          ]
        },
        "outputId": "e9824a31-c188-499a-a523-3b23fce32be2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "119ed777bdc8495aa89c546558d6428e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/205k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1322776c9ce4d5680cb4af5d8b7fafc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05e9b0e590f74938b031384c0fc03ae2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68bad05c969744898be9500c3af26558",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf1ac0d5a3724ea2b49c4fe7ba8d22be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "689bece7607a469d91eb59aae97c1910",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "passou no assert de tamanho do dataset\n",
            "Passou no assert de input\n",
            "Passou no assert de target\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "assert len(dummy_dataset) == 5\n",
        "print('passou no assert de tamanho do dataset')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor([\n",
        "    [ 3396, 10303,   125],\n",
        "    [ 1660,  5971,   785],\n",
        "    [ 5971,   785,   125],\n",
        "    [  785,   125,  1847],\n",
        "    [  125,  1847, 13779]\n",
        "])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor([13239,   125,  1847, 13779, 15616])\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "print('Passou no assert de input')\n",
        "\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "print('Passou no assert de target')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e4b160b",
      "metadata": {
        "id": "7e4b160b",
        "papermill": {
          "duration": 0.027072,
          "end_time": "2022-05-25T20:30:47.277643",
          "exception": false,
          "start_time": "2022-05-25T20:30:47.250571",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ca5805",
      "metadata": {
        "id": "70ca5805",
        "papermill": {
          "duration": 0.026284,
          "end_time": "2022-05-25T20:30:47.330868",
          "exception": false,
          "start_time": "2022-05-25T20:30:47.304584",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa21e5c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:30:47.386335Z",
          "iopub.status.busy": "2022-05-25T20:30:47.386113Z",
          "iopub.status.idle": "2022-05-25T20:30:49.224268Z",
          "shell.execute_reply": "2022-05-25T20:30:49.223281Z"
        },
        "id": "ffa21e5c",
        "papermill": {
          "duration": 1.868753,
          "end_time": "2022-05-25T20:30:49.227422",
          "exception": false,
          "start_time": "2022-05-25T20:30:47.358669",
          "status": "completed"
        },
        "tags": [],
        "outputId": "29b42449-5125-47a4-ccf5-e6623acb1343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-05-25 20:30:47--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt\r\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.107.128, 173.194.215.128, 173.194.212.128, ...\r\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.107.128|:443... connected.\r\n",
            "HTTP request sent, awaiting response... 200 OK\r\n",
            "Length: 123983611 (118M) [text/plain]\r\n",
            "Saving to: ‘sample_brwac.txt’\r\n",
            "\r\n",
            "sample_brwac.txt    100%[===================>] 118.24M   157MB/s    in 0.8s    \r\n",
            "\r\n",
            "2022-05-25 20:30:49 (157 MB/s) - ‘sample_brwac.txt’ saved [123983611/123983611]\r\n",
            "\r\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e91cffd6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:30:49.343710Z",
          "iopub.status.busy": "2022-05-25T20:30:49.343210Z",
          "iopub.status.idle": "2022-05-25T20:41:49.167799Z",
          "shell.execute_reply": "2022-05-25T20:41:49.166064Z"
        },
        "id": "e91cffd6",
        "papermill": {
          "duration": 659.887504,
          "end_time": "2022-05-25T20:41:49.170042",
          "exception": false,
          "start_time": "2022-05-25T20:30:49.282538",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "bb4117dd42c947bcaf909315e5c7a512",
            "469ac0d07ece4f948ff354d9d5a34f2b",
            "c28aa388778e4c73b901a1c95f6add1e"
          ]
        },
        "outputId": "09a2213f-48c5-406b-cd03-1158f5adbee9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb4117dd42c947bcaf909315e5c7a512",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/24800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "469ac0d07ece4f948ff354d9d5a34f2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c28aa388778e4c73b901a1c95f6add1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load datasets\n",
        "context_size = 9\n",
        "\n",
        "texts = open('sample_brwac.txt').readlines()\n",
        "\n",
        "# print('Truncating for debugging purposes.')\n",
        "# texts = texts[:500]\n",
        "valid_examples = 100\n",
        "test_examples = 100\n",
        "\n",
        "training_texts = texts[:-(valid_examples + test_examples)]\n",
        "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "test_texts = texts[-test_examples:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, context_size=context_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79d78e79",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:41:49.253807Z",
          "iopub.status.busy": "2022-05-25T20:41:49.253445Z",
          "iopub.status.idle": "2022-05-25T20:41:49.264814Z",
          "shell.execute_reply": "2022-05-25T20:41:49.263490Z"
        },
        "id": "79d78e79",
        "papermill": {
          "duration": 0.068355,
          "end_time": "2022-05-25T20:41:49.270975",
          "exception": false,
          "start_time": "2022-05-25T20:41:49.202620",
          "status": "completed"
        },
        "tags": [],
        "outputId": "761c4460-dbb6-4921-ec5b-6e9d21f25beb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training examples: 27675945\n",
            "valid examples: 82070\n",
            "test examples: 166726\n"
          ]
        }
      ],
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5eaf79",
      "metadata": {
        "papermill": {
          "duration": 0.05331,
          "end_time": "2022-05-25T20:41:49.391981",
          "exception": false,
          "start_time": "2022-05-25T20:41:49.338671",
          "status": "completed"
        },
        "tags": [],
        "id": "7d5eaf79"
      },
      "source": [
        "## Modelo de Linguagem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e98d12d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:41:49.500794Z",
          "iopub.status.busy": "2022-05-25T20:41:49.500251Z",
          "iopub.status.idle": "2022-05-25T20:41:49.523598Z",
          "shell.execute_reply": "2022-05-25T20:41:49.522872Z"
        },
        "id": "6e98d12d",
        "papermill": {
          "duration": 0.078542,
          "end_time": "2022-05-25T20:41:49.525621",
          "exception": false,
          "start_time": "2022-05-25T20:41:49.447079",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        context_size: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_size: int = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implements the Self-attention, decoder-only.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            context_size (int): Size of the sequence to consider as context for prediction.\n",
        "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        if hidden_size is None:\n",
        "            hidden_size = embedding_dim\n",
        "        self._positions = torch.arange(self.context_size, dtype=torch.long).unsqueeze(0)\n",
        "        \n",
        "        self.C = torch.nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n",
        "        self.P = torch.nn.Embedding(num_embeddings=self.context_size, embedding_dim=self.embedding_dim)\n",
        "\n",
        "        self.Wq = torch.nn.Linear(in_features=self.embedding_dim, out_features=self.embedding_dim, bias=False)\n",
        "        self.Wk = torch.nn.Linear(in_features=self.embedding_dim, out_features=self.embedding_dim, bias=False)\n",
        "        self.Wv = torch.nn.Linear(in_features=self.embedding_dim, out_features=self.embedding_dim, bias=False)\n",
        "        self.Wo = torch.nn.Linear(in_features=self.embedding_dim, out_features=self.embedding_dim, bias=False)\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=self.embedding_dim, out_features=hidden_size),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(in_features=hidden_size, out_features=self.vocab_size, bias=False)\n",
        "        )\n",
        "        \n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, context_size)\n",
        "        \n",
        "        Returns:\n",
        "            logits of shape (batch_size, vocab_size)\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        positions = self._positions.repeat(inputs.shape[0], 1).to(inputs.device)\n",
        "\n",
        "        X = self.C(inputs) + self.P(positions)\n",
        "        Q = self.Wq(torch.unsqueeze(X[:, -1, :], dim=1))\n",
        "        K = self.Wk(X)\n",
        "        V = self.Wv(X)\n",
        "\n",
        "        scores = Q @ torch.transpose(K, 1, 2) / math.sqrt(self.embedding_dim)\n",
        "        probs = self.softmax(scores)\n",
        "\n",
        "        E = probs @ V\n",
        "        E = self.Wo(E)\n",
        "        logits = self.mlp(torch.squeeze(E, dim=1))\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3971522",
      "metadata": {
        "id": "e3971522",
        "papermill": {
          "duration": 0.05127,
          "end_time": "2022-05-25T20:41:49.629010",
          "exception": false,
          "start_time": "2022-05-25T20:41:49.577740",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Teste o modelo com um exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd4802ca",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:41:49.733845Z",
          "iopub.status.busy": "2022-05-25T20:41:49.733555Z",
          "iopub.status.idle": "2022-05-25T20:41:55.098850Z",
          "shell.execute_reply": "2022-05-25T20:41:55.098185Z"
        },
        "id": "cd4802ca",
        "papermill": {
          "duration": 5.419887,
          "end_time": "2022-05-25T20:41:55.100645",
          "exception": false,
          "start_time": "2022-05-25T20:41:49.680758",
          "status": "completed"
        },
        "tags": [],
        "outputId": "808da2af-f303-490f-9bae-784af82cec47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 29794])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=64\n",
        ").to(device)\n",
        "\n",
        "model.eval()\n",
        "sample_train, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_train = torch.stack([sample_train, sample_train]).squeeze(1)\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "model(sample_train_gpu).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2da7a49",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:41:55.165643Z",
          "iopub.status.busy": "2022-05-25T20:41:55.165082Z",
          "iopub.status.idle": "2022-05-25T20:41:55.169602Z",
          "shell.execute_reply": "2022-05-25T20:41:55.168909Z"
        },
        "id": "f2da7a49",
        "papermill": {
          "duration": 0.039213,
          "end_time": "2022-05-25T20:41:55.171814",
          "exception": false,
          "start_time": "2022-05-25T20:41:55.132601",
          "status": "completed"
        },
        "tags": [],
        "outputId": "8585caa6-cf87-4ad0-f18d-c8f8122d176c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of model parameters: 3834752\n"
          ]
        }
      ],
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "227c84cb",
      "metadata": {
        "id": "227c84cb",
        "papermill": {
          "duration": 0.0319,
          "end_time": "2022-05-25T20:41:55.235710",
          "exception": false,
          "start_time": "2022-05-25T20:41:55.203810",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Laço de Treinamento e Validação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ade2b318",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:41:55.300746Z",
          "iopub.status.busy": "2022-05-25T20:41:55.300146Z",
          "iopub.status.idle": "2022-05-25T20:41:56.309818Z",
          "shell.execute_reply": "2022-05-25T20:41:56.308951Z"
        },
        "papermill": {
          "duration": 1.04427,
          "end_time": "2022-05-25T20:41:56.311979",
          "exception": false,
          "start_time": "2022-05-25T20:41:55.267709",
          "status": "completed"
        },
        "tags": [],
        "id": "ade2b318"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27b5483",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:41:58.282115Z",
          "iopub.status.busy": "2022-05-25T20:41:58.281540Z",
          "iopub.status.idle": "2022-05-25T20:42:03.970728Z",
          "shell.execute_reply": "2022-05-25T20:42:03.970004Z"
        },
        "id": "e27b5483",
        "papermill": {
          "duration": 5.725796,
          "end_time": "2022-05-25T20:42:03.972532",
          "exception": false,
          "start_time": "2022-05-25T20:41:58.246736",
          "status": "completed"
        },
        "tags": [],
        "outputId": "a112f969-5d89-4713-b3cf-8fc4583e9d85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflych3r\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/kaggle/working/wandb/run-20220525_204158-3fwjpaxv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/flych3r/language-models/runs/3fwjpaxv\" target=\"_blank\">morning-mountain-19</a></strong> to <a href=\"https://wandb.ai/flych3r/language-models\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.init(project=\"language-models\", anonymous=\"allow\")\n",
        "wandb.run.name = f'selfattention-{wandb.run.name}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33c62171",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T20:42:04.049226Z",
          "iopub.status.busy": "2022-05-25T20:42:04.048999Z",
          "iopub.status.idle": "2022-05-25T23:20:47.186512Z",
          "shell.execute_reply": "2022-05-25T23:20:47.185735Z"
        },
        "id": "33c62171",
        "papermill": {
          "duration": 9523.180186,
          "end_time": "2022-05-25T23:20:47.188705",
          "exception": false,
          "start_time": "2022-05-25T20:42:04.008519",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "2a9ead690bb2444a88f2ef5f4b77a211"
          ]
        },
        "outputId": "9348eb2c-2020-435a-e521-a61b373635dc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a9ead690bb2444a88f2ef5f4b77a211",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/500000000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 steps; 0 examples so far; train ppl: 29827.77, valid ppl: 29637.71\n",
            "1000 steps; 12288000 examples so far; train ppl: 1912.20, valid ppl: 1794.37\n",
            "2000 steps; 24576000 examples so far; train ppl: 1430.30, valid ppl: 1105.06\n",
            "3000 steps; 36864000 examples so far; train ppl: 938.95, valid ppl: 851.88\n",
            "4000 steps; 49152000 examples so far; train ppl: 726.68, valid ppl: 622.31\n",
            "5000 steps; 61440000 examples so far; train ppl: 548.98, valid ppl: 499.93\n",
            "6000 steps; 73728000 examples so far; train ppl: 460.66, valid ppl: 434.98\n",
            "7000 steps; 86016000 examples so far; train ppl: 409.07, valid ppl: 393.38\n",
            "8000 steps; 98304000 examples so far; train ppl: 373.51, valid ppl: 364.92\n",
            "9000 steps; 110592000 examples so far; train ppl: 350.47, valid ppl: 342.77\n",
            "10000 steps; 122880000 examples so far; train ppl: 327.77, valid ppl: 326.87\n",
            "11000 steps; 135168000 examples so far; train ppl: 314.50, valid ppl: 312.52\n",
            "12000 steps; 147456000 examples so far; train ppl: 299.53, valid ppl: 302.58\n",
            "13000 steps; 159744000 examples so far; train ppl: 289.86, valid ppl: 292.53\n",
            "14000 steps; 172032000 examples so far; train ppl: 280.00, valid ppl: 285.09\n",
            "15000 steps; 184320000 examples so far; train ppl: 271.67, valid ppl: 278.21\n",
            "16000 steps; 196608000 examples so far; train ppl: 264.75, valid ppl: 272.40\n",
            "17000 steps; 208896000 examples so far; train ppl: 257.24, valid ppl: 266.63\n",
            "18000 steps; 221184000 examples so far; train ppl: 253.44, valid ppl: 260.85\n",
            "19000 steps; 233472000 examples so far; train ppl: 244.98, valid ppl: 256.52\n",
            "20000 steps; 245760000 examples so far; train ppl: 242.02, valid ppl: 251.44\n",
            "21000 steps; 258048000 examples so far; train ppl: 235.24, valid ppl: 247.37\n",
            "22000 steps; 270336000 examples so far; train ppl: 231.36, valid ppl: 242.86\n",
            "23000 steps; 282624000 examples so far; train ppl: 226.75, valid ppl: 240.33\n",
            "24000 steps; 294912000 examples so far; train ppl: 222.65, valid ppl: 235.91\n",
            "25000 steps; 307200000 examples so far; train ppl: 219.56, valid ppl: 233.15\n",
            "26000 steps; 319488000 examples so far; train ppl: 214.82, valid ppl: 229.44\n",
            "27000 steps; 331776000 examples so far; train ppl: 213.12, valid ppl: 225.70\n",
            "28000 steps; 344064000 examples so far; train ppl: 207.35, valid ppl: 223.31\n",
            "29000 steps; 356352000 examples so far; train ppl: 206.23, valid ppl: 220.58\n",
            "30000 steps; 368640000 examples so far; train ppl: 201.82, valid ppl: 218.13\n",
            "31000 steps; 380928000 examples so far; train ppl: 200.32, valid ppl: 215.72\n",
            "32000 steps; 393216000 examples so far; train ppl: 197.16, valid ppl: 213.63\n",
            "33000 steps; 405504000 examples so far; train ppl: 195.06, valid ppl: 211.84\n",
            "34000 steps; 417792000 examples so far; train ppl: 192.71, valid ppl: 210.44\n",
            "35000 steps; 430080000 examples so far; train ppl: 189.72, valid ppl: 207.93\n",
            "36000 steps; 442368000 examples so far; train ppl: 189.51, valid ppl: 205.92\n",
            "37000 steps; 454656000 examples so far; train ppl: 185.06, valid ppl: 204.77\n",
            "38000 steps; 466944000 examples so far; train ppl: 185.20, valid ppl: 202.19\n",
            "39000 steps; 479232000 examples so far; train ppl: 181.91, valid ppl: 202.07\n",
            "40000 steps; 491520000 examples so far; train ppl: 181.31, valid ppl: 200.16\n"
          ]
        }
      ],
      "source": [
        "max_examples = 500_000_000\n",
        "eval_every_steps = 1_000\n",
        "lr = 3e-4\n",
        "batch_size = 4096 * 3\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_size=256\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def train_step(input, target):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    logits = model(input.to(device))\n",
        "    loss = nn.functional.cross_entropy(logits, target.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input, target):\n",
        "    model.eval()\n",
        "    logits = model(input)\n",
        "    loss = nn.functional.cross_entropy(logits, target)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "\n",
        "best_ppl = torch.inf\n",
        "best_model = deepcopy(model.state_dict())\n",
        "\n",
        "wandb.watch(model, log_freq=100)\n",
        "pbar = tqdm(total=max_examples)\n",
        "\n",
        "while n_examples < max_examples:\n",
        "    for input, target in train_loader:\n",
        "        loss = train_step(input.to(device), target.to(device)) \n",
        "        train_losses.append(loss)\n",
        "\n",
        "        if step % eval_every_steps == 0:\n",
        "            train_loss = np.average(train_losses)\n",
        "            train_ppl = np.exp(train_loss)\n",
        "    \n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(input.to(device), target.to(device))\n",
        "                    for input, target in validation_loader\n",
        "                ]))\n",
        "            \n",
        "            wandb.log({\n",
        "                \"train/loss\": loss,\n",
        "                \"train/perplexity\": train_ppl,\n",
        "                \"eval/perplexity\": valid_ppl\n",
        "            }, step=step)\n",
        "            if valid_ppl < best_ppl:\n",
        "                best_ppl = valid_ppl\n",
        "                best_model = deepcopy(model.state_dict())\n",
        "                torch.save(best_model, 'best_model.pth')\n",
        "                \n",
        "                artifact = wandb.Artifact(\n",
        "                    'model-selfattention',\n",
        "                    type='model',\n",
        "                    metadata={\n",
        "                        \"step\": step,\n",
        "                        \"step_size\": batch_size,\n",
        "                        \"train_loss\": train_loss,\n",
        "                        \"train_perplexity\": train_ppl,\n",
        "                        \"valid_perplexity\": valid_ppl\n",
        "                    }\n",
        "                )\n",
        "                artifact.add_file('best_model.pth')\n",
        "                wandb.run.log_artifact(artifact)\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "\n",
        "        n_examples += len(input)  # Increment of batch size\n",
        "        step += 1\n",
        "        pbar.update(len(input))\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fc03883",
      "metadata": {
        "id": "5fc03883",
        "papermill": {
          "duration": 0.057206,
          "end_time": "2022-05-25T23:20:47.299732",
          "exception": false,
          "start_time": "2022-05-25T23:20:47.242526",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d191744",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T23:20:47.406298Z",
          "iopub.status.busy": "2022-05-25T23:20:47.406079Z",
          "iopub.status.idle": "2022-05-25T23:20:50.629808Z",
          "shell.execute_reply": "2022-05-25T23:20:50.628968Z"
        },
        "id": "7d191744",
        "papermill": {
          "duration": 3.286692,
          "end_time": "2022-05-25T23:20:50.639769",
          "exception": false,
          "start_time": "2022-05-25T23:20:47.353077",
          "status": "completed"
        },
        "tags": [],
        "outputId": "5eed8f0a-cff1-4405-f2b8-f6ea8faa61f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test perplexity: 188.9667604164408\n"
          ]
        }
      ],
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(input.to(device), target.to(device))\n",
        "        for input, target in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "769f8b07",
      "metadata": {
        "id": "769f8b07",
        "papermill": {
          "duration": 0.081429,
          "end_time": "2022-05-25T23:20:50.801093",
          "exception": false,
          "start_time": "2022-05-25T23:20:50.719664",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b97bf447",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T23:20:50.961380Z",
          "iopub.status.busy": "2022-05-25T23:20:50.960824Z",
          "iopub.status.idle": "2022-05-25T23:20:53.180902Z",
          "shell.execute_reply": "2022-05-25T23:20:53.180197Z"
        },
        "papermill": {
          "duration": 2.303892,
          "end_time": "2022-05-25T23:20:53.182835",
          "exception": false,
          "start_time": "2022-05-25T23:20:50.878943",
          "status": "completed"
        },
        "tags": [],
        "id": "b97bf447",
        "outputId": "049ebf12-877f-461b-b004-f6be1de7f537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eu gosto de comer pizza pois me faz com\n",
            "Eu gosto de comer pizza pois me faz com o\n",
            "Eu gosto de comer pizza pois me faz com o que\n",
            "Eu gosto de comer pizza pois me faz com o que eu\n",
            "Eu gosto de comer pizza pois me faz com o que eu não\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que o\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que o que\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que o que se\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que o que se refere\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que o que se refere ao\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que o que se refere ao lado\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que o que se refere ao lado de\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que o que se refere ao lado de Deus\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que o que se refere ao lado de Deus.\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que o que se refere ao lado de Deus. O\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que o que se refere ao lado de Deus. O que\n",
            "Eu gosto de comer pizza pois me faz com o que eu não tenho certeza que o que se refere ao lado de Deus. O que é\n",
            "\n",
            "Cachorro e gato são os melhores amigos do mundo\n",
            "Cachorro e gato são os melhores amigos do mundo.\n",
            "Cachorro e gato são os melhores amigos do mundo. O\n",
            "Cachorro e gato são os melhores amigos do mundo. O que\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é o\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é o que\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é o que se\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é o que se refere\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é o que se refere a\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é o que se refere a ser\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é o que se refere a ser um\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é o que se refere a ser um dos\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é o que se refere a ser um dos maiores\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é o que se refere a ser um dos maiores,\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é o que se refere a ser um dos maiores, a\n",
            "Cachorro e gato são os melhores amigos do mundo. O que é o que é o que se refere a ser um dos maiores, a partir\n",
            "\n",
            "Eu gosto de comer chocolate, pois me faz com\n",
            "Eu gosto de comer chocolate, pois me faz com a\n",
            "Eu gosto de comer chocolate, pois me faz com a sua\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida,\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é o\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é o que\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é o que se\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é o que se refere\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é o que se refere ao\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é o que se refere ao lado\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é o que se refere ao lado de\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é o que se refere ao lado de Deus\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é o que se refere ao lado de Deus.\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é o que se refere ao lado de Deus. O\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é o que se refere ao lado de Deus. O que\n",
            "Eu gosto de comer chocolate, pois me faz com a sua vida, mas não é o que se refere ao lado de Deus. O que é\n",
            "\n",
            "Campinas é uma cidade grande e eu gosto de ser\n",
            "Campinas é uma cidade grande e eu gosto de ser um\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores,\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de um\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de um ano\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de um ano de\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de um ano de 2015\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de um ano de 2015,\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de um ano de 2015, a\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de um ano de 2015, a partir\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de um ano de 2015, a partir de\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de um ano de 2015, a partir de um\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de um ano de 2015, a partir de um ano\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de um ano de 2015, a partir de um ano,\n",
            "Campinas é uma cidade grande e eu gosto de ser um dos maiores, a partir de um ano de 2015, a partir de um ano, o\n",
            "\n",
            "As plantações de cana no Brasil desse ano serão a\n",
            "As plantações de cana no Brasil desse ano serão a partir\n",
            "As plantações de cana no Brasil desse ano serão a partir de\n",
            "As plantações de cana no Brasil desse ano serão a partir de um\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano.\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que é\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que é o\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que é o que\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que é o que se\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que é o que se refere\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que é o que se refere a\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que é o que se refere a ser\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que é o que se refere a ser um\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que é o que se refere a ser um dos\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que é o que se refere a ser um dos maiores\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que é o que se refere a ser um dos maiores,\n",
            "As plantações de cana no Brasil desse ano serão a partir de um ano. O que é o que se refere a ser um dos maiores, a\n",
            "\n",
            "As mulheres estão cada vez mais presentes comparado a um\n",
            "As mulheres estão cada vez mais presentes comparado a um dos\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores,\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um ano\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um ano de\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um ano de 2015\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um ano de 2015,\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um ano de 2015, a\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um ano de 2015, a partir\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um ano de 2015, a partir de\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um ano de 2015, a partir de um\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um ano de 2015, a partir de um ano\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um ano de 2015, a partir de um ano,\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um ano de 2015, a partir de um ano, o\n",
            "As mulheres estão cada vez mais presentes comparado a um dos maiores, a partir de um ano de 2015, a partir de um ano, o que\n",
            "\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida.\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida..\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida...\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida....\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida.....\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida......\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida.......\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida........\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida.........\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida..........\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida...........\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida............\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida.............\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida..............\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida...............\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida................\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida.................\n",
            "Temos que pensar no futuro e guardar o que aprendemos na sua vida..................\n",
            "\n",
            "Uma xícara de café e um livro são os\n",
            "Uma xícara de café e um livro são os que\n",
            "Uma xícara de café e um livro são os que se\n",
            "Uma xícara de café e um livro são os que se trata\n",
            "Uma xícara de café e um livro são os que se tratam\n",
            "Uma xícara de café e um livro são os que se tratam a\n",
            "Uma xícara de café e um livro são os que se tratam a sua\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida,\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida, a\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida, a partir\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida, a partir de\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida, a partir de um\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida, a partir de um ano\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida, a partir de um ano de\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida, a partir de um ano de 2015\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida, a partir de um ano de 2015,\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida, a partir de um ano de 2015, a\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida, a partir de um ano de 2015, a partir\n",
            "Uma xícara de café e um livro são os que se tratam a sua vida, a partir de um ano de 2015, a partir de\n",
            "\n",
            "Viagens de avião são muito mais rápidas, porém,\n",
            "Viagens de avião são muito mais rápidas, porém, o\n",
            "Viagens de avião são muito mais rápidas, porém, o que\n",
            "Viagens de avião são muito mais rápidas, porém, o que se\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de Deus\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de Deus.\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de Deus. O\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de Deus. O que\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de Deus. O que é\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de Deus. O que é o\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de Deus. O que é o que\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de Deus. O que é o que se\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de Deus. O que é o que se refere\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de Deus. O que é o que se refere a\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de Deus. O que é o que se refere a ser\n",
            "Viagens de avião são muito mais rápidas, porém, o que se refere ao lado de Deus. O que é o que se refere a ser um\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompts = [\n",
        "    'Eu gosto de comer pizza pois me faz',\n",
        "    'Cachorro e gato são os melhores amigos do',\n",
        "    'Eu gosto de comer chocolate, pois me faz',\n",
        "    'Campinas é uma cidade grande e eu gosto de',\n",
        "    'As plantações de cana no Brasil desse ano serão',\n",
        "    'As mulheres estão cada vez mais presentes comparado a',\n",
        "    'Temos que pensar no futuro e guardar o que aprendemos na',\n",
        "    'Uma xícara de café e um livro são',\n",
        "    'Viagens de avião são muito mais rápidas, porém',\n",
        "]\n",
        "max_output_tokens = 20\n",
        "\n",
        "model.eval()\n",
        "for prompt in prompts:\n",
        "    for _ in range(max_output_tokens):\n",
        "        input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "        input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
        "        logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "        # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
        "        # Isso se chama decodificação gulosa (greedy decoding).\n",
        "        predicted_id = torch.argmax(logits).item()\n",
        "        input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "        prompt = tokenizer.decode(input_ids)\n",
        "        print(prompt)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc53517",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-25T23:20:53.289232Z",
          "iopub.status.busy": "2022-05-25T23:20:53.289023Z",
          "iopub.status.idle": "2022-05-25T23:20:58.939806Z",
          "shell.execute_reply": "2022-05-25T23:20:58.938904Z"
        },
        "papermill": {
          "duration": 5.705116,
          "end_time": "2022-05-25T23:20:58.942539",
          "exception": false,
          "start_time": "2022-05-25T23:20:53.237423",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "26f256f41b3d4b948cac6dc6e310cf15"
          ]
        },
        "id": "dfc53517",
        "outputId": "370b6873-7162-4c07-eae8-c0820c3b4879"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26f256f41b3d4b948cac6dc6e310cf15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='1805.087 MB of 1805.087 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/perplexity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/perplexity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/perplexity</td><td>200.16263</td></tr><tr><td>train/loss</td><td>5.17382</td></tr><tr><td>train/perplexity</td><td>181.3078</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">morning-mountain-19</strong>: <a href=\"https://wandb.ai/flych3r/language-models/runs/3fwjpaxv\" target=\"_blank\">https://wandb.ai/flych3r/language-models/runs/3fwjpaxv</a><br/>Synced 5 W&B file(s), 0 media file(s), 41 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220525_204158-3fwjpaxv/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 10256.15506,
      "end_time": "2022-05-25T23:21:02.069627",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-05-25T20:30:05.914567",
      "version": "2.3.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "name": "IA025 - A08",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}