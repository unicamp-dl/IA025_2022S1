{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 9 - Exercício - Leonardo de Lellis Rossi RA261900",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a8370e41c4ab4b92b46ec122c3cea8b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ec7e60564e24427be57be18714fc839",
              "IPY_MODEL_51ce005f0ff14ec6b69d8468184d847f",
              "IPY_MODEL_ac69e7df64414bf3a4d5c33f4492317e"
            ],
            "layout": "IPY_MODEL_e1aa3c5814c847b090083bfcf0a9f8d7"
          }
        },
        "4ec7e60564e24427be57be18714fc839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbbe5dd8592344c99ea0d5d728066a0c",
            "placeholder": "​",
            "style": "IPY_MODEL_cf257dd423e54edc8ecf28a0a3ddbea9",
            "value": "100%"
          }
        },
        "51ce005f0ff14ec6b69d8468184d847f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8391e221151b4f3aac714a2a16e1a1f8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1f3207bf545425cb3ea631db47025d5",
            "value": 2
          }
        },
        "ac69e7df64414bf3a4d5c33f4492317e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_033ca3bdac224e448515d99a0aba6d30",
            "placeholder": "​",
            "style": "IPY_MODEL_b698ae882bb14eeabcd6dd9f5ab87559",
            "value": " 2/2 [00:00&lt;00:00, 56.99it/s]"
          }
        },
        "e1aa3c5814c847b090083bfcf0a9f8d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbbe5dd8592344c99ea0d5d728066a0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf257dd423e54edc8ecf28a0a3ddbea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8391e221151b4f3aac714a2a16e1a1f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1f3207bf545425cb3ea631db47025d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "033ca3bdac224e448515d99a0aba6d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b698ae882bb14eeabcd6dd9f5ab87559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex09/leonardo_lellis/Aula_9_Exerc%C3%ADcio_Leonardo_de_Lellis_Rossi_RA261900.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = \"Leonardo de Lellis Rossi RA261900\"\n",
        "print(f'Meu nome é {nome}')\n",
        "\n",
        "last = '22/06/07_13h14'\n",
        "print(f'Last update: {last}')\n"
      ],
      "metadata": {
        "id": "jOdQB41_4ZxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a59dd2d7-af76-41b3-a091-8c7f26f33020"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Leonardo de Lellis Rossi RA261900\n",
            "Last update: 22/06/07_13h14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício - Aula 10: Refeito Modelo de Linguagem com auto-atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Update: retreinado com datasets maiores - utilizado 78.51 % do max de ex, 117.765.000 exemplos - maiores batch size (B), emb_dim (D) e hidden_size. Melhoria de 40% no ppl de teste. Alguns textos de teste estão fazendo mais sentido.\n",
        "\n",
        "Neptune treino: https://app.neptune.ai/leolellisr/dl-ia025/e/DLIA-131/charts\n",
        "\n",
        "Neptune test: https://app.neptune.ai/leolellisr/dl-ia025/e/DLIA-132/charts\n",
        "\n",
        "\n",
        "Este exercício é similar ao da Aula 8, mas iremos agora treinar uma rede neural com **duas camadas** de auto-atenção **causais** para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
        "\n",
        "Iremos também trabalhar com sequencias de tamanho variável.\n",
        "\n",
        "Na camada de auto-atenção, não se esqueça de implementar:\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Conexões residuais\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "\n",
        "O dataset usado neste exercício (BrWaC) possui um tamanho razoável e você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "3cbdbddb-cbe4-4b04-83b6-b93d6185f124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug = False\n",
        "checkinpoint = True\n",
        "save_in_drive = True\n",
        "\n",
        "params = {\n",
        "    'max_examples': 150_000_000,\n",
        "    'eval_every_steps': 10_000,\n",
        "    'lr': 3e-4,\n",
        "    'batch_size': 512,\n",
        "    'embedding_dim': 256,\n",
        "    'optimizer': 'Adam',\n",
        "    'retrain': True,\n",
        "    'path_saved_model': 'gdrive/MyDrive/Colab Notebooks/best_model_',\n",
        "    'path_saved_datasets': 'gdrive/MyDrive/Colab Notebooks/ds_',\n",
        "    'download_ds': False,\n",
        "    'aula': 'Aula10',\n",
        "    'max_seq_length': 9,\n",
        "    'train_examples': 90_000,\n",
        "    'valid_examples': 40_000,\n",
        "    'test_examples': 25_000,\n",
        "    'n_heads':4,\n",
        "    'last_step': 180_000,\n",
        "    'train': True\n",
        "}\n",
        "params['path_saved_model'] = params['path_saved_model']+params['aula']+'_BS'+str(params['batch_size'])+'_HS'+str(params['hidden_size'])+'_EmbDim'+str(params['embedding_dim'])+'_MaxEx'+str(params['max_examples'])+'.pt'\n",
        "\n",
        "params['hidden_size'] = params['embedding_dim']*2"
      ],
      "metadata": {
        "id": "dnfTE2XvSr3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09454da-4de0-44bd-fe90-8a5a65a21923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun  6 16:40:41 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "whTCe2i7AtoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds():\n",
        "  random.seed(123)\n",
        "  np.random.seed(123)\n",
        "  torch.manual_seed(123)\n",
        "  torch.cuda.manual_seed(123)\n",
        "set_seeds()"
      ],
      "metadata": {
        "id": "-O1Gy-aeUbv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neptune config"
      ],
      "metadata": {
        "id": "9D9O0IdFUKdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install -U neptune-client\n",
        " import neptune.new as neptune"
      ],
      "metadata": {
        "id": "Ua-dPVPgUM3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d81a893-33f9-471b-fd14-cb9cfeb1a194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: neptune-client in /usr/local/lib/python3.7/dist-packages (0.16.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.2)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.2.0)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.5)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.1.27)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.3)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.4.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (0.18.2)\n",
            "Requirement already satisfied: boto3>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.2)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.25.11)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.7.4)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: bravado in /usr/local/lib/python3.7/dist-packages (from neptune-client) (11.0.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Requirement already satisfied: botocore<1.28.0,>=1.27.2 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.16.0->neptune-client) (1.27.2)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.16.0->neptune-client) (0.6.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.16.0->neptune-client) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.2->boto3>=1.16.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (4.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (6.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.3.3)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.6)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (3.17.6)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (5.17.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.3)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (0.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2022.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (5.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (4.11.4)\n",
            "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.3.8)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.1.4)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (20.11.0)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.1)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2.3)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.12)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (3.8.0)\n",
            "Requirement already satisfied: cached-property>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from fqdn->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.2)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run = neptune.init(name= params['aula'], tags=[params['aula'], 'Auto-atenção', 'Self-Attention', 'checkinpoint', 'CrossEntropy', 'Adam', 'perplexity', 'BrWaC'],\n",
        "    project=\"leolellisr/dl-ia025\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1NjY1YmJkZi1hYmM5LTQ3M2QtOGU1ZC1iZTFlNWY4NjE1NDQifQ==\",\n",
        ")"
      ],
      "metadata": {
        "id": "KNvj9TrsUOdI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0998ef1-d001-49f9-9f12-88a2b23be836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/leolellisr/dl-ia025/e/DLIA-133\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run['parameters'] = params"
      ],
      "metadata": {
        "id": "2Ld1XqRPVh21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import itertools"
      ],
      "metadata": {
        "id": "K2517Ek9ZBv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "def tokenize(text: str, tokenizer):\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, max_seq_length: int):\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.X = []\n",
        "        for text in tqdm_notebook(texts):\n",
        "          token_ids = tokenize(f'[CLS] {text}', tokenizer) # + [tokenizer.vocab['[SEP]']]  \n",
        "          token_ids += [tokenizer.vocab['[PAD]']] * max(0, 1 + max_seq_length - len(token_ids))\n",
        "          for i in range(0, len(token_ids) - 1, max_seq_length):\n",
        "            if i + max_seq_length < len(token_ids):\n",
        "              self.X.append(token_ids[i: i + max_seq_length + 1])\n",
        "            else:\n",
        "              self.X.append(token_ids[-max_seq_length - 1:])\n",
        "        self.X = torch.LongTensor(self.X)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Escreva seu código aqui\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Escreva seu código aqui\n",
        "        x_y_idx = self.X[idx]\n",
        "        return x_y_idx[:-1], x_y_idx[1:]       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n"
      ],
      "metadata": {
        "id": "NOwJMEllqm4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando se a implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "debug = False\n",
        "params['batch_size'] = 10\n",
        "batch_size = params['batch_size'] \n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=9)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "print(f'len(dummy_dataset): {len(dummy_dataset)}')\n",
        "assert len(dummy_dataset) == 2\n",
        "print('Passou no assert de tamanho do dataset.')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[  101,  3396, 10303,   125, 13239,     0,     0,     0,     0],\n",
        "     [  101,  1660,  5971,   785,   125,  1847, 13779, 15616,     0]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125, 13239,     0,     0,     0,     0,     0],\n",
        "     [ 1660,  5971,   785,   125,  1847, 13779, 15616,     0,     0]])\n",
        "\n",
        "\n",
        "if debug: print(first_batch_input)\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "\n",
        "if debug:  print(first_batch_target)\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "\n",
        "print('Passou no assert de dataset.')"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "a8370e41c4ab4b92b46ec122c3cea8b3",
            "4ec7e60564e24427be57be18714fc839",
            "51ce005f0ff14ec6b69d8468184d847f",
            "ac69e7df64414bf3a4d5c33f4492317e",
            "e1aa3c5814c847b090083bfcf0a9f8d7",
            "bbbe5dd8592344c99ea0d5d728066a0c",
            "cf257dd423e54edc8ecf28a0a3ddbea9",
            "8391e221151b4f3aac714a2a16e1a1f8",
            "b1f3207bf545425cb3ea631db47025d5",
            "033ca3bdac224e448515d99a0aba6d30",
            "b698ae882bb14eeabcd6dd9f5ab87559"
          ]
        },
        "outputId": "57577f39-7ec3-47ed-fc01-68a3b0b842ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8370e41c4ab4b92b46ec122c3cea8b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(dummy_dataset): 2\n",
            "Passou no assert de tamanho do dataset.\n",
            "Passou no assert de dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "if save_in_drive: drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIwYycaugsxY",
        "outputId": "2e719234-e893-4f43-86b0-da47d3343631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGlN1WqrXPA6",
        "outputId": "7f2f2e47-19fa-47ac-d117-b5b5492d57df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-06 11:55:30--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.147.128, 142.250.125.128, 142.250.136.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.147.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1230909256 (1.1G) [text/plain]\n",
            "Saving to: ‘sample-1gb.txt’\n",
            "\n",
            "sample-1gb.txt      100%[===================>]   1.15G   243MB/s    in 5.2s    \n",
            "\n",
            "2022-06-06 11:55:36 (225 MB/s) - ‘sample-1gb.txt’ saved [1230909256/1230909256]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "max_seq_length = params['max_seq_length']\n",
        "\n",
        "train_examples = params['train_examples']\n",
        "valid_examples = params['valid_examples']\n",
        "test_examples = params['test_examples']\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "gxa_4gmiA-wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NGpsakPdG4Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_lines = train_examples + valid_examples + test_examples\n",
        "print(f'Truncating to {max_lines} lines.')\n",
        "\n",
        "if params['download_ds']:\n",
        "  texts = open('sample-1gb.txt').readlines()\n",
        "  print(f'Read {len(texts)} lines.')\n",
        "  \n",
        "  #smart batching\n",
        "  texts = sorted(texts, key=lambda x: len(x[0]))\n",
        "\n",
        "  texts = texts[:max_lines]  \n",
        "  \n",
        "  training_texts = texts[:-(valid_examples + test_examples)]\n",
        "  valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "  test_texts = texts[-test_examples:]\n",
        "\n",
        "  training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "  valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "  test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "else:\n",
        "  path = params['path_saved_datasets']+'train.pt'\n",
        "  training_dataset = torch.load(path)\n",
        "  path = params['path_saved_datasets']+'val.pt'\n",
        "  valid_dataset = torch.load(path)\n",
        "  path = params['path_saved_datasets']+'test.pt'\n",
        "  test_dataset = torch.load(path)\n",
        "params['batch_size'] = 64\n",
        "batch_size = params['batch_size'] \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDari2UL78_5",
        "outputId": "e66791d5-da3c-4a6b-ff81-27edc5b158f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truncating to 155000 lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d833adc-1710-4442-e55c-be2cab19b652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 11198040\n",
            "valid examples: 5011771\n",
            "test examples: 3041508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if save_in_drive and params['download_ds']:\n",
        "  torch.save(training_dataset, params['path_saved_datasets']+\"train.pt\")\n",
        "  torch.save(valid_dataset, params['path_saved_datasets']+\"val.pt\")\n",
        "  torch.save(test_dataset, params['path_saved_datasets']+\"test.pt\")"
      ],
      "metadata": {
        "id": "VeNzN6ZjWP09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "CmjYdmqANlRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# É recomendado reiniciar as seeds antes de inicializar o modelo, pois assim\n",
        "# garantimos que os pesos vao ser sempre os mesmos.\n",
        "set_seeds()\n",
        "\n",
        "class SelfAttentionLayer(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, padding_idx, n_heads, dim, max_length):\n",
        "        super().__init__()\n",
        "        # n_heads: H\n",
        "        # dim: D\n",
        "        # max_lenght: L\n",
        "        # vocab_lenght: V\n",
        "        self.H = n_heads\n",
        "        self.D = dim\n",
        "        self.L = max_length\n",
        "        self.D_H = self.D // self.H # D / H\n",
        "        self.pad = padding_idx\n",
        "\n",
        "        self.W_q = torch.nn.Linear(self.D, self.D, bias=False) # (D, D)\n",
        "        self.W_k = torch.nn.Linear(self.D, self.D, bias=False)\n",
        "        self.W_v = torch.nn.Linear(self.D, self.D, bias=False)\n",
        "        self.W_o = torch.nn.Linear(self.D, self.D, bias=False)\n",
        "\n",
        "        self.layer_norm1  = torch.nn.LayerNorm(self.D, eps=1e-6)\n",
        "\n",
        "        self.feed_forward = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.D, self.D*10),  \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(self.D*10, self.D)\n",
        "        )\n",
        "        self.layer_norm2  = torch.nn.LayerNorm(self.D, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, x, att_mask):\n",
        "        # multi-head self-attention\n",
        "        \n",
        "        fQ = self.W_q(x).reshape(len(x), self.L, self.H, self.D_H) # (B, L, H, D/H)\n",
        "        fK = self.W_k(x).reshape(len(x), self.L, self.H, self.D_H)\n",
        "        fV = self.W_v(x).reshape(len(x), self.L, self.H, self.D_H)\n",
        "\n",
        "        # (B, L, H, D/H) -> (B, H, L, D/H)\n",
        "        fQ_transposed = fQ.transpose(1, 2)                    # (B, H, L, D/H)\n",
        "        fK_transposed = fK.transpose(1, 2)\n",
        "        fV_transposed = fV.transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(fQ_transposed, fK_transposed.transpose(-2, -1)) / math.sqrt(self.D_H) # (B, H, L, L)\n",
        "\n",
        "        scores = scores.masked_fill(att_mask.unsqueeze(1) == self.pad, -float(\"inf\"))\n",
        "\n",
        "        if debug: print(f\"scores (B, H, L, L): {scores.shape}\")  \n",
        "\n",
        "        probs = F.softmax(scores, dim=-1) # shape = B, L, L\n",
        "        if debug: print(f\"probs (B, H, L, L): {probs.shape}\")  \n",
        "\n",
        "        E = torch.matmul(probs, fV_transposed)\n",
        "        if debug: print(f\"E (B, H, L, D/H): {E.shape}\")  \n",
        "\n",
        "        out = E.transpose(1, 2).contiguous()                   # (B, L, H, D/H)\n",
        "        out = out.reshape(len(x), self.L, self.D) # (B, L, D) \n",
        "        out = self.W_o(out)\n",
        "        if debug: print(f\"out (B, L, D): {out.shape}\")  # (B, L, D)    \n",
        "\n",
        "        out = self.layer_norm1(x+out)               # (B, L, D) \n",
        "        out = self.feed_forward(out)\n",
        "        out = self.layer_norm2(x+out)           # (B, L, D) \n",
        "        #att_mean = att_norm2 * padMask.unsqueeze(-1)      \n",
        "        #mean_embeddings = att_mean.sum(dim=1) / padMask.count_nonzero(-1).unsqueeze(1)  \n",
        "        return out"
      ],
      "metadata": {
        "id": "GgdE_7xnMp7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MsAs84KmQVXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "source": [
        "set_seeds()\n",
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int, pad_token_id: int, hidden: int, n_heads):\n",
        "        \"\"\"\n",
        "        Implements the Self-attention, decoder-only.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
        "            dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            n_layers (int): number of self-attention layers.\n",
        "            pad_token_id (int): id of the pad token that will be ignored in the attention.\n",
        "          \n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        super().__init__()\n",
        "        self.H = n_heads \n",
        "        self.D = dim\n",
        "        self.L = max_seq_length\n",
        "        self.D_H = self.D // self.H # D / H\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.V = vocab_size\n",
        "        self.hidden_size = hidden\n",
        "        # word embedding \n",
        "        self.embeddings_c = nn.Embedding(self.V, self.D,padding_idx=pad_token_id)\n",
        "        self.embeddings_p = torch.nn.Linear(self.D, self.L, bias=False)\n",
        "\n",
        "#        self.att_layer = \n",
        "        self.att_layer1 = SelfAttentionLayer(padding_idx=pad_token_id, n_heads=self.H, dim=self.D, max_length=self.L)\n",
        "        self.att_layer2 = SelfAttentionLayer(padding_idx=pad_token_id, n_heads=self.H, dim=self.D, max_length=self.L)\n",
        "\n",
        "        self.feed_forward = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.D, self.hidden_size),    # (D, hidden_size)\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            torch.nn.Linear(self.hidden_size, self.V)     # (hidden_size, V)\n",
        "        )\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
        "            B: batch_size\n",
        "            L: max_seq_length\n",
        "            D: embedding_dim\n",
        "            V: vocab_size\n",
        "            input shape: (B, L)\n",
        "            pos shape: (B, L)\n",
        "        Returns:\n",
        "            logits of shape (batch_size, max_seq_length, vocab_size)\n",
        "        \"\"\"\n",
        "        B = inputs.shape[0]\n",
        "\n",
        "        att_mask = torch.tril(torch.ones(B, self.L, self.L)).to(device)\n",
        "        att_mask = att_mask.masked_fill(inputs.unsqueeze(1) == self.pad_token_id, 0)\n",
        "        att_mask = att_mask.masked_fill(inputs.unsqueeze(2) == self.pad_token_id, 0)\n",
        "        # input shape: (B, L)\n",
        "        x_emb = self.embeddings_c(inputs) + self.embeddings_p.weight # (B, L, D)\n",
        "        \n",
        "        if debug: print(f'shape x_emb: {x_emb.shape}')                # (B, L, D)\n",
        "\n",
        "        x_emb = self.att_layer1(x_emb, att_mask)     \n",
        "        x_emb = self.att_layer2(x_emb, att_mask)\n",
        "        #logits =  self.att_layer(x_emb, att_mask)                     # (B, L, D)\n",
        "        if debug: print(f'shape x_emb: {x_emb.shape}')              # (B, L, D)\n",
        "        logits = self.feed_forward(x_emb)                            # (B, L, V)\n",
        "        if debug: print(f'shape logits (B, L, V): {logits.shape}')    # (B, L, V)\n",
        "        \n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvVgqCENvNfT",
        "outputId": "31d1787b-fb72-4732-c911-366b933f5988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb1d5b2-49fd-4c3d-ac48-806a5d5e6244"
      },
      "source": [
        "debug = True\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=64,\n",
        "    n_layers=2,\n",
        "    hidden = 128,\n",
        "    n_heads = 4,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ").to(device)\n",
        "\n",
        "\n",
        "sample_input, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_input = sample_input.to(device)\n",
        "sample_output = model(sample_input)\n",
        "print(f'sample_input.shape: {sample_input.shape}')\n",
        "print(f'sample_output.shape: {sample_output.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape x_emb: torch.Size([1, 9, 64])\n",
            "scores (B, H, L, L): torch.Size([1, 4, 9, 9])\n",
            "probs (B, H, L, L): torch.Size([1, 4, 9, 9])\n",
            "E (B, H, L, D/H): torch.Size([1, 4, 9, 16])\n",
            "out (B, L, D): torch.Size([1, 9, 64])\n",
            "scores (B, H, L, L): torch.Size([1, 4, 9, 9])\n",
            "probs (B, H, L, L): torch.Size([1, 4, 9, 9])\n",
            "E (B, H, L, D/H): torch.Size([1, 4, 9, 16])\n",
            "out (B, L, D): torch.Size([1, 9, 64])\n",
            "shape x_emb: torch.Size([1, 9, 64])\n",
            "shape logits (B, L, V): torch.Size([1, 9, 29794])\n",
            "sample_input.shape: torch.Size([1, 9])\n",
            "sample_output.shape: torch.Size([1, 9, 29794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0u4m2eFaeo5",
        "outputId": "b504961f-9bec-4acb-f17b-f013e95ff27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101, 20100,  2308,  3074,  1089,   481,   117,   146,  1189]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7adc3b4a-7e77-4ce9-f0c9-e0d3cad29c58"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 5957666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target, ignore_token_id: int):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, seq_length, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size, seq_length)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity\n",
        "    \"\"\"\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target = target.reshape(-1)\n",
        "    if debug: \n",
        "      print(f'logits: {logits.shape}')\n",
        "      print(f'train_target_ids: {target.shape}')\n",
        "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
        "    return torch.exp(loss)\n",
        "\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "train_input_ids, train_target_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "train_input_ids = train_input_ids.to(device)\n",
        "train_target_ids = train_target_ids.to(device)\n",
        "\n",
        "logits = model.forward(train_input_ids)\n",
        "if debug:\n",
        "  print(f'logits: {logits.shape}')\n",
        "  print(f'train_target_ids: {train_target_ids.shape}')\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=train_target_ids, ignore_token_id=tokenizer.pad_token_id)\n",
        "if debug:\n",
        "  print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "  print(f'my perplexity:              {int(my_perplexity)}')\n",
        "\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=7000)\n",
        "print('Passou o no assert da perplexidade')\n",
        "run['perplexity'].log(my_perplexity) # Envia perplexity para o Neptune.\n"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1659ed8-ac6e-4bb9-9f37-3a2a249dfaae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape x_emb: torch.Size([1000, 9, 64])\n",
            "scores (B, H, L, L): torch.Size([1000, 4, 9, 9])\n",
            "probs (B, H, L, L): torch.Size([1000, 4, 9, 9])\n",
            "E (B, H, L, D/H): torch.Size([1000, 4, 9, 16])\n",
            "out (B, L, D): torch.Size([1000, 9, 64])\n",
            "scores (B, H, L, L): torch.Size([1000, 4, 9, 9])\n",
            "probs (B, H, L, L): torch.Size([1000, 4, 9, 9])\n",
            "E (B, H, L, D/H): torch.Size([1000, 4, 9, 16])\n",
            "out (B, L, D): torch.Size([1000, 9, 64])\n",
            "shape x_emb: torch.Size([1000, 9, 64])\n",
            "shape logits (B, L, V): torch.Size([1000, 9, 29794])\n",
            "logits: torch.Size([1000, 9, 29794])\n",
            "train_target_ids: torch.Size([1000, 9])\n",
            "logits: torch.Size([9000, 29794])\n",
            "train_target_ids: torch.Size([9000])\n",
            "correct initial perplexity: 29794\n",
            "my perplexity:              30478\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YyE_DJkfJ8sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = params['batch_size']\n",
        "\n",
        "debug = False\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=params['max_seq_length'],\n",
        "    dim=params['embedding_dim'],\n",
        "    n_layers=2,\n",
        "    hidden = params['hidden_size'],\n",
        "    n_heads = params['n_heads'],\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=params['batch_size'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
        "\n",
        "best_valid_ppl = 10e9\n",
        "\n",
        "if params['retrain']:\n",
        "  model.load_state_dict(torch.load(params['path_saved_model']).state_dict())\n",
        "\n",
        "def train_step(input_ids, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_ids, target_ids):\n",
        "    model.eval()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "MI7smWjSLY93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26431856-0d5d-4d82-f5af-6e178a063053"
      },
      "source": [
        "\n",
        "\n",
        "if params['train']:\n",
        "  train_losses = []\n",
        "  n_examples = 0\n",
        "  step = 0\n",
        "  while n_examples < params['max_examples']:\n",
        "      for train_input_ids, train_target_ids in train_loader:\n",
        "        if step < params['last_step']:\n",
        "          step += 1\n",
        "          \n",
        "          n_examples += len(train_input_ids)  # Increment of batch size\n",
        "        else:\n",
        "          loss = train_step(train_input_ids.to(device), train_target_ids.to(device)) \n",
        "          train_losses.append(loss)\n",
        "          \n",
        "          if step %  params['eval_every_steps'] == 0:\n",
        "              train_ppl = np.exp(np.average(train_losses))\n",
        "              run['train/ppl'].log(train_ppl) # Envia train ppl para o Neptune.\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  valid_ppl = np.exp(np.average([\n",
        "                      validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
        "                      for val_input_ids, val_target_ids in validation_loader]))\n",
        "              run['valid/ppl'].log(valid_ppl) # Envia valid ppl para o Neptune.\n",
        "              if checkinpoint and valid_ppl < best_valid_ppl:\n",
        "                torch.save(model.state_dict(), 'best_model.pt')\n",
        "                if save_in_drive: torch.save(model, params['path_saved_model'])\n",
        "                print(f\"Best model found in step {step}. valid ppl: {valid_ppl:.2f}, best_valid_ppl: {best_valid_ppl:.2f} \")\n",
        "                best_valid_ppl = valid_ppl\n",
        "              ex_least = n_examples/params['max_examples']*100\n",
        "              print(f'{step} steps; {n_examples} examples so far; {ex_least:.2f} % ; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}, best_valid_ppl: {best_valid_ppl:.2f}')\n",
        "              train_losses = []\n",
        "\n",
        "          n_examples += len(train_input_ids)  # Increment of batch size\n",
        "          step += 1\n",
        "          params['step'] = step\n",
        "          run['parameters'] = params\n",
        "          if n_examples >= params['max_examples']:\n",
        "              break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model found in step 180000. valid ppl: 143.95, best_valid_ppl: 10000000000.00 \n",
            "180000 steps; 92160000 examples so far; 61.44 % ; train ppl: 133.73, valid ppl: 143.95, best_valid_ppl: 143.95\n",
            "Best model found in step 190000. valid ppl: 130.36, best_valid_ppl: 143.95 \n",
            "190000 steps; 97280000 examples so far; 64.85 % ; train ppl: 136.05, valid ppl: 130.36, best_valid_ppl: 130.36\n",
            "Best model found in step 200000. valid ppl: 126.65, best_valid_ppl: 130.36 \n",
            "200000 steps; 102400000 examples so far; 68.27 % ; train ppl: 130.93, valid ppl: 126.65, best_valid_ppl: 126.65\n",
            "Best model found in step 210000. valid ppl: 124.30, best_valid_ppl: 126.65 \n",
            "210000 steps; 107520000 examples so far; 71.68 % ; train ppl: 127.59, valid ppl: 124.30, best_valid_ppl: 124.30\n",
            "Best model found in step 220000. valid ppl: 122.17, best_valid_ppl: 124.30 \n",
            "220000 steps; 112640000 examples so far; 75.09 % ; train ppl: 125.50, valid ppl: 122.17, best_valid_ppl: 122.17\n",
            "Best model found in step 230000. valid ppl: 120.51, best_valid_ppl: 122.17 \n",
            "230000 steps; 117760000 examples so far; 78.51 % ; train ppl: 122.86, valid ppl: 120.51, best_valid_ppl: 120.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if params['retrain']:\n",
        "  model.load_state_dict(torch.load(params['path_saved_model']).state_dict())"
      ],
      "metadata": {
        "id": "wKxF1aykIYdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=params['batch_size'])"
      ],
      "metadata": {
        "id": "i64OYq5U5Xh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56102f1-6794-49f1-f611-0bab43634f54"
      },
      "source": [
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(test_input_ids.to(device), test_target_ids.to(device))\n",
        "        for test_input_ids, test_target_ids in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 119.45353756193624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run['test/perplexity'].log(test_ppl)\n"
      ],
      "metadata": {
        "id": "dqKk8eGXKgfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.stop()"
      ],
      "metadata": {
        "id": "ZLT8x2WYKicU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f809c6-7083-47b8-d157-948871ffc187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 4 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 4 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/leolellisr/dl-ia025/e/DLIA-132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Eu gosto de comer pizza pois me faz'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    print(f'pred: {predicted_id}')\n",
        "    print(f'input_ids: {input_ids}')\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "pqxFR-wRvflc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d9d2a2-4cb8-41c7-e496-3ff3273abdac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred: 13519\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519]\n",
            "Eu gosto de comer pizza pois me faz lembrar\n",
            "pred: 146\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146]\n",
            "Eu gosto de comer pizza pois me faz lembrar o\n",
            "pred: 179\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que\n",
            "pred: 2779\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu\n",
            "pred: 346\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não\n",
            "pred: 978\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha\n",
            "pred: 123\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a\n",
            "pred: 792\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver\n",
            "pred: 170\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792, 170]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver com\n",
            "pred: 146\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792, 170, 146]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver com o\n",
            "pred: 179\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792, 170, 146, 179]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver com o que\n",
            "pred: 2779\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792, 170, 146, 179, 2779]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver com o que eu\n",
            "pred: 346\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792, 170, 146, 179, 2779, 346]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver com o que eu não\n",
            "pred: 978\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792, 170, 146, 179, 2779, 346, 978]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver com o que eu não tinha\n",
            "pred: 123\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792, 170, 146, 179, 2779, 346, 978, 123]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver com o que eu não tinha a\n",
            "pred: 792\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792, 170, 146, 179, 2779, 346, 978, 123, 792]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver com o que eu não tinha a ver\n",
            "pred: 170\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792, 170, 146, 179, 2779, 346, 978, 123, 792, 170]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver com o que eu não tinha a ver com\n",
            "pred: 146\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792, 170, 146, 179, 2779, 346, 978, 123, 792, 170, 146]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver com o que eu não tinha a ver com o\n",
            "pred: 179\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792, 170, 146, 179, 2779, 346, 978, 123, 792, 170, 146, 179]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver com o que eu não tinha a ver com o que\n",
            "pred: 2779\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 146, 179, 2779, 346, 978, 123, 792, 170, 146, 179, 2779, 346, 978, 123, 792, 170, 146, 179, 2779]\n",
            "Eu gosto de comer pizza pois me faz lembrar o que eu não tinha a ver com o que eu não tinha a ver com o que eu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Ouviram do Ipiranga em suas margens plácidas um grito'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "bzU6DHWHvhDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2244d0d7-c434-4916-804f-d874e561c157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ouviram do Ipiranga em suas margens plácidas um grito de\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é o\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é o que\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é o que é\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é o que é o\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é o que é o que\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é o que é o que é\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é o que é o que é o\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é o que é o que é o que\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é o que é o que é o que é\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é o que é o que é o que é o\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é o que é o que é o que é o que\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de que o que é o que é o que é o que é o que é o que é\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt ='A galinha atravessou a rua para chegar'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "i8NDCTAmyf25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ac5edb-33bf-4f2a-a2fa-3e9cfc0fb3de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A galinha atravessou a rua para chegar ao\n",
            "A galinha atravessou a rua para chegar ao local\n",
            "A galinha atravessou a rua para chegar ao local.\n",
            "A galinha atravessou a rua para chegar ao local. O\n",
            "A galinha atravessou a rua para chegar ao local. O que\n",
            "A galinha atravessou a rua para chegar ao local. O que é\n",
            "A galinha atravessou a rua para chegar ao local. O que é que\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o que\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o que se\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o que se refere\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o que se refere à\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o que se refere à sua\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o que se refere à sua vida\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o que se refere à sua vida.\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o que se refere à sua vida. O\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o que se refere à sua vida. O que\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o que se refere à sua vida. O que é\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o que se refere à sua vida. O que é o\n",
            "A galinha atravessou a rua para chegar ao local. O que é que o que se refere à sua vida. O que é o que\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt ='Ouça com cuidado, o segredo para a felicidade é'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "XOefflDDynTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db2d916-7fe5-4f4a-f56e-92537982fcee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ouça com cuidado, o segredo para a felicidade é a\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida.\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que o\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que o que\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que o que se\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que o que se pode\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que o que se pode fazer\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que o que se pode fazer é\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que o que se pode fazer é a\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que o que se pode fazer é a sua\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que o que se pode fazer é a sua própria\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que o que se pode fazer é a sua própria vida\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que o que se pode fazer é a sua própria vida.\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é que o que se pode fazer é a sua própria vida. O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt ='Temos que pegar! Isso eu sei. Pegá-los eu tentarei! Vai ser grande a'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "mJJHktsjyIfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f9028a-7ccc-4f65-e67c-fc2a03438255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe,\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lemb\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro de\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro de que\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro de que o\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro de que o que\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro de que o que eu\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro de que o que eu não\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro de que o que eu não tinha\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro de que o que eu não tinha a\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro de que o que eu não tinha a ver\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro de que o que eu não tinha a ver com\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro de que o que eu não tinha a ver com o\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a minha mãe, mas não me lembro de que o que eu não tinha a ver com o que\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt ='Desejo para todas as inimigas vida longa'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "AT_dpOD8y2_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0349d73d-8289-49c0-8beb-2b80683a9b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Desejo para todas as inimigas vida longa,\n",
            "Desejo para todas as inimigas vida longa, mas\n",
            "Desejo para todas as inimigas vida longa, mas não\n",
            "Desejo para todas as inimigas vida longa, mas não é\n",
            "Desejo para todas as inimigas vida longa, mas não é o\n",
            "Desejo para todas as inimigas vida longa, mas não é o que\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o que\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o que é\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o que é o\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o que é o que\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o que é o que é\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o que é o que é o\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o que é o que é o que\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o que é o que é o que é\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o que é o que é o que é o\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o que é o que é o que é o que\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o que é o que é o que é o que é\n",
            "Desejo para todas as inimigas vida longa, mas não é o que é o que é o que é o que é o que é o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = next(iter(test_loader))\n",
        "input_ids = torch.cat((test[0][0],test[0][1],test[0][2]), dim=0)\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "input_ids = input_ids.tolist()\n",
        "prompt = tokenizer.decode(input_ids)\n",
        "print(prompt)\n",
        "for _ in range(max_output_tokens):\n",
        "    #input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    #print(input_ids)\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    \n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    #print(input_ids)\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "lpvUtaRLzJHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae15377a-0c1f-45f2-a0de-a0778e043607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é o\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é o que\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é o que é\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é o que é o\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é o que é o que\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é o que é o que é\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é o que é o que é o\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é o que é o que é o que\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é o que é o que é o que é\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é o que é o que é o que é o\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é o que é o que é o que é o que\n",
            "[CLS] Decameron, de Boccaccio, chega ao Brasil numa tradução completa e numa tradução parcial, ambas de alta qualidade O que é o que é o que é o que é o que é o que é o que é\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = next(iter(train_loader))\n",
        "input_ids = torch.cat((test[0][0],test[0][1],test[0][2]), dim=0)\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "input_ids = input_ids.tolist()\n",
        "prompt = tokenizer.decode(input_ids)\n",
        "print(prompt)\n",
        "for _ in range(max_output_tokens):\n",
        "    #input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    #print(input_ids)\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    \n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    #print(input_ids)\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "trv2e9fR1eb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ae10a5-f0ad-49da-a82c-5ae83245aae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde,\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que é\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que é o\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que é o caso\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que é o caso de\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que é o caso de um\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que é o caso de um dos\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que é o caso de um dos mais\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que é o caso de um dos mais importantes\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que é o caso de um dos mais importantes nomes\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que é o caso de um dos mais importantes nomes da\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que é o caso de um dos mais importantes nomes da história\n",
            "alguns fornecedores, possíveis parceiros, e para deus é uma nota de 100 \", masrespiratória. Além disso, ajudam na formação de um sistema de saúde, que é o caso de um dos mais importantes nomes da história do\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = next(iter(validation_loader))\n",
        "input_ids = torch.cat((test[0][0],test[0][1],test[0][2]), dim=0)\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "input_ids = input_ids.tolist()\n",
        "prompt = tokenizer.decode(input_ids)\n",
        "print(prompt)\n",
        "for _ in range(max_output_tokens):\n",
        "    #input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    #print(input_ids)\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    \n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    #print(input_ids)\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "4-t5IeyD1jdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a772de-16ce-4886-baa6-1a0397fa0a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode fazer\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode fazer.\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode fazer. O\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode fazer. O que\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode fazer. O que é\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode fazer. O que é que\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode fazer. O que é que o\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode fazer. O que é que o que\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode fazer. O que é que o que é\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode fazer. O que é que o que é o\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode fazer. O que é que o que é o que\n",
            "[CLS] Andre Torreta Publicitário, escritor e palestrante. Trabalhou como redator e diretor de Criação em empresas de comunicação, como a de que o governo não se pode fazer. O que é que o que é o que é\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus 1\n",
        "Quem conseguir a menor perplexidade no dataset de testes ganha 0.5 ponto na média final.\n",
        "\n",
        "## Bonus 2\n",
        "Qual é a complexidade (em notação O-grande) da função de geração de texto acima?\n",
        "\n",
        "Quem responder corretamente a pergunta acima e deixar a função com menor complexidade ganha 0.5 ponto na média final."
      ],
      "metadata": {
        "id": "nGdxlXhGq7Ua"
      }
    }
  ]
}